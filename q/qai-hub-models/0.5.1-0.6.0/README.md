# Comparing `tmp/qai_hub_models-0.5.1-py3-none-any.whl.zip` & `tmp/qai_hub_models-0.6.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,944 +1,1018 @@
-Zip file size: 1056690 bytes, number of entries: 942
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/__init__.py
--rw-r--r--  2.0 unx      281 b- defN 24-Apr-30 21:08 qai_hub_models/_version.py
--rw-r--r--  2.0 unx      620 b- defN 24-Apr-30 21:10 qai_hub_models/asset_bases.yaml
--rw-r--r--  2.0 unx      734 b- defN 24-Apr-30 21:08 qai_hub_models/conftest.py
--rw-r--r--  2.0 unx      913 b- defN 24-Apr-30 21:08 qai_hub_models/global_requirements.txt
--rw-r--r--  2.0 unx      395 b- defN 24-Apr-30 21:08 qai_hub_models/requirements-dev.txt
--rw-r--r--  2.0 unx      427 b- defN 24-Apr-30 21:08 qai_hub_models/requirements.txt
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/__init__.py
--rw-r--r--  2.0 unx     4757 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/bsd300.py
--rw-r--r--  2.0 unx     4109 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/coco.py
--rw-r--r--  2.0 unx     1614 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/common.py
--rw-r--r--  2.0 unx     3173 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/imagenette.py
--rw-r--r--  2.0 unx     2616 b- defN 24-Apr-30 21:08 qai_hub_models/datasets/pascal_voc.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/__init__.py
--rw-r--r--  2.0 unx     6212 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/base_evaluators.py
--rw-r--r--  2.0 unx     1326 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/classification_evaluator.py
--rw-r--r--  2.0 unx     3699 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/detection_evaluator.py
--rw-r--r--  2.0 unx     2434 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/image_evaluator.py
--rw-r--r--  2.0 unx     2181 b- defN 24-Apr-30 21:08 qai_hub_models/evaluators/superres_evaluator.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/__init__.py
--rw-r--r--  2.0 unx      666 b- defN 24-Apr-30 21:08 qai_hub_models/models/common.py
--rw-r--r--  2.0 unx     7882 b- defN 24-Apr-30 21:08 qai_hub_models/models/protocols.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/__init__.py
--rw-r--r--  2.0 unx     1655 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/common.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/__init__.py
--rw-r--r--  2.0 unx     4346 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/app.py
--rw-r--r--  2.0 unx     2904 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/demo.py
--rw-r--r--  2.0 unx      890 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py
--rw-r--r--  2.0 unx     2888 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/model.py
--rw-r--r--  2.0 unx     8565 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/cityscapes_segmentation/patches/move_datasets.diff
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/deeplab/__init__.py
--rw-r--r--  2.0 unx     2449 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/deeplab/app.py
--rw-r--r--  2.0 unx     2256 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/deeplab/demo.py
--rw-r--r--  2.0 unx      915 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/deeplab/evaluator.py
--rw-r--r--  2.0 unx     2015 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/deeplab/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/detr/__init__.py
--rw-r--r--  2.0 unx     4604 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/detr/app.py
--rw-r--r--  2.0 unx     3565 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/detr/coco_label_map.py
--rw-r--r--  2.0 unx     2091 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/detr/demo.py
--rw-r--r--  2.0 unx     2050 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/detr/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/fastsam/__init__.py
--rw-r--r--  2.0 unx     4883 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/fastsam/app.py
--rw-r--r--  2.0 unx     2022 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/fastsam/demo.py
--rw-r--r--  2.0 unx     1935 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/fastsam/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet/__init__.py
--rw-r--r--  2.0 unx     4548 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet/model.py
--rw-r--r--  2.0 unx     1569 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet/test_utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet_quantized/__init__.py
--rw-r--r--  2.0 unx     1165 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet_quantized/aimet_config.json
--rw-r--r--  2.0 unx     2475 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/ffnet_quantized/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/imagenet_classifier/__init__.py
--rw-r--r--  2.0 unx     3041 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/imagenet_classifier/app.py
--rw-r--r--  2.0 unx     2432 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/imagenet_classifier/demo.py
--rw-r--r--  2.0 unx     4661 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/imagenet_classifier/model.py
--rw-r--r--  2.0 unx     3781 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/imagenet_classifier/test_utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/mediapipe/__init__.py
--rw-r--r--  2.0 unx    30293 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/mediapipe/app.py
--rw-r--r--  2.0 unx     4394 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/mediapipe/utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/quicksrnet/__init__.py
--rw-r--r--  2.0 unx      985 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/quicksrnet/common.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/repaint/__init__.py
--rw-r--r--  2.0 unx     3366 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/repaint/app.py
--rw-r--r--  2.0 unx     2229 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/repaint/demo.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/sesr/__init__.py
--rw-r--r--  2.0 unx     1029 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/sesr/common.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/super_resolution/__init__.py
--rw-r--r--  2.0 unx     2143 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/super_resolution/app.py
--rw-r--r--  2.0 unx     2775 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/super_resolution/demo.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/swin/__init__.py
--rw-r--r--  2.0 unx     9175 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/swin/swin_transformer.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/video_classifier/__init__.py
--rw-r--r--  2.0 unx    11183 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/video_classifier/app.py
--rw-r--r--  2.0 unx     1581 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/video_classifier/demo.py
--rw-r--r--  2.0 unx     1969 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/video_classifier/model.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/whisper/__init__.py
--rw-r--r--  2.0 unx    10188 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/whisper/app.py
--rw-r--r--  2.0 unx     1302 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/whisper/demo.py
--rw-r--r--  2.0 unx    14119 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/whisper/model.py
--rw-r--r--  2.0 unx     2848 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/whisper/test_utils.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/yolo/__init__.py
--rw-r--r--  2.0 unx     7413 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/yolo/app.py
--rw-r--r--  2.0 unx     2432 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/yolo/demo.py
--rw-r--r--  2.0 unx     6081 b- defN 24-Apr-30 21:08 qai_hub_models/models/_shared/yolo/utils.py
--rw-r--r--  2.0 unx      450 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/__init__.py
--rw-r--r--  2.0 unx     1405 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/conftest.py
--rw-r--r--  2.0 unx      597 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/demo.py
--rw-r--r--  2.0 unx     8437 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/export.py
--rw-r--r--  2.0 unx     1023 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/info.yaml
--rw-r--r--  2.0 unx     4838 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/model.py
--rw-r--r--  2.0 unx     4545 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/perf.yaml
--rw-r--r--  2.0 unx     2006 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/test.py
--rw-r--r--  2.0 unx      532 b- defN 24-Apr-30 21:08 qai_hub_models/models/aotgan/patches/layer_norm.diff
--rw-r--r--  2.0 unx     1987 b- defN 24-Apr-30 21:08 qai_hub_models/models/baichuan_7b_quantized/info.yaml
--rw-r--r--  2.0 unx     2038 b- defN 24-Apr-30 21:08 qai_hub_models/models/baichuan_7b_quantized/perf.yaml
--rw-r--r--  2.0 unx      559 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/__init__.py
--rw-r--r--  2.0 unx     9705 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/app.py
--rw-r--r--  2.0 unx     6397 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/demo.py
--rw-r--r--  2.0 unx     7894 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/export.py
--rw-r--r--  2.0 unx     1329 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/info.yaml
--rw-r--r--  2.0 unx     5426 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/model.py
--rw-r--r--  2.0 unx     8427 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/perf.yaml
--rw-r--r--  2.0 unx       46 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/requirements.txt
--rw-r--r--  2.0 unx     1556 b- defN 24-Apr-30 21:08 qai_hub_models/models/controlnet_quantized/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/__init__.py
--rw-r--r--  2.0 unx     1318 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/conftest.py
--rw-r--r--  2.0 unx      543 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/demo.py
--rw-r--r--  2.0 unx     8196 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/export.py
--rw-r--r--  2.0 unx     1287 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/info.yaml
--rw-r--r--  2.0 unx      708 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/model.py
--rw-r--r--  2.0 unx     2707 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/perf.yaml
--rw-r--r--  2.0 unx      857 b- defN 24-Apr-30 21:08 qai_hub_models/models/convnext_tiny/test.py
--rw-r--r--  2.0 unx      398 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/__init__.py
--rw-r--r--  2.0 unx     3750 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/app.py
--rw-r--r--  2.0 unx     1412 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/conftest.py
--rw-r--r--  2.0 unx     2128 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/demo.py
--rw-r--r--  2.0 unx     8474 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/export.py
--rw-r--r--  2.0 unx     1334 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/info.yaml
--rw-r--r--  2.0 unx     3831 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/model.py
--rw-r--r--  2.0 unx     3358 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/perf.yaml
--rw-r--r--  2.0 unx     1790 b- defN 24-Apr-30 21:08 qai_hub_models/models/ddrnet23_slim/test.py
--rw-r--r--  2.0 unx      455 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/__init__.py
--rw-r--r--  2.0 unx     1423 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/conftest.py
--rw-r--r--  2.0 unx     1091 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/demo.py
--rw-r--r--  2.0 unx     8498 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/export.py
--rw-r--r--  2.0 unx     1266 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/info.yaml
--rw-r--r--  2.0 unx     2098 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/model.py
--rw-r--r--  2.0 unx     3812 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/perf.yaml
--rw-r--r--  2.0 unx     1862 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet/test.py
--rw-r--r--  2.0 unx      466 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/__init__.py
--rw-r--r--  2.0 unx     1433 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/conftest.py
--rw-r--r--  2.0 unx     1156 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/demo.py
--rw-r--r--  2.0 unx     8914 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/export.py
--rw-r--r--  2.0 unx     1306 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/info.yaml
--rw-r--r--  2.0 unx     2861 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/model.py
--rw-r--r--  2.0 unx     5887 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/perf.yaml
--rw-r--r--  2.0 unx     2192 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/test.py
--rw-r--r--  2.0 unx      451 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/__init__.py
--rw-r--r--  2.0 unx     1417 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/conftest.py
--rw-r--r--  2.0 unx     1077 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/demo.py
--rw-r--r--  2.0 unx     8492 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/export.py
--rw-r--r--  2.0 unx     1285 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/info.yaml
--rw-r--r--  2.0 unx     1581 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/model.py
--rw-r--r--  2.0 unx     3781 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/perf.yaml
--rw-r--r--  2.0 unx     1766 b- defN 24-Apr-30 21:08 qai_hub_models/models/deeplabv3_resnet50/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/__init__.py
--rw-r--r--  2.0 unx     1316 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/demo.py
--rw-r--r--  2.0 unx     8169 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/export.py
--rw-r--r--  2.0 unx     1310 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/info.yaml
--rw-r--r--  2.0 unx      698 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/model.py
--rw-r--r--  2.0 unx     4525 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/perf.yaml
--rw-r--r--  2.0 unx      841 b- defN 24-Apr-30 21:08 qai_hub_models/models/densenet121/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/__init__.py
--rw-r--r--  2.0 unx     1319 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/conftest.py
--rw-r--r--  2.0 unx      896 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/demo.py
--rw-r--r--  2.0 unx     8186 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/export.py
--rw-r--r--  2.0 unx     1188 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/info.yaml
--rw-r--r--  2.0 unx      661 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/model.py
--rw-r--r--  2.0 unx     3401 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/requirements.txt
--rw-r--r--  2.0 unx     1316 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/__init__.py
--rw-r--r--  2.0 unx     1323 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/conftest.py
--rw-r--r--  2.0 unx      906 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/demo.py
--rw-r--r--  2.0 unx     8202 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/export.py
--rw-r--r--  2.0 unx     1215 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/info.yaml
--rw-r--r--  2.0 unx      668 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/model.py
--rw-r--r--  2.0 unx     3420 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/requirements.txt
--rw-r--r--  2.0 unx     1373 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet101_dc5/test.py
--rw-r--r--  2.0 unx      481 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/__init__.py
--rw-r--r--  2.0 unx     1318 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/conftest.py
--rw-r--r--  2.0 unx      893 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/demo.py
--rw-r--r--  2.0 unx     8182 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/export.py
--rw-r--r--  2.0 unx     1185 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/info.yaml
--rw-r--r--  2.0 unx      659 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/model.py
--rw-r--r--  2.0 unx     3406 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/requirements.txt
--rw-r--r--  2.0 unx     1636 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50/test.py
--rw-r--r--  2.0 unx      488 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/__init__.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/conftest.py
--rw-r--r--  2.0 unx      903 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/demo.py
--rw-r--r--  2.0 unx     8198 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/export.py
--rw-r--r--  2.0 unx     1212 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/info.yaml
--rw-r--r--  2.0 unx      666 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/model.py
--rw-r--r--  2.0 unx     3413 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/perf.yaml
--rw-r--r--  2.0 unx       34 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/requirements.txt
--rw-r--r--  2.0 unx     1344 b- defN 24-Apr-30 21:08 qai_hub_models/models/detr_resnet50_dc5/test.py
--rw-r--r--  2.0 unx      477 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/__init__.py
--rw-r--r--  2.0 unx     1320 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/conftest.py
--rw-r--r--  2.0 unx      549 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/demo.py
--rw-r--r--  2.0 unx     8184 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/export.py
--rw-r--r--  2.0 unx     1361 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/info.yaml
--rw-r--r--  2.0 unx      714 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/model.py
--rw-r--r--  2.0 unx     4559 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/perf.yaml
--rw-r--r--  2.0 unx      867 b- defN 24-Apr-30 21:08 qai_hub_models/models/efficientnet_b0/test.py
--rw-r--r--  2.0 unx      463 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/__init__.py
--rw-r--r--  2.0 unx     1405 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/conftest.py
--rw-r--r--  2.0 unx      939 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/demo.py
--rw-r--r--  2.0 unx     8283 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/export.py
--rw-r--r--  2.0 unx     1117 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/info.yaml
--rw-r--r--  2.0 unx     3473 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/model.py
--rw-r--r--  2.0 unx     4597 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/perf.yaml
--rw-r--r--  2.0 unx     1831 b- defN 24-Apr-30 21:08 qai_hub_models/models/esrgan/test.py
--rw-r--r--  2.0 unx      418 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/__init__.py
--rw-r--r--  2.0 unx     3207 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/app.py
--rw-r--r--  2.0 unx     1416 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/conftest.py
--rw-r--r--  2.0 unx     3172 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/demo.py
--rw-r--r--  2.0 unx     7951 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/export.py
--rw-r--r--  2.0 unx     1070 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/info.yaml
--rw-r--r--  2.0 unx     2414 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/model.py
--rw-r--r--  2.0 unx     3431 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/perf.yaml
--rw-r--r--  2.0 unx       74 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/requirements.txt
--rw-r--r--  2.0 unx     2492 b- defN 24-Apr-30 21:08 qai_hub_models/models/facebook_denoiser/test.py
--rw-r--r--  2.0 unx      440 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/conftest.py
--rw-r--r--  2.0 unx      762 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/demo.py
--rw-r--r--  2.0 unx     8545 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/export.py
--rw-r--r--  2.0 unx     1301 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/info.yaml
--rw-r--r--  2.0 unx      683 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/model.py
--rw-r--r--  2.0 unx     3402 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/perf.yaml
--rw-r--r--  2.0 unx       64 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/requirements.txt
--rw-r--r--  2.0 unx     1332 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_s/test.py
--rw-r--r--  2.0 unx      440 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/conftest.py
--rw-r--r--  2.0 unx      762 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/demo.py
--rw-r--r--  2.0 unx     8545 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/export.py
--rw-r--r--  2.0 unx     1300 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/info.yaml
--rw-r--r--  2.0 unx      683 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/model.py
--rw-r--r--  2.0 unx     3409 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/perf.yaml
--rw-r--r--  2.0 unx       64 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/requirements.txt
--rw-r--r--  2.0 unx     1332 b- defN 24-Apr-30 21:08 qai_hub_models/models/fastsam_x/test.py
--rw-r--r--  2.0 unx      410 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/__init__.py
--rw-r--r--  2.0 unx     2683 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/app.py
--rw-r--r--  2.0 unx     1411 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/conftest.py
--rw-r--r--  2.0 unx     2315 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/demo.py
--rw-r--r--  2.0 unx     8450 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/export.py
--rw-r--r--  2.0 unx     1241 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/info.yaml
--rw-r--r--  2.0 unx     1989 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/model.py
--rw-r--r--  2.0 unx     4576 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/perf.yaml
--rw-r--r--  2.0 unx     1637 b- defN 24-Apr-30 21:08 qai_hub_models/models/fcn_resnet50/test.py
--rw-r--r--  2.0 unx      487 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/__init__.py
--rw-r--r--  2.0 unx     1417 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/conftest.py
--rw-r--r--  2.0 unx      607 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/demo.py
--rw-r--r--  2.0 unx     8331 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/export.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/info.yaml
--rw-r--r--  2.0 unx      648 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/model.py
--rw-r--r--  2.0 unx     4585 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/requirements.txt
--rw-r--r--  2.0 unx      804 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_122ns_lowres/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/__init__.py
--rw-r--r--  2.0 unx     1408 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/conftest.py
--rw-r--r--  2.0 unx      582 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/demo.py
--rw-r--r--  2.0 unx     8295 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/export.py
--rw-r--r--  2.0 unx     1298 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/info.yaml
--rw-r--r--  2.0 unx      580 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/model.py
--rw-r--r--  2.0 unx     4571 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/requirements.txt
--rw-r--r--  2.0 unx      746 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/__init__.py
--rw-r--r--  2.0 unx     1418 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/conftest.py
--rw-r--r--  2.0 unx      627 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/demo.py
--rw-r--r--  2.0 unx     8731 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/export.py
--rw-r--r--  2.0 unx     1347 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/info.yaml
--rw-r--r--  2.0 unx     1169 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/model.py
--rw-r--r--  2.0 unx     5112 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/requirements.txt
--rw-r--r--  2.0 unx      840 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_40s_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/__init__.py
--rw-r--r--  2.0 unx     1408 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/conftest.py
--rw-r--r--  2.0 unx      582 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/demo.py
--rw-r--r--  2.0 unx     8295 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/export.py
--rw-r--r--  2.0 unx     1275 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/info.yaml
--rw-r--r--  2.0 unx      580 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/model.py
--rw-r--r--  2.0 unx     4591 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/requirements.txt
--rw-r--r--  2.0 unx      746 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/__init__.py
--rw-r--r--  2.0 unx     1418 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/conftest.py
--rw-r--r--  2.0 unx      627 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/demo.py
--rw-r--r--  2.0 unx     8731 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/export.py
--rw-r--r--  2.0 unx     1347 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/info.yaml
--rw-r--r--  2.0 unx     1156 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/model.py
--rw-r--r--  2.0 unx     5127 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/requirements.txt
--rw-r--r--  2.0 unx      840 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_54s_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/__init__.py
--rw-r--r--  2.0 unx     1408 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/conftest.py
--rw-r--r--  2.0 unx      582 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/demo.py
--rw-r--r--  2.0 unx     8295 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/export.py
--rw-r--r--  2.0 unx     1279 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/info.yaml
--rw-r--r--  2.0 unx      580 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/model.py
--rw-r--r--  2.0 unx     4585 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/requirements.txt
--rw-r--r--  2.0 unx      746 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s/test.py
--rw-r--r--  2.0 unx      485 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/__init__.py
--rw-r--r--  2.0 unx     1415 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/conftest.py
--rw-r--r--  2.0 unx      601 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/demo.py
--rw-r--r--  2.0 unx     8323 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/export.py
--rw-r--r--  2.0 unx     1327 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/info.yaml
--rw-r--r--  2.0 unx      640 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/model.py
--rw-r--r--  2.0 unx     4582 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/requirements.txt
--rw-r--r--  2.0 unx      794 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_lowres/test.py
--rw-r--r--  2.0 unx      490 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/__init__.py
--rw-r--r--  2.0 unx     1418 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/conftest.py
--rw-r--r--  2.0 unx      627 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/demo.py
--rw-r--r--  2.0 unx     8731 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/export.py
--rw-r--r--  2.0 unx     1347 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/info.yaml
--rw-r--r--  2.0 unx     1156 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/model.py
--rw-r--r--  2.0 unx     5123 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/perf.yaml
--rw-r--r--  2.0 unx       21 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/requirements.txt
--rw-r--r--  2.0 unx      840 b- defN 24-Apr-30 21:08 qai_hub_models/models/ffnet_78s_quantized/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/demo.py
--rw-r--r--  2.0 unx     8160 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/export.py
--rw-r--r--  2.0 unx     1295 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/info.yaml
--rw-r--r--  2.0 unx      743 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/model.py
--rw-r--r--  2.0 unx     4547 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/perf.yaml
--rw-r--r--  2.0 unx      840 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet/test.py
--rw-r--r--  2.0 unx      573 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/__init__.py
--rw-r--r--  2.0 unx     1324 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/conftest.py
--rw-r--r--  2.0 unx      578 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/demo.py
--rw-r--r--  2.0 unx     8623 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/export.py
--rw-r--r--  2.0 unx     1325 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/info.yaml
--rw-r--r--  2.0 unx     4298 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/model.py
--rw-r--r--  2.0 unx     6623 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/perf.yaml
--rw-r--r--  2.0 unx      885 b- defN 24-Apr-30 21:08 qai_hub_models/models/googlenet_quantized/test.py
--rw-r--r--  2.0 unx      430 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/__init__.py
--rw-r--r--  2.0 unx     8458 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/app.py
--rw-r--r--  2.0 unx     1409 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/conftest.py
--rw-r--r--  2.0 unx     1739 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/demo.py
--rw-r--r--  2.0 unx     8441 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/export.py
--rw-r--r--  2.0 unx     1195 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/info.yaml
--rw-r--r--  2.0 unx     3360 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/model.py
--rw-r--r--  2.0 unx     4557 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/perf.yaml
--rw-r--r--  2.0 unx       51 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/requirements.txt
--rw-r--r--  2.0 unx     1420 b- defN 24-Apr-30 21:08 qai_hub_models/models/hrnet_pose/test.py
--rw-r--r--  2.0 unx      434 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/__init__.py
--rw-r--r--  2.0 unx     2133 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/app.py
--rw-r--r--  2.0 unx     1426 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py
--rw-r--r--  2.0 unx     1517 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/demo.py
--rw-r--r--  2.0 unx     7848 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/export.py
--rw-r--r--  2.0 unx     1248 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/info.yaml
--rw-r--r--  2.0 unx     7587 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/model.py
--rw-r--r--  2.0 unx     3446 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml
--rw-r--r--  2.0 unx       72 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/requirements.txt
--rw-r--r--  2.0 unx     2560 b- defN 24-Apr-30 21:08 qai_hub_models/models/huggingface_wavlm_base_plus/test.py
--rw-r--r--  2.0 unx      477 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/__init__.py
--rw-r--r--  2.0 unx     1317 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/conftest.py
--rw-r--r--  2.0 unx      546 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/demo.py
--rw-r--r--  2.0 unx     8172 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/export.py
--rw-r--r--  2.0 unx     1359 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/info.yaml
--rw-r--r--  2.0 unx      756 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/model.py
--rw-r--r--  2.0 unx     4565 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/perf.yaml
--rw-r--r--  2.0 unx      861 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3/test.py
--rw-r--r--  2.0 unx      584 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/__init__.py
--rw-r--r--  2.0 unx     1327 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/conftest.py
--rw-r--r--  2.0 unx      591 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/demo.py
--rw-r--r--  2.0 unx     8636 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/export.py
--rw-r--r--  2.0 unx     1556 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/info.yaml
--rw-r--r--  2.0 unx     7762 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/model.py
--rw-r--r--  2.0 unx     5106 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/perf.yaml
--rw-r--r--  2.0 unx      901 b- defN 24-Apr-30 21:08 qai_hub_models/models/inception_v3_quantized/test.py
--rw-r--r--  2.0 unx      455 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/__init__.py
--rw-r--r--  2.0 unx     1411 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/conftest.py
--rw-r--r--  2.0 unx      911 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/demo.py
--rw-r--r--  2.0 unx     8460 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/export.py
--rw-r--r--  2.0 unx     1082 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/info.yaml
--rw-r--r--  2.0 unx     4977 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/model.py
--rw-r--r--  2.0 unx     4545 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/perf.yaml
--rw-r--r--  2.0 unx      171 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/requirements.txt
--rw-r--r--  2.0 unx     2074 b- defN 24-Apr-30 21:08 qai_hub_models/models/lama_dilated/test.py
--rw-r--r--  2.0 unx      404 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/__init__.py
--rw-r--r--  2.0 unx     3986 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/app.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/conftest.py
--rw-r--r--  2.0 unx     2072 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/demo.py
--rw-r--r--  2.0 unx     7919 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/export.py
--rw-r--r--  2.0 unx     1112 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/info.yaml
--rw-r--r--  2.0 unx     3788 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/model.py
--rw-r--r--  2.0 unx     3366 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/perf.yaml
--rw-r--r--  2.0 unx       39 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/requirements.txt
--rw-r--r--  2.0 unx     1714 b- defN 24-Apr-30 21:08 qai_hub_models/models/litehrnet/test.py
--rw-r--r--  2.0 unx     1997 b- defN 24-Apr-30 21:08 qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml
--rw-r--r--  2.0 unx     2039 b- defN 24-Apr-30 21:08 qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml
--rw-r--r--  2.0 unx      412 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/__init__.py
--rw-r--r--  2.0 unx     2099 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/app.py
--rw-r--r--  2.0 unx     1413 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/conftest.py
--rw-r--r--  2.0 unx     2862 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/demo.py
--rw-r--r--  2.0 unx    10089 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/export.py
--rw-r--r--  2.0 unx     1469 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/info.yaml
--rw-r--r--  2.0 unx     7669 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/model.py
--rw-r--r--  2.0 unx     8427 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/perf.yaml
--rw-r--r--  2.0 unx     1441 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_face/test.py
--rw-r--r--  2.0 unx      412 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/__init__.py
--rw-r--r--  2.0 unx     9819 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/app.py
--rw-r--r--  2.0 unx     1413 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/conftest.py
--rw-r--r--  2.0 unx     2832 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/demo.py
--rw-r--r--  2.0 unx    10044 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/export.py
--rw-r--r--  2.0 unx     1374 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/info.yaml
--rw-r--r--  2.0 unx     6017 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/model.py
--rw-r--r--  2.0 unx     8437 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/perf.yaml
--rw-r--r--  2.0 unx     1442 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_hand/test.py
--rw-r--r--  2.0 unx      412 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/__init__.py
--rw-r--r--  2.0 unx     4430 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/app.py
--rw-r--r--  2.0 unx     1413 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/conftest.py
--rw-r--r--  2.0 unx     2889 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/demo.py
--rw-r--r--  2.0 unx    10045 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/export.py
--rw-r--r--  2.0 unx     1387 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/info.yaml
--rw-r--r--  2.0 unx     5801 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/model.py
--rw-r--r--  2.0 unx     8435 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/perf.yaml
--rw-r--r--  2.0 unx     1443 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_pose/test.py
--rw-r--r--  2.0 unx      362 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/__init__.py
--rw-r--r--  2.0 unx     1411 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/app.py
--rw-r--r--  2.0 unx     1321 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/conftest.py
--rw-r--r--  2.0 unx     2674 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/demo.py
--rw-r--r--  2.0 unx     8479 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/export.py
--rw-r--r--  2.0 unx     1455 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/info.yaml
--rw-r--r--  2.0 unx    12352 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/model.py
--rw-r--r--  2.0 unx     4583 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/perf.yaml
--rw-r--r--  2.0 unx       15 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/requirements.txt
--rw-r--r--  2.0 unx     1396 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/test.py
--rw-r--r--  2.0 unx     2492 b- defN 24-Apr-30 21:08 qai_hub_models/models/mediapipe_selfie/utils.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/demo.py
--rw-r--r--  2.0 unx     8160 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/export.py
--rw-r--r--  2.0 unx     1333 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/info.yaml
--rw-r--r--  2.0 unx      699 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/model.py
--rw-r--r--  2.0 unx     4528 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/perf.yaml
--rw-r--r--  2.0 unx      882 b- defN 24-Apr-30 21:08 qai_hub_models/models/mnasnet05/test.py
--rw-r--r--  2.0 unx      474 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/__init__.py
--rw-r--r--  2.0 unx     1411 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/conftest.py
--rw-r--r--  2.0 unx      540 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/demo.py
--rw-r--r--  2.0 unx     8172 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/export.py
--rw-r--r--  2.0 unx     1380 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/info.yaml
--rw-r--r--  2.0 unx     2457 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/model.py
--rw-r--r--  2.0 unx     4553 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/perf.yaml
--rw-r--r--  2.0 unx     1091 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2/test.py
--rw-r--r--  2.0 unx      485 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/__init__.py
--rw-r--r--  2.0 unx     1421 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/conftest.py
--rw-r--r--  2.0 unx      585 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/demo.py
--rw-r--r--  2.0 unx     8636 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/export.py
--rw-r--r--  2.0 unx     1362 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/info.yaml
--rw-r--r--  2.0 unx     3429 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/model.py
--rw-r--r--  2.0 unx     6626 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/perf.yaml
--rw-r--r--  2.0 unx     1002 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v2_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/__init__.py
--rw-r--r--  2.0 unx     1323 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/conftest.py
--rw-r--r--  2.0 unx      556 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/demo.py
--rw-r--r--  2.0 unx     8216 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/export.py
--rw-r--r--  2.0 unx     1340 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/info.yaml
--rw-r--r--  2.0 unx      721 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/model.py
--rw-r--r--  2.0 unx     3392 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/perf.yaml
--rw-r--r--  2.0 unx      879 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large/test.py
--rw-r--r--  2.0 unx      607 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/__init__.py
--rw-r--r--  2.0 unx     1333 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py
--rw-r--r--  2.0 unx      748 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/demo.py
--rw-r--r--  2.0 unx     8368 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/export.py
--rw-r--r--  2.0 unx     1374 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/info.yaml
--rw-r--r--  2.0 unx     3075 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/model.py
--rw-r--r--  2.0 unx     5123 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml
--rw-r--r--  2.0 unx      917 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_large_quantized/test.py
--rw-r--r--  2.0 unx      479 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/__init__.py
--rw-r--r--  2.0 unx     1323 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/conftest.py
--rw-r--r--  2.0 unx      556 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/demo.py
--rw-r--r--  2.0 unx     8216 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/export.py
--rw-r--r--  2.0 unx     1338 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/info.yaml
--rw-r--r--  2.0 unx      721 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/model.py
--rw-r--r--  2.0 unx     3396 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/perf.yaml
--rw-r--r--  2.0 unx      879 b- defN 24-Apr-30 21:08 qai_hub_models/models/mobilenet_v3_small/test.py
--rw-r--r--  2.0 unx      394 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/__init__.py
--rw-r--r--  2.0 unx     3958 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/app.py
--rw-r--r--  2.0 unx     1410 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/conftest.py
--rw-r--r--  2.0 unx     3404 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/demo.py
--rw-r--r--  2.0 unx    10000 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/export.py
--rw-r--r--  2.0 unx     1494 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/info.yaml
--rw-r--r--  2.0 unx     5374 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/model.py
--rw-r--r--  2.0 unx     6070 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/perf.yaml
--rw-r--r--  2.0 unx       29 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/requirements.txt
--rw-r--r--  2.0 unx     2118 b- defN 24-Apr-30 21:08 qai_hub_models/models/openai_clip/test.py
--rw-r--r--  2.0 unx      402 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/__init__.py
--rw-r--r--  2.0 unx    12008 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/app.py
--rw-r--r--  2.0 unx     1407 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/conftest.py
--rw-r--r--  2.0 unx     2053 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/demo.py
--rw-r--r--  2.0 unx     8452 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/export.py
--rw-r--r--  2.0 unx     1246 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/info.yaml
--rw-r--r--  2.0 unx     5084 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/model.py
--rw-r--r--  2.0 unx     4572 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/perf.yaml
--rw-r--r--  2.0 unx       31 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/requirements.txt
--rw-r--r--  2.0 unx     1321 b- defN 24-Apr-30 21:08 qai_hub_models/models/openpose/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/__init__.py
--rw-r--r--  2.0 unx     1414 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/conftest.py
--rw-r--r--  2.0 unx      972 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/demo.py
--rw-r--r--  2.0 unx     8462 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/export.py
--rw-r--r--  2.0 unx     1248 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/info.yaml
--rw-r--r--  2.0 unx     3170 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/model.py
--rw-r--r--  2.0 unx     4555 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/perf.yaml
--rw-r--r--  2.0 unx     1422 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/__init__.py
--rw-r--r--  2.0 unx     1424 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/conftest.py
--rw-r--r--  2.0 unx      891 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/demo.py
--rw-r--r--  2.0 unx     8898 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/export.py
--rw-r--r--  2.0 unx     1274 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/info.yaml
--rw-r--r--  2.0 unx     4587 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/model.py
--rw-r--r--  2.0 unx     3932 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml
--rw-r--r--  2.0 unx     2921 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetlarge_quantized/test.py
--rw-r--r--  2.0 unx      473 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/__init__.py
--rw-r--r--  2.0 unx     1415 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/conftest.py
--rw-r--r--  2.0 unx      976 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/demo.py
--rw-r--r--  2.0 unx     8466 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/export.py
--rw-r--r--  2.0 unx     1242 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/info.yaml
--rw-r--r--  2.0 unx     3177 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/model.py
--rw-r--r--  2.0 unx     4554 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/perf.yaml
--rw-r--r--  2.0 unx     1428 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium/test.py
--rw-r--r--  2.0 unx      484 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/__init__.py
--rw-r--r--  2.0 unx     1425 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/conftest.py
--rw-r--r--  2.0 unx      900 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/demo.py
--rw-r--r--  2.0 unx     8902 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/export.py
--rw-r--r--  2.0 unx     1282 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/info.yaml
--rw-r--r--  2.0 unx     4597 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/model.py
--rw-r--r--  2.0 unx     3935 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml
--rw-r--r--  2.0 unx     2912 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetmedium_quantized/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/__init__.py
--rw-r--r--  2.0 unx     1414 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/conftest.py
--rw-r--r--  2.0 unx      972 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/demo.py
--rw-r--r--  2.0 unx     8462 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/export.py
--rw-r--r--  2.0 unx     1238 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/info.yaml
--rw-r--r--  2.0 unx     3170 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/model.py
--rw-r--r--  2.0 unx     4546 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/perf.yaml
--rw-r--r--  2.0 unx     1422 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/__init__.py
--rw-r--r--  2.0 unx     1424 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/conftest.py
--rw-r--r--  2.0 unx      891 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/demo.py
--rw-r--r--  2.0 unx     8898 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/export.py
--rw-r--r--  2.0 unx     1278 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/info.yaml
--rw-r--r--  2.0 unx     4577 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/model.py
--rw-r--r--  2.0 unx     3932 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml
--rw-r--r--  2.0 unx     2859 b- defN 24-Apr-30 21:08 qai_hub_models/models/quicksrnetsmall_quantized/test.py
--rw-r--r--  2.0 unx      481 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/__init__.py
--rw-r--r--  2.0 unx     1423 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/conftest.py
--rw-r--r--  2.0 unx     1280 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/demo.py
--rw-r--r--  2.0 unx     8498 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/export.py
--rw-r--r--  2.0 unx     1206 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/info.yaml
--rw-r--r--  2.0 unx     5435 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/model.py
--rw-r--r--  2.0 unx     4580 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml
--rw-r--r--  2.0 unx       44 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/requirements.txt
--rw-r--r--  2.0 unx     1480 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_general_x4v3/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/__init__.py
--rw-r--r--  2.0 unx     1417 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/conftest.py
--rw-r--r--  2.0 unx     1256 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/demo.py
--rw-r--r--  2.0 unx     7935 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/export.py
--rw-r--r--  2.0 unx     1330 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/info.yaml
--rw-r--r--  2.0 unx     4443 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/model.py
--rw-r--r--  2.0 unx     3412 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/perf.yaml
--rw-r--r--  2.0 unx       44 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/requirements.txt
--rw-r--r--  2.0 unx     1440 b- defN 24-Apr-30 21:08 qai_hub_models/models/real_esrgan_x4plus/test.py
--rw-r--r--  2.0 unx      469 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/__init__.py
--rw-r--r--  2.0 unx     1311 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/conftest.py
--rw-r--r--  2.0 unx      524 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/demo.py
--rw-r--r--  2.0 unx     8148 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/export.py
--rw-r--r--  2.0 unx     1291 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/info.yaml
--rw-r--r--  2.0 unx      635 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/model.py
--rw-r--r--  2.0 unx     4558 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/perf.yaml
--rw-r--r--  2.0 unx      984 b- defN 24-Apr-30 21:08 qai_hub_models/models/regnet/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/demo.py
--rw-r--r--  2.0 unx     8160 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/export.py
--rw-r--r--  2.0 unx     1312 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/info.yaml
--rw-r--r--  2.0 unx      609 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/model.py
--rw-r--r--  2.0 unx     4570 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/perf.yaml
--rw-r--r--  2.0 unx      961 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/__init__.py
--rw-r--r--  2.0 unx     1324 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/conftest.py
--rw-r--r--  2.0 unx      578 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/demo.py
--rw-r--r--  2.0 unx     8623 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/export.py
--rw-r--r--  2.0 unx     1346 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/info.yaml
--rw-r--r--  2.0 unx     3210 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/model.py
--rw-r--r--  2.0 unx     6649 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/perf.yaml
--rw-r--r--  2.0 unx      921 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet101_quantized/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/__init__.py
--rw-r--r--  2.0 unx     1313 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/conftest.py
--rw-r--r--  2.0 unx      530 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/demo.py
--rw-r--r--  2.0 unx     8156 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/export.py
--rw-r--r--  2.0 unx     1310 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/info.yaml
--rw-r--r--  2.0 unx      607 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/model.py
--rw-r--r--  2.0 unx     4539 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/perf.yaml
--rw-r--r--  2.0 unx      955 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18/test.py
--rw-r--r--  2.0 unx      482 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/__init__.py
--rw-r--r--  2.0 unx     1323 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/conftest.py
--rw-r--r--  2.0 unx      562 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/demo.py
--rw-r--r--  2.0 unx     8619 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/export.py
--rw-r--r--  2.0 unx     1343 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/info.yaml
--rw-r--r--  2.0 unx     3012 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/model.py
--rw-r--r--  2.0 unx     6614 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/perf.yaml
--rw-r--r--  2.0 unx      917 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet18_quantized/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/__init__.py
--rw-r--r--  2.0 unx     1313 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/conftest.py
--rw-r--r--  2.0 unx      530 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/demo.py
--rw-r--r--  2.0 unx     8156 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/export.py
--rw-r--r--  2.0 unx     1303 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/info.yaml
--rw-r--r--  2.0 unx      607 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/model.py
--rw-r--r--  2.0 unx     4553 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/perf.yaml
--rw-r--r--  2.0 unx      955 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnet50/test.py
--rw-r--r--  2.0 unx      473 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/__init__.py
--rw-r--r--  2.0 unx     1315 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/conftest.py
--rw-r--r--  2.0 unx      536 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/demo.py
--rw-r--r--  2.0 unx     8164 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/export.py
--rw-r--r--  2.0 unx     1324 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/info.yaml
--rw-r--r--  2.0 unx      617 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/model.py
--rw-r--r--  2.0 unx     4568 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/perf.yaml
--rw-r--r--  2.0 unx      897 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101/test.py
--rw-r--r--  2.0 unx      484 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/__init__.py
--rw-r--r--  2.0 unx     1325 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/conftest.py
--rw-r--r--  2.0 unx      581 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/demo.py
--rw-r--r--  2.0 unx     8647 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/export.py
--rw-r--r--  2.0 unx     1365 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/info.yaml
--rw-r--r--  2.0 unx     3017 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/model.py
--rw-r--r--  2.0 unx     5115 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/perf.yaml
--rw-r--r--  2.0 unx      925 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext101_quantized/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/conftest.py
--rw-r--r--  2.0 unx      533 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/demo.py
--rw-r--r--  2.0 unx     8160 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/export.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/info.yaml
--rw-r--r--  2.0 unx      704 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/model.py
--rw-r--r--  2.0 unx     4554 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/perf.yaml
--rw-r--r--  2.0 unx      840 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50/test.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/__init__.py
--rw-r--r--  2.0 unx     1324 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/conftest.py
--rw-r--r--  2.0 unx      578 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/demo.py
--rw-r--r--  2.0 unx     8643 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/export.py
--rw-r--r--  2.0 unx     1362 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/info.yaml
--rw-r--r--  2.0 unx     3008 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/model.py
--rw-r--r--  2.0 unx     5084 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/perf.yaml
--rw-r--r--  2.0 unx      921 b- defN 24-Apr-30 21:08 qai_hub_models/models/resnext50_quantized/test.py
--rw-r--r--  2.0 unx      404 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/__init__.py
--rw-r--r--  2.0 unx     5101 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/app.py
--rw-r--r--  2.0 unx     1402 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/conftest.py
--rw-r--r--  2.0 unx     3088 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/demo.py
--rw-r--r--  2.0 unx    10468 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/export.py
--rw-r--r--  2.0 unx     1391 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/info.yaml
--rw-r--r--  2.0 unx    12000 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/model.py
--rw-r--r--  2.0 unx     3411 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/perf.yaml
--rw-r--r--  2.0 unx       37 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/requirements.txt
--rw-r--r--  2.0 unx     3062 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/test.py
--rw-r--r--  2.0 unx      826 b- defN 24-Apr-30 21:08 qai_hub_models/models/sam/utils.py
--rw-r--r--  2.0 unx      464 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/__init__.py
--rw-r--r--  2.0 unx     1406 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/conftest.py
--rw-r--r--  2.0 unx      923 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/demo.py
--rw-r--r--  2.0 unx     8287 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/export.py
--rw-r--r--  2.0 unx     1104 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/info.yaml
--rw-r--r--  2.0 unx     2984 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/model.py
--rw-r--r--  2.0 unx     4547 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/perf.yaml
--rw-r--r--  2.0 unx     1471 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/__init__.py
--rw-r--r--  2.0 unx     1416 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/conftest.py
--rw-r--r--  2.0 unx      990 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/demo.py
--rw-r--r--  2.0 unx     8431 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/export.py
--rw-r--r--  2.0 unx     1151 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/info.yaml
--rw-r--r--  2.0 unx     4269 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/model.py
--rw-r--r--  2.0 unx     3925 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/perf.yaml
--rw-r--r--  2.0 unx     2927 b- defN 24-Apr-30 21:08 qai_hub_models/models/sesr_m5_quantized/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/__init__.py
--rw-r--r--  2.0 unx     1318 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/conftest.py
--rw-r--r--  2.0 unx      543 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/demo.py
--rw-r--r--  2.0 unx     8176 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/export.py
--rw-r--r--  2.0 unx     1353 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/info.yaml
--rw-r--r--  2.0 unx      713 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/model.py
--rw-r--r--  2.0 unx     4561 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/perf.yaml
--rw-r--r--  2.0 unx      857 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2/test.py
--rw-r--r--  2.0 unx      584 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/__init__.py
--rw-r--r--  2.0 unx     1328 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/conftest.py
--rw-r--r--  2.0 unx      588 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/demo.py
--rw-r--r--  2.0 unx     8639 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/export.py
--rw-r--r--  2.0 unx     1383 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/info.yaml
--rw-r--r--  2.0 unx     5989 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/model.py
--rw-r--r--  2.0 unx     5481 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/perf.yaml
--rw-r--r--  2.0 unx      899 b- defN 24-Apr-30 21:08 qai_hub_models/models/shufflenet_v2_quantized/test.py
--rw-r--r--  2.0 unx      396 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/__init__.py
--rw-r--r--  2.0 unx     3793 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/app.py
--rw-r--r--  2.0 unx     1404 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/conftest.py
--rw-r--r--  2.0 unx     1657 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/demo.py
--rw-r--r--  2.0 unx     8422 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/export.py
--rw-r--r--  2.0 unx     1260 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/info.yaml
--rw-r--r--  2.0 unx     4770 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/model.py
--rw-r--r--  2.0 unx     4518 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/perf.yaml
--rw-r--r--  2.0 unx     1355 b- defN 24-Apr-30 21:08 qai_hub_models/models/sinet/test.py
--rw-r--r--  2.0 unx      473 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/__init__.py
--rw-r--r--  2.0 unx     1318 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/conftest.py
--rw-r--r--  2.0 unx      539 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/demo.py
--rw-r--r--  2.0 unx     8177 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/export.py
--rw-r--r--  2.0 unx     1325 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/info.yaml
--rw-r--r--  2.0 unx      696 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/model.py
--rw-r--r--  2.0 unx     4551 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/perf.yaml
--rw-r--r--  2.0 unx      851 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1/test.py
--rw-r--r--  2.0 unx      582 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/__init__.py
--rw-r--r--  2.0 unx     1328 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/conftest.py
--rw-r--r--  2.0 unx      584 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/demo.py
--rw-r--r--  2.0 unx     8592 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/export.py
--rw-r--r--  2.0 unx     1358 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/info.yaml
--rw-r--r--  2.0 unx     3023 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/model.py
--rw-r--r--  2.0 unx     6622 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/perf.yaml
--rw-r--r--  2.0 unx      895 b- defN 24-Apr-30 21:08 qai_hub_models/models/squeezenet1_1_quantized/test.py
--rw-r--r--  2.0 unx      540 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/__init__.py
--rw-r--r--  2.0 unx     7966 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/app.py
--rw-r--r--  2.0 unx     5765 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/demo.py
--rw-r--r--  2.0 unx     7704 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/export.py
--rw-r--r--  2.0 unx     1355 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/info.yaml
--rw-r--r--  2.0 unx     3604 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/model.py
--rw-r--r--  2.0 unx     6384 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/perf.yaml
--rw-r--r--  2.0 unx       46 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/requirements.txt
--rw-r--r--  2.0 unx     1599 b- defN 24-Apr-30 21:08 qai_hub_models/models/stable_diffusion_quantized/test.py
--rw-r--r--  2.0 unx      404 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/__init__.py
--rw-r--r--  2.0 unx     4155 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/app.py
--rw-r--r--  2.0 unx     1408 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/conftest.py
--rw-r--r--  2.0 unx     2847 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/demo.py
--rw-r--r--  2.0 unx     8043 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/export.py
--rw-r--r--  2.0 unx     1084 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/info.yaml
--rw-r--r--  2.0 unx     8406 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/model.py
--rw-r--r--  2.0 unx     3385 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/perf.yaml
--rw-r--r--  2.0 unx       11 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/requirements.txt
--rw-r--r--  2.0 unx     2497 b- defN 24-Apr-30 21:08 qai_hub_models/models/stylegan2/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/conftest.py
--rw-r--r--  2.0 unx      531 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/demo.py
--rw-r--r--  2.0 unx     8180 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/export.py
--rw-r--r--  2.0 unx     1383 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/info.yaml
--rw-r--r--  2.0 unx     1241 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/model.py
--rw-r--r--  2.0 unx     3405 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/perf.yaml
--rw-r--r--  2.0 unx     1358 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_base/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/__init__.py
--rw-r--r--  2.0 unx     1315 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/conftest.py
--rw-r--r--  2.0 unx      534 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/demo.py
--rw-r--r--  2.0 unx     8184 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/export.py
--rw-r--r--  2.0 unx     1378 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/info.yaml
--rw-r--r--  2.0 unx     1242 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/model.py
--rw-r--r--  2.0 unx     3404 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/perf.yaml
--rw-r--r--  2.0 unx     1364 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_small/test.py
--rw-r--r--  2.0 unx      471 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/__init__.py
--rw-r--r--  2.0 unx     1314 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/conftest.py
--rw-r--r--  2.0 unx      531 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/demo.py
--rw-r--r--  2.0 unx     8180 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/export.py
--rw-r--r--  2.0 unx     1376 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/info.yaml
--rw-r--r--  2.0 unx     1241 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/model.py
--rw-r--r--  2.0 unx     3398 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/perf.yaml
--rw-r--r--  2.0 unx     1476 b- defN 24-Apr-30 21:08 qai_hub_models/models/swin_tiny/test.py
--rw-r--r--  2.0 unx      396 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/__init__.py
--rw-r--r--  2.0 unx    10207 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/app.py
--rw-r--r--  2.0 unx     1310 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/conftest.py
--rw-r--r--  2.0 unx     1779 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/demo.py
--rw-r--r--  2.0 unx     9944 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/export.py
--rw-r--r--  2.0 unx     1370 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/info.yaml
--rw-r--r--  2.0 unx    10482 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/model.py
--rw-r--r--  2.0 unx     6080 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/perf.yaml
--rw-r--r--  2.0 unx       42 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/requirements.txt
--rw-r--r--  2.0 unx     2357 b- defN 24-Apr-30 21:08 qai_hub_models/models/trocr/test.py
--rw-r--r--  2.0 unx      348 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/__init__.py
--rw-r--r--  2.0 unx     1305 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/app.py
--rw-r--r--  2.0 unx     1322 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/conftest.py
--rw-r--r--  2.0 unx     2509 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/demo.py
--rw-r--r--  2.0 unx     8470 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/export.py
--rw-r--r--  2.0 unx     1310 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/info.yaml
--rw-r--r--  2.0 unx     2666 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/model.py
--rw-r--r--  2.0 unx     4595 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/perf.yaml
--rw-r--r--  2.0 unx     1215 b- defN 24-Apr-30 21:08 qai_hub_models/models/unet_segmentation/test.py
--rw-r--r--  2.0 unx      466 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/__init__.py
--rw-r--r--  2.0 unx     1308 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/conftest.py
--rw-r--r--  2.0 unx      515 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/demo.py
--rw-r--r--  2.0 unx     8189 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/export.py
--rw-r--r--  2.0 unx     1342 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/info.yaml
--rw-r--r--  2.0 unx      685 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/model.py
--rw-r--r--  2.0 unx     3394 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/perf.yaml
--rw-r--r--  2.0 unx      807 b- defN 24-Apr-30 21:08 qai_hub_models/models/vit/test.py
--rw-r--r--  2.0 unx      444 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/__init__.py
--rw-r--r--  2.0 unx     1320 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/conftest.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/demo.py
--rw-r--r--  2.0 unx     9996 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/export.py
--rw-r--r--  2.0 unx     1849 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/info.yaml
--rw-r--r--  2.0 unx      558 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/model.py
--rw-r--r--  2.0 unx     6064 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/perf.yaml
--rw-r--r--  2.0 unx       31 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/requirements.txt
--rw-r--r--  2.0 unx      696 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_base_en/test.py
--rw-r--r--  2.0 unx      445 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/__init__.py
--rw-r--r--  2.0 unx     1321 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/conftest.py
--rw-r--r--  2.0 unx      486 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/demo.py
--rw-r--r--  2.0 unx    10000 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/export.py
--rw-r--r--  2.0 unx     1848 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/info.yaml
--rw-r--r--  2.0 unx      560 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/model.py
--rw-r--r--  2.0 unx     6078 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/perf.yaml
--rw-r--r--  2.0 unx       38 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/requirements.txt
--rw-r--r--  2.0 unx      696 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_small_en/test.py
--rw-r--r--  2.0 unx      444 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/__init__.py
--rw-r--r--  2.0 unx     1320 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/conftest.py
--rw-r--r--  2.0 unx      483 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/demo.py
--rw-r--r--  2.0 unx     9996 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/export.py
--rw-r--r--  2.0 unx     1849 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/info.yaml
--rw-r--r--  2.0 unx      558 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/model.py
--rw-r--r--  2.0 unx     6057 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/perf.yaml
--rw-r--r--  2.0 unx       31 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/requirements.txt
--rw-r--r--  2.0 unx      696 b- defN 24-Apr-30 21:08 qai_hub_models/models/whisper_tiny_en/test.py
--rw-r--r--  2.0 unx      475 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/__init__.py
--rw-r--r--  2.0 unx     1317 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/conftest.py
--rw-r--r--  2.0 unx      542 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/demo.py
--rw-r--r--  2.0 unx     8172 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/export.py
--rw-r--r--  2.0 unx     1298 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/info.yaml
--rw-r--r--  2.0 unx      710 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/model.py
--rw-r--r--  2.0 unx     4567 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/perf.yaml
--rw-r--r--  2.0 unx      855 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50/test.py
--rw-r--r--  2.0 unx      582 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/__init__.py
--rw-r--r--  2.0 unx     1327 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/conftest.py
--rw-r--r--  2.0 unx      587 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/demo.py
--rw-r--r--  2.0 unx     8588 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/export.py
--rw-r--r--  2.0 unx     1333 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/info.yaml
--rw-r--r--  2.0 unx     3214 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/model.py
--rw-r--r--  2.0 unx     6629 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/perf.yaml
--rw-r--r--  2.0 unx      932 b- defN 24-Apr-30 21:08 qai_hub_models/models/wideresnet50_quantized/test.py
--rw-r--r--  2.0 unx      461 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/__init__.py
--rw-r--r--  2.0 unx     1403 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/conftest.py
--rw-r--r--  2.0 unx      742 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/demo.py
--rw-r--r--  2.0 unx     8275 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/export.py
--rw-r--r--  2.0 unx     1156 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/info.yaml
--rw-r--r--  2.0 unx     3403 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/model.py
--rw-r--r--  2.0 unx     4545 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/perf.yaml
--rw-r--r--  2.0 unx     1402 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr/test.py
--rw-r--r--  2.0 unx      472 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/__init__.py
--rw-r--r--  2.0 unx     1413 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/conftest.py
--rw-r--r--  2.0 unx      956 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/demo.py
--rw-r--r--  2.0 unx     8711 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/export.py
--rw-r--r--  2.0 unx     1191 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/info.yaml
--rw-r--r--  2.0 unx     3915 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/model.py
--rw-r--r--  2.0 unx     3919 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/perf.yaml
--rw-r--r--  2.0 unx     1607 b- defN 24-Apr-30 21:08 qai_hub_models/models/xlsr_quantized/test.py
--rw-r--r--  2.0 unx      436 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/__init__.py
--rw-r--r--  2.0 unx     1071 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/app.py
--rw-r--r--  2.0 unx     1405 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/conftest.py
--rw-r--r--  2.0 unx     1027 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/demo.py
--rw-r--r--  2.0 unx     8178 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/export.py
--rw-r--r--  2.0 unx     1168 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/info.yaml
--rw-r--r--  2.0 unx     4686 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/model.py
--rw-r--r--  2.0 unx     4574 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/perf.yaml
--rw-r--r--  2.0 unx     1845 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov6/test.py
--rw-r--r--  2.0 unx      436 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/__init__.py
--rw-r--r--  2.0 unx     2033 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/app.py
--rw-r--r--  2.0 unx     1405 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/conftest.py
--rw-r--r--  2.0 unx      909 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/demo.py
--rw-r--r--  2.0 unx     8198 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/export.py
--rw-r--r--  2.0 unx     1131 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/info.yaml
--rw-r--r--  2.0 unx    11972 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/model.py
--rw-r--r--  2.0 unx     3408 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/perf.yaml
--rw-r--r--  2.0 unx       83 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/requirements.txt
--rw-r--r--  2.0 unx     2322 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7/test.py
--rw-r--r--  2.0 unx      447 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/__init__.py
--rw-r--r--  2.0 unx     1415 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/conftest.py
--rw-r--r--  2.0 unx      853 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/demo.py
--rw-r--r--  2.0 unx     8614 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/export.py
--rw-r--r--  2.0 unx     1318 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/info.yaml
--rw-r--r--  2.0 unx     3251 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/model.py
--rw-r--r--  2.0 unx     4045 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/perf.yaml
--rw-r--r--  2.0 unx       83 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/requirements.txt
--rw-r--r--  2.0 unx     1558 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov7_quantized/test.py
--rw-r--r--  2.0 unx      415 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/__init__.py
--rw-r--r--  2.0 unx      892 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/app.py
--rw-r--r--  2.0 unx     1409 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/conftest.py
--rw-r--r--  2.0 unx      926 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/demo.py
--rw-r--r--  2.0 unx     8252 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/export.py
--rw-r--r--  2.0 unx     1171 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/info.yaml
--rw-r--r--  2.0 unx     8031 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/model.py
--rw-r--r--  2.0 unx     2768 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/perf.yaml
--rw-r--r--  2.0 unx      100 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/requirements.txt
--rw-r--r--  2.0 unx     2270 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det/test.py
--rw-r--r--  2.0 unx      459 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/__init__.py
--rw-r--r--  2.0 unx     1419 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/conftest.py
--rw-r--r--  2.0 unx      804 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/demo.py
--rw-r--r--  2.0 unx     8635 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/export.py
--rw-r--r--  2.0 unx     1354 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/info.yaml
--rw-r--r--  2.0 unx     3540 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/model.py
--rw-r--r--  2.0 unx     2777 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/perf.yaml
--rw-r--r--  2.0 unx      100 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/requirements.txt
--rw-r--r--  2.0 unx     1542 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_det_quantized/test.py
--rw-r--r--  2.0 unx      419 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/__init__.py
--rw-r--r--  2.0 unx     7698 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/app.py
--rw-r--r--  2.0 unx     1315 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/conftest.py
--rw-r--r--  2.0 unx     3155 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/demo.py
--rw-r--r--  2.0 unx     8255 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/export.py
--rw-r--r--  2.0 unx     1287 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/info.yaml
--rw-r--r--  2.0 unx     4663 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/model.py
--rw-r--r--  2.0 unx     3409 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/perf.yaml
--rw-r--r--  2.0 unx       64 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/requirements.txt
--rw-r--r--  2.0 unx     2536 b- defN 24-Apr-30 21:08 qai_hub_models/models/yolov8_seg/test.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/test/__init__.py
--rw-r--r--  2.0 unx     1043 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_async_compile_jobs.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/test/e2e/__init__.py
--rw-r--r--  2.0 unx     1661 b- defN 24-Apr-30 21:08 qai_hub_models/test/e2e/test_aimet_compile.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_utils/__init__.py
--rw-r--r--  2.0 unx     1493 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_utils/perf.yaml
--rw-r--r--  2.0 unx     3229 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_utils/test_info_specs.py
--rw-r--r--  2.0 unx     6525 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_utils/test_perf_summary.py
--rw-r--r--  2.0 unx     3295 b- defN 24-Apr-30 21:08 qai_hub_models/test/test_utils/test_qai_hub_helpers.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/utils/__init__.py
--rw-r--r--  2.0 unx    17286 b- defN 24-Apr-30 21:08 qai_hub_models/utils/args.py
--rw-r--r--  2.0 unx    34094 b- defN 24-Apr-30 21:08 qai_hub_models/utils/asset_loaders.py
--rw-r--r--  2.0 unx     5946 b- defN 24-Apr-30 21:08 qai_hub_models/utils/base_model.py
--rw-r--r--  2.0 unx     9261 b- defN 24-Apr-30 21:08 qai_hub_models/utils/bounding_box_processing.py
--rw-r--r--  2.0 unx     1771 b- defN 24-Apr-30 21:08 qai_hub_models/utils/camera_capture.py
--rw-r--r--  2.0 unx     5276 b- defN 24-Apr-30 21:08 qai_hub_models/utils/compare.py
--rw-r--r--  2.0 unx    32743 b- defN 24-Apr-30 21:08 qai_hub_models/utils/config_loaders.py
--rw-r--r--  2.0 unx     3066 b- defN 24-Apr-30 21:08 qai_hub_models/utils/display.py
--rw-r--r--  2.0 unx     6403 b- defN 24-Apr-30 21:08 qai_hub_models/utils/draw.py
--rw-r--r--  2.0 unx     1549 b- defN 24-Apr-30 21:08 qai_hub_models/utils/huggingface.py
--rw-r--r--  2.0 unx    13246 b- defN 24-Apr-30 21:08 qai_hub_models/utils/image_processing.py
--rw-r--r--  2.0 unx    12482 b- defN 24-Apr-30 21:08 qai_hub_models/utils/inference.py
--rw-r--r--  2.0 unx     1308 b- defN 24-Apr-30 21:08 qai_hub_models/utils/input_spec.py
--rw-r--r--  2.0 unx     4559 b- defN 24-Apr-30 21:08 qai_hub_models/utils/measurement.py
--rw-r--r--  2.0 unx     1577 b- defN 24-Apr-30 21:08 qai_hub_models/utils/model_adapters.py
--rw-r--r--  2.0 unx     1406 b- defN 24-Apr-30 21:08 qai_hub_models/utils/path_helpers.py
--rw-r--r--  2.0 unx     5007 b- defN 24-Apr-30 21:08 qai_hub_models/utils/printing.py
--rw-r--r--  2.0 unx     5365 b- defN 24-Apr-30 21:08 qai_hub_models/utils/qai_hub_helpers.py
--rw-r--r--  2.0 unx     1463 b- defN 24-Apr-30 21:08 qai_hub_models/utils/qnn_helpers.py
--rw-r--r--  2.0 unx     2170 b- defN 24-Apr-30 21:08 qai_hub_models/utils/quantization.py
--rw-r--r--  2.0 unx    17774 b- defN 24-Apr-30 21:08 qai_hub_models/utils/quantization_aimet.py
--rw-r--r--  2.0 unx      754 b- defN 24-Apr-30 21:08 qai_hub_models/utils/test_compare.py
--rw-r--r--  2.0 unx     3173 b- defN 24-Apr-30 21:08 qai_hub_models/utils/testing.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/__init__.py
--rw-r--r--  2.0 unx      876 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/config_loader.py
--rw-r--r--  2.0 unx     1233 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/default_config.json
--rw-r--r--  2.0 unx      946 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/default_config_legacy_v1.json
--rw-r--r--  2.0 unx      955 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/default_config_legacy_v2.json
--rw-r--r--  2.0 unx      919 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/default_config_per_channel_qnn.json
--rw-r--r--  2.0 unx     1187 b- defN 24-Apr-30 21:08 qai_hub_models/utils/aimet/repo.py
--rw-r--r--  2.0 unx      259 b- defN 24-Apr-30 21:08 qai_hub_models/utils/scorecard/__init__.py
--rw-r--r--  2.0 unx     1289 b- defN 24-Apr-30 21:08 qai_hub_models/utils/scorecard/common.py
--rw-r--r--  2.0 unx    12491 b- defN 24-Apr-30 21:08 qai_hub_models/utils/scorecard/job_summary.py
--rw-r--r--  2.0 unx    13385 b- defN 24-Apr-30 21:08 qai_hub_models/utils/scorecard/model_card.py
--rw-r--r--  2.0 unx    11415 b- defN 24-Apr-30 21:08 qai_hub_models/utils/scorecard/perf_summary.py
--rw-r--r--  2.0 unx     1481 b- defN 24-Apr-30 21:10 qai_hub_models-0.5.1.dist-info/LICENSE
--rw-r--r--  2.0 unx    43084 b- defN 24-Apr-30 21:10 qai_hub_models-0.5.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-30 21:10 qai_hub_models-0.5.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       15 b- defN 24-Apr-30 21:10 qai_hub_models-0.5.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    96763 b- defN 24-Apr-30 21:10 qai_hub_models-0.5.1.dist-info/RECORD
-942 files, 2799879 bytes uncompressed, 898180 bytes compressed:  67.9%
+Zip file size: 1173286 bytes, number of entries: 1016
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/__init__.py
+-rw-r--r--  2.0 unx      281 b- defN 24-May-20 18:58 qai_hub_models/_version.py
+-rw-r--r--  2.0 unx      620 b- defN 24-May-20 19:00 qai_hub_models/asset_bases.yaml
+-rw-r--r--  2.0 unx      734 b- defN 24-May-20 18:58 qai_hub_models/conftest.py
+-rw-r--r--  2.0 unx     1052 b- defN 24-May-20 18:58 qai_hub_models/global_requirements.txt
+-rw-r--r--  2.0 unx      395 b- defN 24-May-20 18:58 qai_hub_models/requirements-dev.txt
+-rw-r--r--  2.0 unx      427 b- defN 24-May-20 18:58 qai_hub_models/requirements.txt
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/datasets/__init__.py
+-rw-r--r--  2.0 unx     4635 b- defN 24-May-20 18:58 qai_hub_models/datasets/bsd300.py
+-rw-r--r--  2.0 unx     4109 b- defN 24-May-20 18:58 qai_hub_models/datasets/coco.py
+-rw-r--r--  2.0 unx     1632 b- defN 24-May-20 18:58 qai_hub_models/datasets/common.py
+-rw-r--r--  2.0 unx     3312 b- defN 24-May-20 18:58 qai_hub_models/datasets/imagenet.py
+-rw-r--r--  2.0 unx     3029 b- defN 24-May-20 18:58 qai_hub_models/datasets/imagenette.py
+-rw-r--r--  2.0 unx     2585 b- defN 24-May-20 18:58 qai_hub_models/datasets/pascal_voc.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/evaluators/__init__.py
+-rw-r--r--  2.0 unx     6212 b- defN 24-May-20 18:58 qai_hub_models/evaluators/base_evaluators.py
+-rw-r--r--  2.0 unx     1326 b- defN 24-May-20 18:58 qai_hub_models/evaluators/classification_evaluator.py
+-rw-r--r--  2.0 unx     3699 b- defN 24-May-20 18:58 qai_hub_models/evaluators/detection_evaluator.py
+-rw-r--r--  2.0 unx     2587 b- defN 24-May-20 18:58 qai_hub_models/evaluators/segmentation_evaluator.py
+-rw-r--r--  2.0 unx     2181 b- defN 24-May-20 18:58 qai_hub_models/evaluators/superres_evaluator.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/__init__.py
+-rw-r--r--  2.0 unx      666 b- defN 24-May-20 18:58 qai_hub_models/models/common.py
+-rw-r--r--  2.0 unx     8042 b- defN 24-May-20 18:58 qai_hub_models/models/protocols.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/__init__.py
+-rw-r--r--  2.0 unx     1655 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/common.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/cityscapes_segmentation/__init__.py
+-rw-r--r--  2.0 unx     4346 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/cityscapes_segmentation/app.py
+-rw-r--r--  2.0 unx     2904 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/cityscapes_segmentation/demo.py
+-rw-r--r--  2.0 unx      747 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py
+-rw-r--r--  2.0 unx     2888 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/cityscapes_segmentation/model.py
+-rw-r--r--  2.0 unx     8565 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/cityscapes_segmentation/patches/move_datasets.diff
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/convnext_tiny_quantized/__init__.py
+-rw-r--r--  2.0 unx     4270 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/convnext_tiny_quantized/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/deeplab/__init__.py
+-rw-r--r--  2.0 unx     2449 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/deeplab/app.py
+-rw-r--r--  2.0 unx     2256 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/deeplab/demo.py
+-rw-r--r--  2.0 unx     2034 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/deeplab/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/detr/__init__.py
+-rw-r--r--  2.0 unx     4604 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/detr/app.py
+-rw-r--r--  2.0 unx     3565 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/detr/coco_label_map.py
+-rw-r--r--  2.0 unx     2091 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/detr/demo.py
+-rw-r--r--  2.0 unx     2050 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/detr/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/fastsam/__init__.py
+-rw-r--r--  2.0 unx     4883 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/fastsam/app.py
+-rw-r--r--  2.0 unx     2026 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/fastsam/demo.py
+-rw-r--r--  2.0 unx     1935 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/fastsam/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/ffnet/__init__.py
+-rw-r--r--  2.0 unx     4548 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/ffnet/model.py
+-rw-r--r--  2.0 unx     1569 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/ffnet/test_utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/ffnet_quantized/__init__.py
+-rw-r--r--  2.0 unx     1165 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/ffnet_quantized/aimet_config.json
+-rw-r--r--  2.0 unx     2475 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/ffnet_quantized/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/imagenet_classifier/__init__.py
+-rw-r--r--  2.0 unx     2794 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/imagenet_classifier/app.py
+-rw-r--r--  2.0 unx     2432 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/imagenet_classifier/demo.py
+-rw-r--r--  2.0 unx     4669 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/imagenet_classifier/model.py
+-rw-r--r--  2.0 unx     3781 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/imagenet_classifier/test_utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/mediapipe/__init__.py
+-rw-r--r--  2.0 unx    30302 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/mediapipe/app.py
+-rw-r--r--  2.0 unx     4394 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/mediapipe/utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/quicksrnet/__init__.py
+-rw-r--r--  2.0 unx      985 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/quicksrnet/common.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/repaint/__init__.py
+-rw-r--r--  2.0 unx     3366 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/repaint/app.py
+-rw-r--r--  2.0 unx     2229 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/repaint/demo.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/sesr/__init__.py
+-rw-r--r--  2.0 unx     1029 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/sesr/common.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/super_resolution/__init__.py
+-rw-r--r--  2.0 unx     2143 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/super_resolution/app.py
+-rw-r--r--  2.0 unx     2775 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/super_resolution/demo.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/swin/__init__.py
+-rw-r--r--  2.0 unx     9175 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/swin/swin_transformer.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/video_classifier/__init__.py
+-rw-r--r--  2.0 unx    11183 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/video_classifier/app.py
+-rw-r--r--  2.0 unx     1568 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/video_classifier/demo.py
+-rw-r--r--  2.0 unx     1969 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/video_classifier/model.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/whisper/__init__.py
+-rw-r--r--  2.0 unx    10427 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/whisper/app.py
+-rw-r--r--  2.0 unx     1302 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/whisper/demo.py
+-rw-r--r--  2.0 unx    18911 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/whisper/model.py
+-rw-r--r--  2.0 unx     3123 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/whisper/test_utils.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/yolo/__init__.py
+-rw-r--r--  2.0 unx     7413 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/yolo/app.py
+-rw-r--r--  2.0 unx     2494 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/yolo/demo.py
+-rw-r--r--  2.0 unx     6081 b- defN 24-May-20 18:58 qai_hub_models/models/_shared/yolo/utils.py
+-rw-r--r--  2.0 unx      450 b- defN 24-May-20 18:58 qai_hub_models/models/aotgan/__init__.py
+-rw-r--r--  2.0 unx     1405 b- defN 24-May-20 18:58 qai_hub_models/models/aotgan/conftest.py
+-rw-r--r--  2.0 unx      597 b- defN 24-May-20 18:58 qai_hub_models/models/aotgan/demo.py
+-rw-r--r--  2.0 unx     8895 b- defN 24-May-20 18:58 qai_hub_models/models/aotgan/export.py
+-rw-r--r--  2.0 unx     1023 b- defN 24-May-20 18:58 qai_hub_models/models/aotgan/info.yaml
+-rw-r--r--  2.0 unx     4838 b- defN 24-May-20 18:58 qai_hub_models/models/aotgan/model.py
+-rw-r--r--  2.0 unx     5978 b- defN 24-May-20 18:58 qai_hub_models/models/aotgan/perf.yaml
+-rw-r--r--  2.0 unx     2006 b- defN 24-May-20 18:58 qai_hub_models/models/aotgan/test.py
+-rw-r--r--  2.0 unx      532 b- defN 24-May-20 18:58 qai_hub_models/models/aotgan/patches/layer_norm.diff
+-rw-r--r--  2.0 unx     1987 b- defN 24-May-20 18:58 qai_hub_models/models/baichuan_7b_quantized/info.yaml
+-rw-r--r--  2.0 unx     2038 b- defN 24-May-20 18:58 qai_hub_models/models/baichuan_7b_quantized/perf.yaml
+-rw-r--r--  2.0 unx      559 b- defN 24-May-20 18:58 qai_hub_models/models/controlnet_quantized/__init__.py
+-rw-r--r--  2.0 unx     9705 b- defN 24-May-20 18:58 qai_hub_models/models/controlnet_quantized/app.py
+-rw-r--r--  2.0 unx     6397 b- defN 24-May-20 18:58 qai_hub_models/models/controlnet_quantized/demo.py
+-rw-r--r--  2.0 unx     7894 b- defN 24-May-20 18:58 qai_hub_models/models/controlnet_quantized/export.py
+-rw-r--r--  2.0 unx     1334 b- defN 24-May-20 18:58 qai_hub_models/models/controlnet_quantized/info.yaml
+-rw-r--r--  2.0 unx     5426 b- defN 24-May-20 18:58 qai_hub_models/models/controlnet_quantized/model.py
+-rw-r--r--  2.0 unx     8427 b- defN 24-May-20 18:58 qai_hub_models/models/controlnet_quantized/perf.yaml
+-rw-r--r--  2.0 unx       46 b- defN 24-May-20 18:58 qai_hub_models/models/controlnet_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1588 b- defN 24-May-20 18:58 qai_hub_models/models/controlnet_quantized/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny/__init__.py
+-rw-r--r--  2.0 unx     1318 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny/conftest.py
+-rw-r--r--  2.0 unx      543 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny/demo.py
+-rw-r--r--  2.0 unx     8514 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny/export.py
+-rw-r--r--  2.0 unx     1287 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny/info.yaml
+-rw-r--r--  2.0 unx      708 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny/model.py
+-rw-r--r--  2.0 unx     6053 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny/perf.yaml
+-rw-r--r--  2.0 unx      857 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny/test.py
+-rw-r--r--  2.0 unx      491 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a16_quantized/__init__.py
+-rw-r--r--  2.0 unx     1428 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a16_quantized/conftest.py
+-rw-r--r--  2.0 unx      604 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a16_quantized/demo.py
+-rw-r--r--  2.0 unx     9022 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a16_quantized/export.py
+-rw-r--r--  2.0 unx     1384 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a16_quantized/info.yaml
+-rw-r--r--  2.0 unx     1180 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a16_quantized/model.py
+-rw-r--r--  2.0 unx     1028 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a16_quantized/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a8_quantized/__init__.py
+-rw-r--r--  2.0 unx     1427 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a8_quantized/conftest.py
+-rw-r--r--  2.0 unx      601 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a8_quantized/demo.py
+-rw-r--r--  2.0 unx     9018 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a8_quantized/export.py
+-rw-r--r--  2.0 unx     1380 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a8_quantized/info.yaml
+-rw-r--r--  2.0 unx     1177 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a8_quantized/model.py
+-rw-r--r--  2.0 unx     1024 b- defN 24-May-20 18:58 qai_hub_models/models/convnext_tiny_w8a8_quantized/test.py
+-rw-r--r--  2.0 unx      398 b- defN 24-May-20 18:58 qai_hub_models/models/ddrnet23_slim/__init__.py
+-rw-r--r--  2.0 unx     3750 b- defN 24-May-20 18:58 qai_hub_models/models/ddrnet23_slim/app.py
+-rw-r--r--  2.0 unx     1412 b- defN 24-May-20 18:58 qai_hub_models/models/ddrnet23_slim/conftest.py
+-rw-r--r--  2.0 unx     2128 b- defN 24-May-20 18:58 qai_hub_models/models/ddrnet23_slim/demo.py
+-rw-r--r--  2.0 unx     8874 b- defN 24-May-20 18:58 qai_hub_models/models/ddrnet23_slim/export.py
+-rw-r--r--  2.0 unx     1334 b- defN 24-May-20 18:58 qai_hub_models/models/ddrnet23_slim/info.yaml
+-rw-r--r--  2.0 unx     3831 b- defN 24-May-20 18:58 qai_hub_models/models/ddrnet23_slim/model.py
+-rw-r--r--  2.0 unx     4493 b- defN 24-May-20 18:58 qai_hub_models/models/ddrnet23_slim/perf.yaml
+-rw-r--r--  2.0 unx     1790 b- defN 24-May-20 18:58 qai_hub_models/models/ddrnet23_slim/test.py
+-rw-r--r--  2.0 unx      455 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet/__init__.py
+-rw-r--r--  2.0 unx     1423 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet/conftest.py
+-rw-r--r--  2.0 unx     1091 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet/demo.py
+-rw-r--r--  2.0 unx     8898 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet/export.py
+-rw-r--r--  2.0 unx     1266 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet/info.yaml
+-rw-r--r--  2.0 unx     2098 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet/model.py
+-rw-r--r--  2.0 unx     6082 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet/perf.yaml
+-rw-r--r--  2.0 unx     1862 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet/test.py
+-rw-r--r--  2.0 unx      466 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/__init__.py
+-rw-r--r--  2.0 unx     1433 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/conftest.py
+-rw-r--r--  2.0 unx     1156 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/demo.py
+-rw-r--r--  2.0 unx     9314 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/export.py
+-rw-r--r--  2.0 unx     1306 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/info.yaml
+-rw-r--r--  2.0 unx     2861 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/model.py
+-rw-r--r--  2.0 unx     7788 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2183 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/test.py
+-rw-r--r--  2.0 unx      451 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_resnet50/__init__.py
+-rw-r--r--  2.0 unx     1417 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_resnet50/conftest.py
+-rw-r--r--  2.0 unx     1077 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_resnet50/demo.py
+-rw-r--r--  2.0 unx     8940 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_resnet50/export.py
+-rw-r--r--  2.0 unx     1285 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_resnet50/info.yaml
+-rw-r--r--  2.0 unx     2094 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_resnet50/model.py
+-rw-r--r--  2.0 unx     4764 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_resnet50/perf.yaml
+-rw-r--r--  2.0 unx     1766 b- defN 24-May-20 18:58 qai_hub_models/models/deeplabv3_resnet50/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-May-20 18:58 qai_hub_models/models/densenet121/__init__.py
+-rw-r--r--  2.0 unx     1316 b- defN 24-May-20 18:58 qai_hub_models/models/densenet121/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-May-20 18:58 qai_hub_models/models/densenet121/demo.py
+-rw-r--r--  2.0 unx     8507 b- defN 24-May-20 18:58 qai_hub_models/models/densenet121/export.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-May-20 18:58 qai_hub_models/models/densenet121/info.yaml
+-rw-r--r--  2.0 unx      698 b- defN 24-May-20 18:58 qai_hub_models/models/densenet121/model.py
+-rw-r--r--  2.0 unx     6039 b- defN 24-May-20 18:58 qai_hub_models/models/densenet121/perf.yaml
+-rw-r--r--  2.0 unx      841 b- defN 24-May-20 18:58 qai_hub_models/models/densenet121/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101/__init__.py
+-rw-r--r--  2.0 unx     1319 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101/conftest.py
+-rw-r--r--  2.0 unx      896 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101/demo.py
+-rw-r--r--  2.0 unx     8494 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101/export.py
+-rw-r--r--  2.0 unx     1188 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101/info.yaml
+-rw-r--r--  2.0 unx      661 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101/model.py
+-rw-r--r--  2.0 unx     6087 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101/requirements.txt
+-rw-r--r--  2.0 unx     1316 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101_dc5/__init__.py
+-rw-r--r--  2.0 unx     1323 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101_dc5/conftest.py
+-rw-r--r--  2.0 unx      906 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101_dc5/demo.py
+-rw-r--r--  2.0 unx     8510 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101_dc5/export.py
+-rw-r--r--  2.0 unx     1215 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101_dc5/info.yaml
+-rw-r--r--  2.0 unx      668 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101_dc5/model.py
+-rw-r--r--  2.0 unx     6106 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101_dc5/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101_dc5/requirements.txt
+-rw-r--r--  2.0 unx     1373 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet101_dc5/test.py
+-rw-r--r--  2.0 unx      481 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50/__init__.py
+-rw-r--r--  2.0 unx     1318 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50/conftest.py
+-rw-r--r--  2.0 unx      893 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50/demo.py
+-rw-r--r--  2.0 unx     8490 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50/export.py
+-rw-r--r--  2.0 unx     1185 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50/info.yaml
+-rw-r--r--  2.0 unx      659 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50/model.py
+-rw-r--r--  2.0 unx     6067 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50/requirements.txt
+-rw-r--r--  2.0 unx     1636 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50/test.py
+-rw-r--r--  2.0 unx      488 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50_dc5/__init__.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50_dc5/conftest.py
+-rw-r--r--  2.0 unx      903 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50_dc5/demo.py
+-rw-r--r--  2.0 unx     8506 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50_dc5/export.py
+-rw-r--r--  2.0 unx     1212 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50_dc5/info.yaml
+-rw-r--r--  2.0 unx      666 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50_dc5/model.py
+-rw-r--r--  2.0 unx     6057 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50_dc5/perf.yaml
+-rw-r--r--  2.0 unx       34 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50_dc5/requirements.txt
+-rw-r--r--  2.0 unx     1344 b- defN 24-May-20 18:58 qai_hub_models/models/detr_resnet50_dc5/test.py
+-rw-r--r--  2.0 unx      477 b- defN 24-May-20 18:58 qai_hub_models/models/efficientnet_b0/__init__.py
+-rw-r--r--  2.0 unx     1320 b- defN 24-May-20 18:58 qai_hub_models/models/efficientnet_b0/conftest.py
+-rw-r--r--  2.0 unx      549 b- defN 24-May-20 18:58 qai_hub_models/models/efficientnet_b0/demo.py
+-rw-r--r--  2.0 unx     8522 b- defN 24-May-20 18:58 qai_hub_models/models/efficientnet_b0/export.py
+-rw-r--r--  2.0 unx     1361 b- defN 24-May-20 18:58 qai_hub_models/models/efficientnet_b0/info.yaml
+-rw-r--r--  2.0 unx      714 b- defN 24-May-20 18:58 qai_hub_models/models/efficientnet_b0/model.py
+-rw-r--r--  2.0 unx     6039 b- defN 24-May-20 18:58 qai_hub_models/models/efficientnet_b0/perf.yaml
+-rw-r--r--  2.0 unx      867 b- defN 24-May-20 18:58 qai_hub_models/models/efficientnet_b0/test.py
+-rw-r--r--  2.0 unx      463 b- defN 24-May-20 18:58 qai_hub_models/models/esrgan/__init__.py
+-rw-r--r--  2.0 unx     1405 b- defN 24-May-20 18:58 qai_hub_models/models/esrgan/conftest.py
+-rw-r--r--  2.0 unx      939 b- defN 24-May-20 18:58 qai_hub_models/models/esrgan/demo.py
+-rw-r--r--  2.0 unx     8683 b- defN 24-May-20 18:58 qai_hub_models/models/esrgan/export.py
+-rw-r--r--  2.0 unx     1117 b- defN 24-May-20 18:58 qai_hub_models/models/esrgan/info.yaml
+-rw-r--r--  2.0 unx     3473 b- defN 24-May-20 18:58 qai_hub_models/models/esrgan/model.py
+-rw-r--r--  2.0 unx     6085 b- defN 24-May-20 18:58 qai_hub_models/models/esrgan/perf.yaml
+-rw-r--r--  2.0 unx     1831 b- defN 24-May-20 18:58 qai_hub_models/models/esrgan/test.py
+-rw-r--r--  2.0 unx      418 b- defN 24-May-20 18:58 qai_hub_models/models/facebook_denoiser/__init__.py
+-rw-r--r--  2.0 unx     3207 b- defN 24-May-20 18:58 qai_hub_models/models/facebook_denoiser/app.py
+-rw-r--r--  2.0 unx     1416 b- defN 24-May-20 18:58 qai_hub_models/models/facebook_denoiser/conftest.py
+-rw-r--r--  2.0 unx     3176 b- defN 24-May-20 18:58 qai_hub_models/models/facebook_denoiser/demo.py
+-rw-r--r--  2.0 unx     7943 b- defN 24-May-20 18:58 qai_hub_models/models/facebook_denoiser/export.py
+-rw-r--r--  2.0 unx     1070 b- defN 24-May-20 18:58 qai_hub_models/models/facebook_denoiser/info.yaml
+-rw-r--r--  2.0 unx     2414 b- defN 24-May-20 18:58 qai_hub_models/models/facebook_denoiser/model.py
+-rw-r--r--  2.0 unx     4531 b- defN 24-May-20 18:58 qai_hub_models/models/facebook_denoiser/perf.yaml
+-rw-r--r--  2.0 unx       74 b- defN 24-May-20 18:58 qai_hub_models/models/facebook_denoiser/requirements.txt
+-rw-r--r--  2.0 unx     2492 b- defN 24-May-20 18:58 qai_hub_models/models/facebook_denoiser/test.py
+-rw-r--r--  2.0 unx      440 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_s/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_s/conftest.py
+-rw-r--r--  2.0 unx      762 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_s/demo.py
+-rw-r--r--  2.0 unx     8953 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_s/export.py
+-rw-r--r--  2.0 unx     1301 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_s/info.yaml
+-rw-r--r--  2.0 unx      683 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_s/model.py
+-rw-r--r--  2.0 unx     6071 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_s/perf.yaml
+-rw-r--r--  2.0 unx       64 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_s/requirements.txt
+-rw-r--r--  2.0 unx     1332 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_s/test.py
+-rw-r--r--  2.0 unx      440 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_x/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_x/conftest.py
+-rw-r--r--  2.0 unx      762 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_x/demo.py
+-rw-r--r--  2.0 unx     8953 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_x/export.py
+-rw-r--r--  2.0 unx     1300 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_x/info.yaml
+-rw-r--r--  2.0 unx      683 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_x/model.py
+-rw-r--r--  2.0 unx     6081 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_x/perf.yaml
+-rw-r--r--  2.0 unx       64 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_x/requirements.txt
+-rw-r--r--  2.0 unx     1332 b- defN 24-May-20 18:58 qai_hub_models/models/fastsam_x/test.py
+-rw-r--r--  2.0 unx      410 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50/__init__.py
+-rw-r--r--  2.0 unx     2481 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50/app.py
+-rw-r--r--  2.0 unx     1411 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50/conftest.py
+-rw-r--r--  2.0 unx     2470 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50/demo.py
+-rw-r--r--  2.0 unx     8850 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50/export.py
+-rw-r--r--  2.0 unx     1254 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50/info.yaml
+-rw-r--r--  2.0 unx     2362 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50/model.py
+-rw-r--r--  2.0 unx     6084 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50/perf.yaml
+-rw-r--r--  2.0 unx     1637 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50/test.py
+-rw-r--r--  2.0 unx      456 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50_quantized/__init__.py
+-rw-r--r--  2.0 unx     1421 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50_quantized/conftest.py
+-rw-r--r--  2.0 unx      547 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50_quantized/demo.py
+-rw-r--r--  2.0 unx     9266 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50_quantized/export.py
+-rw-r--r--  2.0 unx     1308 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50_quantized/info.yaml
+-rw-r--r--  2.0 unx     2829 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50_quantized/model.py
+-rw-r--r--  2.0 unx     7744 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50_quantized/perf.yaml
+-rw-r--r--  2.0 unx     1414 b- defN 24-May-20 18:58 qai_hub_models/models/fcn_resnet50_quantized/test.py
+-rw-r--r--  2.0 unx      487 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_122ns_lowres/__init__.py
+-rw-r--r--  2.0 unx     1417 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_122ns_lowres/conftest.py
+-rw-r--r--  2.0 unx      607 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_122ns_lowres/demo.py
+-rw-r--r--  2.0 unx     8731 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_122ns_lowres/export.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_122ns_lowres/info.yaml
+-rw-r--r--  2.0 unx      648 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_122ns_lowres/model.py
+-rw-r--r--  2.0 unx     6072 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_122ns_lowres/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_122ns_lowres/requirements.txt
+-rw-r--r--  2.0 unx      804 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_122ns_lowres/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s/__init__.py
+-rw-r--r--  2.0 unx     1408 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s/conftest.py
+-rw-r--r--  2.0 unx      582 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s/demo.py
+-rw-r--r--  2.0 unx     8695 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s/export.py
+-rw-r--r--  2.0 unx     1298 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s/info.yaml
+-rw-r--r--  2.0 unx      580 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s/model.py
+-rw-r--r--  2.0 unx     6073 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s/requirements.txt
+-rw-r--r--  2.0 unx      746 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s_quantized/__init__.py
+-rw-r--r--  2.0 unx     1418 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s_quantized/conftest.py
+-rw-r--r--  2.0 unx      627 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s_quantized/demo.py
+-rw-r--r--  2.0 unx     9131 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s_quantized/export.py
+-rw-r--r--  2.0 unx     1347 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s_quantized/info.yaml
+-rw-r--r--  2.0 unx     1169 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s_quantized/model.py
+-rw-r--r--  2.0 unx     7760 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s_quantized/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s_quantized/requirements.txt
+-rw-r--r--  2.0 unx      840 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_40s_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s/__init__.py
+-rw-r--r--  2.0 unx     1408 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s/conftest.py
+-rw-r--r--  2.0 unx      582 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s/demo.py
+-rw-r--r--  2.0 unx     8695 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s/export.py
+-rw-r--r--  2.0 unx     1275 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s/info.yaml
+-rw-r--r--  2.0 unx      580 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s/model.py
+-rw-r--r--  2.0 unx     6082 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s/requirements.txt
+-rw-r--r--  2.0 unx      746 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s_quantized/__init__.py
+-rw-r--r--  2.0 unx     1418 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s_quantized/conftest.py
+-rw-r--r--  2.0 unx      627 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s_quantized/demo.py
+-rw-r--r--  2.0 unx     9131 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s_quantized/export.py
+-rw-r--r--  2.0 unx     1347 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s_quantized/info.yaml
+-rw-r--r--  2.0 unx     1156 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s_quantized/model.py
+-rw-r--r--  2.0 unx     7787 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s_quantized/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s_quantized/requirements.txt
+-rw-r--r--  2.0 unx      840 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_54s_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s/__init__.py
+-rw-r--r--  2.0 unx     1408 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s/conftest.py
+-rw-r--r--  2.0 unx      582 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s/demo.py
+-rw-r--r--  2.0 unx     8695 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s/export.py
+-rw-r--r--  2.0 unx     1279 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s/info.yaml
+-rw-r--r--  2.0 unx      580 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s/model.py
+-rw-r--r--  2.0 unx     6086 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s/requirements.txt
+-rw-r--r--  2.0 unx      746 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s/test.py
+-rw-r--r--  2.0 unx      485 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_lowres/__init__.py
+-rw-r--r--  2.0 unx     1415 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_lowres/conftest.py
+-rw-r--r--  2.0 unx      601 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_lowres/demo.py
+-rw-r--r--  2.0 unx     8723 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_lowres/export.py
+-rw-r--r--  2.0 unx     1327 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_lowres/info.yaml
+-rw-r--r--  2.0 unx      640 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_lowres/model.py
+-rw-r--r--  2.0 unx     6073 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_lowres/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_lowres/requirements.txt
+-rw-r--r--  2.0 unx      794 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_lowres/test.py
+-rw-r--r--  2.0 unx      490 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_quantized/__init__.py
+-rw-r--r--  2.0 unx     1418 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_quantized/conftest.py
+-rw-r--r--  2.0 unx      627 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_quantized/demo.py
+-rw-r--r--  2.0 unx     9131 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_quantized/export.py
+-rw-r--r--  2.0 unx     1347 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_quantized/info.yaml
+-rw-r--r--  2.0 unx     1156 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_quantized/model.py
+-rw-r--r--  2.0 unx     7789 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_quantized/perf.yaml
+-rw-r--r--  2.0 unx       21 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_quantized/requirements.txt
+-rw-r--r--  2.0 unx      840 b- defN 24-May-20 18:58 qai_hub_models/models/ffnet_78s_quantized/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet/demo.py
+-rw-r--r--  2.0 unx     8498 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet/export.py
+-rw-r--r--  2.0 unx     1295 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet/info.yaml
+-rw-r--r--  2.0 unx      743 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet/model.py
+-rw-r--r--  2.0 unx     6029 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet/perf.yaml
+-rw-r--r--  2.0 unx      840 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet/test.py
+-rw-r--r--  2.0 unx      573 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet_quantized/__init__.py
+-rw-r--r--  2.0 unx     1324 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet_quantized/conftest.py
+-rw-r--r--  2.0 unx      578 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet_quantized/demo.py
+-rw-r--r--  2.0 unx     8961 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet_quantized/export.py
+-rw-r--r--  2.0 unx     1325 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet_quantized/info.yaml
+-rw-r--r--  2.0 unx     4298 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet_quantized/model.py
+-rw-r--r--  2.0 unx     7700 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet_quantized/perf.yaml
+-rw-r--r--  2.0 unx      885 b- defN 24-May-20 18:58 qai_hub_models/models/googlenet_quantized/test.py
+-rw-r--r--  2.0 unx      430 b- defN 24-May-20 18:58 qai_hub_models/models/hrnet_pose/__init__.py
+-rw-r--r--  2.0 unx     8458 b- defN 24-May-20 18:58 qai_hub_models/models/hrnet_pose/app.py
+-rw-r--r--  2.0 unx     1409 b- defN 24-May-20 18:58 qai_hub_models/models/hrnet_pose/conftest.py
+-rw-r--r--  2.0 unx     1739 b- defN 24-May-20 18:58 qai_hub_models/models/hrnet_pose/demo.py
+-rw-r--r--  2.0 unx     8841 b- defN 24-May-20 18:58 qai_hub_models/models/hrnet_pose/export.py
+-rw-r--r--  2.0 unx     1196 b- defN 24-May-20 18:58 qai_hub_models/models/hrnet_pose/info.yaml
+-rw-r--r--  2.0 unx     3360 b- defN 24-May-20 18:58 qai_hub_models/models/hrnet_pose/model.py
+-rw-r--r--  2.0 unx     6037 b- defN 24-May-20 18:58 qai_hub_models/models/hrnet_pose/perf.yaml
+-rw-r--r--  2.0 unx       51 b- defN 24-May-20 18:58 qai_hub_models/models/hrnet_pose/requirements.txt
+-rw-r--r--  2.0 unx     1420 b- defN 24-May-20 18:58 qai_hub_models/models/hrnet_pose/test.py
+-rw-r--r--  2.0 unx      434 b- defN 24-May-20 18:58 qai_hub_models/models/huggingface_wavlm_base_plus/__init__.py
+-rw-r--r--  2.0 unx     2133 b- defN 24-May-20 18:58 qai_hub_models/models/huggingface_wavlm_base_plus/app.py
+-rw-r--r--  2.0 unx     1426 b- defN 24-May-20 18:58 qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py
+-rw-r--r--  2.0 unx     1517 b- defN 24-May-20 18:58 qai_hub_models/models/huggingface_wavlm_base_plus/demo.py
+-rw-r--r--  2.0 unx     7860 b- defN 24-May-20 18:58 qai_hub_models/models/huggingface_wavlm_base_plus/export.py
+-rw-r--r--  2.0 unx     1248 b- defN 24-May-20 18:58 qai_hub_models/models/huggingface_wavlm_base_plus/info.yaml
+-rw-r--r--  2.0 unx     7764 b- defN 24-May-20 18:58 qai_hub_models/models/huggingface_wavlm_base_plus/model.py
+-rw-r--r--  2.0 unx     5540 b- defN 24-May-20 18:58 qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml
+-rw-r--r--  2.0 unx       72 b- defN 24-May-20 18:58 qai_hub_models/models/huggingface_wavlm_base_plus/requirements.txt
+-rw-r--r--  2.0 unx     2560 b- defN 24-May-20 18:58 qai_hub_models/models/huggingface_wavlm_base_plus/test.py
+-rw-r--r--  2.0 unx      477 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3/__init__.py
+-rw-r--r--  2.0 unx     1317 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3/conftest.py
+-rw-r--r--  2.0 unx      546 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3/demo.py
+-rw-r--r--  2.0 unx     8510 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3/export.py
+-rw-r--r--  2.0 unx     1359 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3/info.yaml
+-rw-r--r--  2.0 unx      756 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3/model.py
+-rw-r--r--  2.0 unx     6043 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3/perf.yaml
+-rw-r--r--  2.0 unx      861 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3/test.py
+-rw-r--r--  2.0 unx      584 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3_quantized/__init__.py
+-rw-r--r--  2.0 unx     1327 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3_quantized/conftest.py
+-rw-r--r--  2.0 unx      591 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3_quantized/demo.py
+-rw-r--r--  2.0 unx     8974 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3_quantized/export.py
+-rw-r--r--  2.0 unx     1496 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3_quantized/info.yaml
+-rw-r--r--  2.0 unx     7762 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3_quantized/model.py
+-rw-r--r--  2.0 unx     7750 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3_quantized/perf.yaml
+-rw-r--r--  2.0 unx      901 b- defN 24-May-20 18:58 qai_hub_models/models/inception_v3_quantized/test.py
+-rw-r--r--  2.0 unx      455 b- defN 24-May-20 18:58 qai_hub_models/models/lama_dilated/__init__.py
+-rw-r--r--  2.0 unx     1411 b- defN 24-May-20 18:58 qai_hub_models/models/lama_dilated/conftest.py
+-rw-r--r--  2.0 unx      911 b- defN 24-May-20 18:58 qai_hub_models/models/lama_dilated/demo.py
+-rw-r--r--  2.0 unx     8918 b- defN 24-May-20 18:58 qai_hub_models/models/lama_dilated/export.py
+-rw-r--r--  2.0 unx     1082 b- defN 24-May-20 18:58 qai_hub_models/models/lama_dilated/info.yaml
+-rw-r--r--  2.0 unx     4977 b- defN 24-May-20 18:58 qai_hub_models/models/lama_dilated/model.py
+-rw-r--r--  2.0 unx     6013 b- defN 24-May-20 18:58 qai_hub_models/models/lama_dilated/perf.yaml
+-rw-r--r--  2.0 unx      171 b- defN 24-May-20 18:58 qai_hub_models/models/lama_dilated/requirements.txt
+-rw-r--r--  2.0 unx     2074 b- defN 24-May-20 18:58 qai_hub_models/models/lama_dilated/test.py
+-rw-r--r--  2.0 unx      404 b- defN 24-May-20 18:58 qai_hub_models/models/litehrnet/__init__.py
+-rw-r--r--  2.0 unx     3986 b- defN 24-May-20 18:58 qai_hub_models/models/litehrnet/app.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-20 18:58 qai_hub_models/models/litehrnet/conftest.py
+-rw-r--r--  2.0 unx     2072 b- defN 24-May-20 18:58 qai_hub_models/models/litehrnet/demo.py
+-rw-r--r--  2.0 unx     7931 b- defN 24-May-20 18:58 qai_hub_models/models/litehrnet/export.py
+-rw-r--r--  2.0 unx     1113 b- defN 24-May-20 18:58 qai_hub_models/models/litehrnet/info.yaml
+-rw-r--r--  2.0 unx     3788 b- defN 24-May-20 18:58 qai_hub_models/models/litehrnet/model.py
+-rw-r--r--  2.0 unx     4428 b- defN 24-May-20 18:58 qai_hub_models/models/litehrnet/perf.yaml
+-rw-r--r--  2.0 unx       39 b- defN 24-May-20 18:58 qai_hub_models/models/litehrnet/requirements.txt
+-rw-r--r--  2.0 unx     1714 b- defN 24-May-20 18:58 qai_hub_models/models/litehrnet/test.py
+-rw-r--r--  2.0 unx     1954 b- defN 24-May-20 18:58 qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml
+-rw-r--r--  2.0 unx     2039 b- defN 24-May-20 18:58 qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml
+-rw-r--r--  2.0 unx      412 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_face/__init__.py
+-rw-r--r--  2.0 unx     2099 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_face/app.py
+-rw-r--r--  2.0 unx     1413 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_face/conftest.py
+-rw-r--r--  2.0 unx     2862 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_face/demo.py
+-rw-r--r--  2.0 unx    10022 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_face/export.py
+-rw-r--r--  2.0 unx     1469 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_face/info.yaml
+-rw-r--r--  2.0 unx     7669 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_face/model.py
+-rw-r--r--  2.0 unx    11312 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_face/perf.yaml
+-rw-r--r--  2.0 unx     1441 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_face/test.py
+-rw-r--r--  2.0 unx      412 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_hand/__init__.py
+-rw-r--r--  2.0 unx     9819 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_hand/app.py
+-rw-r--r--  2.0 unx     1413 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_hand/conftest.py
+-rw-r--r--  2.0 unx     2832 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_hand/demo.py
+-rw-r--r--  2.0 unx    10022 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_hand/export.py
+-rw-r--r--  2.0 unx     1374 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_hand/info.yaml
+-rw-r--r--  2.0 unx     6017 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_hand/model.py
+-rw-r--r--  2.0 unx    11335 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_hand/perf.yaml
+-rw-r--r--  2.0 unx     1442 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_hand/test.py
+-rw-r--r--  2.0 unx      412 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_pose/__init__.py
+-rw-r--r--  2.0 unx     4430 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_pose/app.py
+-rw-r--r--  2.0 unx     1413 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_pose/conftest.py
+-rw-r--r--  2.0 unx     2889 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_pose/demo.py
+-rw-r--r--  2.0 unx    10023 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_pose/export.py
+-rw-r--r--  2.0 unx     1387 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_pose/info.yaml
+-rw-r--r--  2.0 unx     5801 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_pose/model.py
+-rw-r--r--  2.0 unx    11330 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_pose/perf.yaml
+-rw-r--r--  2.0 unx     1443 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_pose/test.py
+-rw-r--r--  2.0 unx      362 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/__init__.py
+-rw-r--r--  2.0 unx     1411 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/app.py
+-rw-r--r--  2.0 unx     1321 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/conftest.py
+-rw-r--r--  2.0 unx     2674 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/demo.py
+-rw-r--r--  2.0 unx     8879 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/export.py
+-rw-r--r--  2.0 unx     1455 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/info.yaml
+-rw-r--r--  2.0 unx    12352 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/model.py
+-rw-r--r--  2.0 unx     6059 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/perf.yaml
+-rw-r--r--  2.0 unx       15 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/requirements.txt
+-rw-r--r--  2.0 unx     1396 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/test.py
+-rw-r--r--  2.0 unx     2492 b- defN 24-May-20 18:58 qai_hub_models/models/mediapipe_selfie/utils.py
+-rw-r--r--  2.0 unx      396 b- defN 24-May-20 18:58 qai_hub_models/models/midas/__init__.py
+-rw-r--r--  2.0 unx     2254 b- defN 24-May-20 18:58 qai_hub_models/models/midas/app.py
+-rw-r--r--  2.0 unx     1404 b- defN 24-May-20 18:58 qai_hub_models/models/midas/conftest.py
+-rw-r--r--  2.0 unx     2144 b- defN 24-May-20 18:58 qai_hub_models/models/midas/demo.py
+-rw-r--r--  2.0 unx     8474 b- defN 24-May-20 18:58 qai_hub_models/models/midas/export.py
+-rw-r--r--  2.0 unx     1100 b- defN 24-May-20 18:58 qai_hub_models/models/midas/info.yaml
+-rw-r--r--  2.0 unx     1811 b- defN 24-May-20 18:58 qai_hub_models/models/midas/model.py
+-rw-r--r--  2.0 unx     1919 b- defN 24-May-20 18:58 qai_hub_models/models/midas/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-May-20 18:58 qai_hub_models/models/mnasnet05/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-20 18:58 qai_hub_models/models/mnasnet05/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-May-20 18:58 qai_hub_models/models/mnasnet05/demo.py
+-rw-r--r--  2.0 unx     8498 b- defN 24-May-20 18:58 qai_hub_models/models/mnasnet05/export.py
+-rw-r--r--  2.0 unx     1333 b- defN 24-May-20 18:58 qai_hub_models/models/mnasnet05/info.yaml
+-rw-r--r--  2.0 unx      699 b- defN 24-May-20 18:58 qai_hub_models/models/mnasnet05/model.py
+-rw-r--r--  2.0 unx     6026 b- defN 24-May-20 18:58 qai_hub_models/models/mnasnet05/perf.yaml
+-rw-r--r--  2.0 unx      882 b- defN 24-May-20 18:58 qai_hub_models/models/mnasnet05/test.py
+-rw-r--r--  2.0 unx      474 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2/__init__.py
+-rw-r--r--  2.0 unx     1411 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2/conftest.py
+-rw-r--r--  2.0 unx      540 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2/demo.py
+-rw-r--r--  2.0 unx     8510 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2/export.py
+-rw-r--r--  2.0 unx     1380 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2/info.yaml
+-rw-r--r--  2.0 unx     2457 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2/model.py
+-rw-r--r--  2.0 unx     6029 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2/perf.yaml
+-rw-r--r--  2.0 unx     1091 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2/test.py
+-rw-r--r--  2.0 unx      485 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2_quantized/__init__.py
+-rw-r--r--  2.0 unx     1421 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2_quantized/conftest.py
+-rw-r--r--  2.0 unx      585 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2_quantized/demo.py
+-rw-r--r--  2.0 unx     8974 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2_quantized/export.py
+-rw-r--r--  2.0 unx     1362 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2_quantized/info.yaml
+-rw-r--r--  2.0 unx     3429 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2_quantized/model.py
+-rw-r--r--  2.0 unx     7719 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2_quantized/perf.yaml
+-rw-r--r--  2.0 unx     1002 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v2_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large/__init__.py
+-rw-r--r--  2.0 unx     1323 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large/conftest.py
+-rw-r--r--  2.0 unx      556 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large/demo.py
+-rw-r--r--  2.0 unx     8534 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large/export.py
+-rw-r--r--  2.0 unx     1340 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large/info.yaml
+-rw-r--r--  2.0 unx      721 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large/model.py
+-rw-r--r--  2.0 unx     6038 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large/perf.yaml
+-rw-r--r--  2.0 unx      879 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large/test.py
+-rw-r--r--  2.0 unx      607 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large_quantized/__init__.py
+-rw-r--r--  2.0 unx     1333 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py
+-rw-r--r--  2.0 unx      748 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large_quantized/demo.py
+-rw-r--r--  2.0 unx     8950 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large_quantized/export.py
+-rw-r--r--  2.0 unx     1374 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large_quantized/info.yaml
+-rw-r--r--  2.0 unx     3075 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large_quantized/model.py
+-rw-r--r--  2.0 unx     7744 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml
+-rw-r--r--  2.0 unx      917 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_large_quantized/test.py
+-rw-r--r--  2.0 unx      479 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_small/__init__.py
+-rw-r--r--  2.0 unx     1323 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_small/conftest.py
+-rw-r--r--  2.0 unx      556 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_small/demo.py
+-rw-r--r--  2.0 unx     8534 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_small/export.py
+-rw-r--r--  2.0 unx     1338 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_small/info.yaml
+-rw-r--r--  2.0 unx      721 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_small/model.py
+-rw-r--r--  2.0 unx     6042 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_small/perf.yaml
+-rw-r--r--  2.0 unx      879 b- defN 24-May-20 18:58 qai_hub_models/models/mobilenet_v3_small/test.py
+-rw-r--r--  2.0 unx      394 b- defN 24-May-20 18:58 qai_hub_models/models/openai_clip/__init__.py
+-rw-r--r--  2.0 unx     3958 b- defN 24-May-20 18:58 qai_hub_models/models/openai_clip/app.py
+-rw-r--r--  2.0 unx     1410 b- defN 24-May-20 18:58 qai_hub_models/models/openai_clip/conftest.py
+-rw-r--r--  2.0 unx     3404 b- defN 24-May-20 18:58 qai_hub_models/models/openai_clip/demo.py
+-rw-r--r--  2.0 unx     9933 b- defN 24-May-20 18:58 qai_hub_models/models/openai_clip/export.py
+-rw-r--r--  2.0 unx     1494 b- defN 24-May-20 18:58 qai_hub_models/models/openai_clip/info.yaml
+-rw-r--r--  2.0 unx     5374 b- defN 24-May-20 18:58 qai_hub_models/models/openai_clip/model.py
+-rw-r--r--  2.0 unx    11341 b- defN 24-May-20 18:58 qai_hub_models/models/openai_clip/perf.yaml
+-rw-r--r--  2.0 unx       29 b- defN 24-May-20 18:58 qai_hub_models/models/openai_clip/requirements.txt
+-rw-r--r--  2.0 unx     2118 b- defN 24-May-20 18:58 qai_hub_models/models/openai_clip/test.py
+-rw-r--r--  2.0 unx      402 b- defN 24-May-20 18:58 qai_hub_models/models/openpose/__init__.py
+-rw-r--r--  2.0 unx    12008 b- defN 24-May-20 18:58 qai_hub_models/models/openpose/app.py
+-rw-r--r--  2.0 unx     1407 b- defN 24-May-20 18:58 qai_hub_models/models/openpose/conftest.py
+-rw-r--r--  2.0 unx     2053 b- defN 24-May-20 18:58 qai_hub_models/models/openpose/demo.py
+-rw-r--r--  2.0 unx     8860 b- defN 24-May-20 18:58 qai_hub_models/models/openpose/export.py
+-rw-r--r--  2.0 unx     1247 b- defN 24-May-20 18:58 qai_hub_models/models/openpose/info.yaml
+-rw-r--r--  2.0 unx     5084 b- defN 24-May-20 18:58 qai_hub_models/models/openpose/model.py
+-rw-r--r--  2.0 unx     6061 b- defN 24-May-20 18:58 qai_hub_models/models/openpose/perf.yaml
+-rw-r--r--  2.0 unx       31 b- defN 24-May-20 18:58 qai_hub_models/models/openpose/requirements.txt
+-rw-r--r--  2.0 unx     1321 b- defN 24-May-20 18:58 qai_hub_models/models/openpose/test.py
+-rw-r--r--  2.0 unx      402 b- defN 24-May-20 18:58 qai_hub_models/models/posenet_mobilenet/__init__.py
+-rw-r--r--  2.0 unx    20176 b- defN 24-May-20 18:58 qai_hub_models/models/posenet_mobilenet/app.py
+-rw-r--r--  2.0 unx     1416 b- defN 24-May-20 18:58 qai_hub_models/models/posenet_mobilenet/conftest.py
+-rw-r--r--  2.0 unx     1998 b- defN 24-May-20 18:58 qai_hub_models/models/posenet_mobilenet/demo.py
+-rw-r--r--  2.0 unx     8486 b- defN 24-May-20 18:58 qai_hub_models/models/posenet_mobilenet/export.py
+-rw-r--r--  2.0 unx     1221 b- defN 24-May-20 18:58 qai_hub_models/models/posenet_mobilenet/info.yaml
+-rw-r--r--  2.0 unx     2661 b- defN 24-May-20 18:58 qai_hub_models/models/posenet_mobilenet/model.py
+-rw-r--r--  2.0 unx     1757 b- defN 24-May-20 18:58 qai_hub_models/models/posenet_mobilenet/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge/__init__.py
+-rw-r--r--  2.0 unx     1414 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge/conftest.py
+-rw-r--r--  2.0 unx      972 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge/demo.py
+-rw-r--r--  2.0 unx     8862 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge/export.py
+-rw-r--r--  2.0 unx     1248 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge/info.yaml
+-rw-r--r--  2.0 unx     3140 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge/model.py
+-rw-r--r--  2.0 unx     6029 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge/perf.yaml
+-rw-r--r--  2.0 unx     1422 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge_quantized/__init__.py
+-rw-r--r--  2.0 unx     1424 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge_quantized/conftest.py
+-rw-r--r--  2.0 unx      891 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge_quantized/demo.py
+-rw-r--r--  2.0 unx     9278 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge_quantized/export.py
+-rw-r--r--  2.0 unx     1274 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge_quantized/info.yaml
+-rw-r--r--  2.0 unx     2980 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge_quantized/model.py
+-rw-r--r--  2.0 unx     7710 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2925 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetlarge_quantized/test.py
+-rw-r--r--  2.0 unx      473 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium/__init__.py
+-rw-r--r--  2.0 unx     1415 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium/conftest.py
+-rw-r--r--  2.0 unx      976 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium/demo.py
+-rw-r--r--  2.0 unx     8866 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium/export.py
+-rw-r--r--  2.0 unx     1242 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium/info.yaml
+-rw-r--r--  2.0 unx     3217 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium/model.py
+-rw-r--r--  2.0 unx     6013 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium/perf.yaml
+-rw-r--r--  2.0 unx     1428 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium/test.py
+-rw-r--r--  2.0 unx      484 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium_quantized/__init__.py
+-rw-r--r--  2.0 unx     1425 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium_quantized/conftest.py
+-rw-r--r--  2.0 unx      900 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium_quantized/demo.py
+-rw-r--r--  2.0 unx     9282 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium_quantized/export.py
+-rw-r--r--  2.0 unx     1282 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium_quantized/info.yaml
+-rw-r--r--  2.0 unx     2988 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium_quantized/model.py
+-rw-r--r--  2.0 unx     7724 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2916 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetmedium_quantized/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall/__init__.py
+-rw-r--r--  2.0 unx     1414 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall/conftest.py
+-rw-r--r--  2.0 unx      972 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall/demo.py
+-rw-r--r--  2.0 unx     8862 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall/export.py
+-rw-r--r--  2.0 unx     1238 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall/info.yaml
+-rw-r--r--  2.0 unx     3140 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall/model.py
+-rw-r--r--  2.0 unx     6010 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall/perf.yaml
+-rw-r--r--  2.0 unx     1422 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall_quantized/__init__.py
+-rw-r--r--  2.0 unx     1424 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall_quantized/conftest.py
+-rw-r--r--  2.0 unx      891 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall_quantized/demo.py
+-rw-r--r--  2.0 unx     9278 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall_quantized/export.py
+-rw-r--r--  2.0 unx     1278 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall_quantized/info.yaml
+-rw-r--r--  2.0 unx     2975 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall_quantized/model.py
+-rw-r--r--  2.0 unx     7711 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2863 b- defN 24-May-20 18:58 qai_hub_models/models/quicksrnetsmall_quantized/test.py
+-rw-r--r--  2.0 unx      481 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_general_x4v3/__init__.py
+-rw-r--r--  2.0 unx     1423 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_general_x4v3/conftest.py
+-rw-r--r--  2.0 unx     1280 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_general_x4v3/demo.py
+-rw-r--r--  2.0 unx     8898 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_general_x4v3/export.py
+-rw-r--r--  2.0 unx     1206 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_general_x4v3/info.yaml
+-rw-r--r--  2.0 unx     5435 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_general_x4v3/model.py
+-rw-r--r--  2.0 unx     6051 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml
+-rw-r--r--  2.0 unx       44 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_general_x4v3/requirements.txt
+-rw-r--r--  2.0 unx     1480 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_general_x4v3/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_x4plus/__init__.py
+-rw-r--r--  2.0 unx     1417 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_x4plus/conftest.py
+-rw-r--r--  2.0 unx     1256 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_x4plus/demo.py
+-rw-r--r--  2.0 unx     7927 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_x4plus/export.py
+-rw-r--r--  2.0 unx     1330 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_x4plus/info.yaml
+-rw-r--r--  2.0 unx     4443 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_x4plus/model.py
+-rw-r--r--  2.0 unx     6100 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_x4plus/perf.yaml
+-rw-r--r--  2.0 unx       44 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_x4plus/requirements.txt
+-rw-r--r--  2.0 unx     1440 b- defN 24-May-20 18:58 qai_hub_models/models/real_esrgan_x4plus/test.py
+-rw-r--r--  2.0 unx      469 b- defN 24-May-20 18:58 qai_hub_models/models/regnet/__init__.py
+-rw-r--r--  2.0 unx     1311 b- defN 24-May-20 18:58 qai_hub_models/models/regnet/conftest.py
+-rw-r--r--  2.0 unx      524 b- defN 24-May-20 18:58 qai_hub_models/models/regnet/demo.py
+-rw-r--r--  2.0 unx     8486 b- defN 24-May-20 18:58 qai_hub_models/models/regnet/export.py
+-rw-r--r--  2.0 unx     1291 b- defN 24-May-20 18:58 qai_hub_models/models/regnet/info.yaml
+-rw-r--r--  2.0 unx      635 b- defN 24-May-20 18:58 qai_hub_models/models/regnet/model.py
+-rw-r--r--  2.0 unx     6033 b- defN 24-May-20 18:58 qai_hub_models/models/regnet/perf.yaml
+-rw-r--r--  2.0 unx      984 b- defN 24-May-20 18:58 qai_hub_models/models/regnet/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101/demo.py
+-rw-r--r--  2.0 unx     8498 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101/export.py
+-rw-r--r--  2.0 unx     1312 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101/info.yaml
+-rw-r--r--  2.0 unx      609 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101/model.py
+-rw-r--r--  2.0 unx     6052 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101/perf.yaml
+-rw-r--r--  2.0 unx      961 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101_quantized/__init__.py
+-rw-r--r--  2.0 unx     1324 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101_quantized/conftest.py
+-rw-r--r--  2.0 unx      578 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101_quantized/demo.py
+-rw-r--r--  2.0 unx     8961 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101_quantized/export.py
+-rw-r--r--  2.0 unx     1346 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101_quantized/info.yaml
+-rw-r--r--  2.0 unx     3210 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101_quantized/model.py
+-rw-r--r--  2.0 unx     7756 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101_quantized/perf.yaml
+-rw-r--r--  2.0 unx      921 b- defN 24-May-20 18:58 qai_hub_models/models/resnet101_quantized/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18/__init__.py
+-rw-r--r--  2.0 unx     1313 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18/conftest.py
+-rw-r--r--  2.0 unx      530 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18/demo.py
+-rw-r--r--  2.0 unx     8494 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18/export.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18/info.yaml
+-rw-r--r--  2.0 unx      607 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18/model.py
+-rw-r--r--  2.0 unx     6014 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18/perf.yaml
+-rw-r--r--  2.0 unx      955 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18/test.py
+-rw-r--r--  2.0 unx      482 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18_quantized/__init__.py
+-rw-r--r--  2.0 unx     1323 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18_quantized/conftest.py
+-rw-r--r--  2.0 unx      562 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18_quantized/demo.py
+-rw-r--r--  2.0 unx     8957 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18_quantized/export.py
+-rw-r--r--  2.0 unx     1343 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18_quantized/info.yaml
+-rw-r--r--  2.0 unx     3012 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18_quantized/model.py
+-rw-r--r--  2.0 unx     7715 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18_quantized/perf.yaml
+-rw-r--r--  2.0 unx      917 b- defN 24-May-20 18:58 qai_hub_models/models/resnet18_quantized/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-May-20 18:58 qai_hub_models/models/resnet50/__init__.py
+-rw-r--r--  2.0 unx     1313 b- defN 24-May-20 18:58 qai_hub_models/models/resnet50/conftest.py
+-rw-r--r--  2.0 unx      530 b- defN 24-May-20 18:58 qai_hub_models/models/resnet50/demo.py
+-rw-r--r--  2.0 unx     8494 b- defN 24-May-20 18:58 qai_hub_models/models/resnet50/export.py
+-rw-r--r--  2.0 unx     1303 b- defN 24-May-20 18:58 qai_hub_models/models/resnet50/info.yaml
+-rw-r--r--  2.0 unx      607 b- defN 24-May-20 18:58 qai_hub_models/models/resnet50/model.py
+-rw-r--r--  2.0 unx     6045 b- defN 24-May-20 18:58 qai_hub_models/models/resnet50/perf.yaml
+-rw-r--r--  2.0 unx      955 b- defN 24-May-20 18:58 qai_hub_models/models/resnet50/test.py
+-rw-r--r--  2.0 unx      473 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101/__init__.py
+-rw-r--r--  2.0 unx     1315 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101/conftest.py
+-rw-r--r--  2.0 unx      536 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101/demo.py
+-rw-r--r--  2.0 unx     8502 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101/export.py
+-rw-r--r--  2.0 unx     1324 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101/info.yaml
+-rw-r--r--  2.0 unx      617 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101/model.py
+-rw-r--r--  2.0 unx     6050 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101/perf.yaml
+-rw-r--r--  2.0 unx      897 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101/test.py
+-rw-r--r--  2.0 unx      484 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101_quantized/__init__.py
+-rw-r--r--  2.0 unx     1325 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101_quantized/conftest.py
+-rw-r--r--  2.0 unx      581 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101_quantized/demo.py
+-rw-r--r--  2.0 unx     8965 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101_quantized/export.py
+-rw-r--r--  2.0 unx     1365 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101_quantized/info.yaml
+-rw-r--r--  2.0 unx     3017 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101_quantized/model.py
+-rw-r--r--  2.0 unx     7769 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101_quantized/perf.yaml
+-rw-r--r--  2.0 unx      925 b- defN 24-May-20 18:58 qai_hub_models/models/resnext101_quantized/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50/conftest.py
+-rw-r--r--  2.0 unx      533 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50/demo.py
+-rw-r--r--  2.0 unx     8498 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50/export.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50/info.yaml
+-rw-r--r--  2.0 unx      704 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50/model.py
+-rw-r--r--  2.0 unx     6041 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50/perf.yaml
+-rw-r--r--  2.0 unx      840 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50/test.py
+-rw-r--r--  2.0 unx      483 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50_quantized/__init__.py
+-rw-r--r--  2.0 unx     1324 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50_quantized/conftest.py
+-rw-r--r--  2.0 unx      578 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50_quantized/demo.py
+-rw-r--r--  2.0 unx     8961 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50_quantized/export.py
+-rw-r--r--  2.0 unx     1362 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50_quantized/info.yaml
+-rw-r--r--  2.0 unx     3008 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50_quantized/model.py
+-rw-r--r--  2.0 unx     7731 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50_quantized/perf.yaml
+-rw-r--r--  2.0 unx      921 b- defN 24-May-20 18:58 qai_hub_models/models/resnext50_quantized/test.py
+-rw-r--r--  2.0 unx      453 b- defN 24-May-20 18:58 qai_hub_models/models/riffusion_quantized/__init__.py
+-rw-r--r--  2.0 unx     1714 b- defN 24-May-20 18:58 qai_hub_models/models/riffusion_quantized/demo.py
+-rw-r--r--  2.0 unx     7676 b- defN 24-May-20 18:58 qai_hub_models/models/riffusion_quantized/export.py
+-rw-r--r--  2.0 unx     1422 b- defN 24-May-20 18:58 qai_hub_models/models/riffusion_quantized/info.yaml
+-rw-r--r--  2.0 unx     3452 b- defN 24-May-20 18:58 qai_hub_models/models/riffusion_quantized/model.py
+-rw-r--r--  2.0 unx       46 b- defN 24-May-20 18:58 qai_hub_models/models/riffusion_quantized/requirements.txt
+-rw-r--r--  2.0 unx      995 b- defN 24-May-20 18:58 qai_hub_models/models/riffusion_quantized/test.py
+-rw-r--r--  2.0 unx      404 b- defN 24-May-20 18:58 qai_hub_models/models/sam/__init__.py
+-rw-r--r--  2.0 unx     5101 b- defN 24-May-20 18:58 qai_hub_models/models/sam/app.py
+-rw-r--r--  2.0 unx     1402 b- defN 24-May-20 18:58 qai_hub_models/models/sam/conftest.py
+-rw-r--r--  2.0 unx     3088 b- defN 24-May-20 18:58 qai_hub_models/models/sam/demo.py
+-rw-r--r--  2.0 unx    10440 b- defN 24-May-20 18:58 qai_hub_models/models/sam/export.py
+-rw-r--r--  2.0 unx     1391 b- defN 24-May-20 18:58 qai_hub_models/models/sam/info.yaml
+-rw-r--r--  2.0 unx    12018 b- defN 24-May-20 18:58 qai_hub_models/models/sam/model.py
+-rw-r--r--  2.0 unx     8143 b- defN 24-May-20 18:58 qai_hub_models/models/sam/perf.yaml
+-rw-r--r--  2.0 unx       37 b- defN 24-May-20 18:58 qai_hub_models/models/sam/requirements.txt
+-rw-r--r--  2.0 unx     3062 b- defN 24-May-20 18:58 qai_hub_models/models/sam/test.py
+-rw-r--r--  2.0 unx      826 b- defN 24-May-20 18:58 qai_hub_models/models/sam/utils.py
+-rw-r--r--  2.0 unx      464 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5/__init__.py
+-rw-r--r--  2.0 unx     1406 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5/conftest.py
+-rw-r--r--  2.0 unx      923 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5/demo.py
+-rw-r--r--  2.0 unx     8687 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5/export.py
+-rw-r--r--  2.0 unx     1104 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5/info.yaml
+-rw-r--r--  2.0 unx     2984 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5/model.py
+-rw-r--r--  2.0 unx     6005 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5/perf.yaml
+-rw-r--r--  2.0 unx     1471 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5_quantized/__init__.py
+-rw-r--r--  2.0 unx     1416 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5_quantized/conftest.py
+-rw-r--r--  2.0 unx      990 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5_quantized/demo.py
+-rw-r--r--  2.0 unx     8719 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5_quantized/export.py
+-rw-r--r--  2.0 unx     1151 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5_quantized/info.yaml
+-rw-r--r--  2.0 unx     3956 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5_quantized/model.py
+-rw-r--r--  2.0 unx     7726 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5_quantized/perf.yaml
+-rw-r--r--  2.0 unx     2931 b- defN 24-May-20 18:58 qai_hub_models/models/sesr_m5_quantized/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2/__init__.py
+-rw-r--r--  2.0 unx     1318 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2/conftest.py
+-rw-r--r--  2.0 unx      543 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2/demo.py
+-rw-r--r--  2.0 unx     8514 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2/export.py
+-rw-r--r--  2.0 unx     1353 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2/info.yaml
+-rw-r--r--  2.0 unx      713 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2/model.py
+-rw-r--r--  2.0 unx     6037 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2/perf.yaml
+-rw-r--r--  2.0 unx      857 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2/test.py
+-rw-r--r--  2.0 unx      584 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2_quantized/__init__.py
+-rw-r--r--  2.0 unx     1328 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2_quantized/conftest.py
+-rw-r--r--  2.0 unx      588 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2_quantized/demo.py
+-rw-r--r--  2.0 unx     8997 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2_quantized/export.py
+-rw-r--r--  2.0 unx     1383 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2_quantized/info.yaml
+-rw-r--r--  2.0 unx     5989 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2_quantized/model.py
+-rw-r--r--  2.0 unx     7689 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2_quantized/perf.yaml
+-rw-r--r--  2.0 unx      899 b- defN 24-May-20 18:58 qai_hub_models/models/shufflenet_v2_quantized/test.py
+-rw-r--r--  2.0 unx      396 b- defN 24-May-20 18:58 qai_hub_models/models/sinet/__init__.py
+-rw-r--r--  2.0 unx     3793 b- defN 24-May-20 18:58 qai_hub_models/models/sinet/app.py
+-rw-r--r--  2.0 unx     1404 b- defN 24-May-20 18:58 qai_hub_models/models/sinet/conftest.py
+-rw-r--r--  2.0 unx     1657 b- defN 24-May-20 18:58 qai_hub_models/models/sinet/demo.py
+-rw-r--r--  2.0 unx     8822 b- defN 24-May-20 18:58 qai_hub_models/models/sinet/export.py
+-rw-r--r--  2.0 unx     1260 b- defN 24-May-20 18:58 qai_hub_models/models/sinet/info.yaml
+-rw-r--r--  2.0 unx     4770 b- defN 24-May-20 18:58 qai_hub_models/models/sinet/model.py
+-rw-r--r--  2.0 unx     6033 b- defN 24-May-20 18:58 qai_hub_models/models/sinet/perf.yaml
+-rw-r--r--  2.0 unx     1355 b- defN 24-May-20 18:58 qai_hub_models/models/sinet/test.py
+-rw-r--r--  2.0 unx      473 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1/__init__.py
+-rw-r--r--  2.0 unx     1318 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1/conftest.py
+-rw-r--r--  2.0 unx      539 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1/demo.py
+-rw-r--r--  2.0 unx     8515 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1/export.py
+-rw-r--r--  2.0 unx     1325 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1/info.yaml
+-rw-r--r--  2.0 unx      696 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1/model.py
+-rw-r--r--  2.0 unx     6015 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1/perf.yaml
+-rw-r--r--  2.0 unx      851 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1/test.py
+-rw-r--r--  2.0 unx      582 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1_quantized/__init__.py
+-rw-r--r--  2.0 unx     1328 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1_quantized/conftest.py
+-rw-r--r--  2.0 unx      584 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1_quantized/demo.py
+-rw-r--r--  2.0 unx     8930 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1_quantized/export.py
+-rw-r--r--  2.0 unx     1358 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1_quantized/info.yaml
+-rw-r--r--  2.0 unx     3023 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1_quantized/model.py
+-rw-r--r--  2.0 unx     7723 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1_quantized/perf.yaml
+-rw-r--r--  2.0 unx      895 b- defN 24-May-20 18:58 qai_hub_models/models/squeezenet1_1_quantized/test.py
+-rw-r--r--  2.0 unx      492 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v1_5_quantized/__init__.py
+-rw-r--r--  2.0 unx     1735 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v1_5_quantized/demo.py
+-rw-r--r--  2.0 unx     7724 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v1_5_quantized/export.py
+-rw-r--r--  2.0 unx     1401 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v1_5_quantized/info.yaml
+-rw-r--r--  2.0 unx     3604 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v1_5_quantized/model.py
+-rw-r--r--  2.0 unx     6384 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v1_5_quantized/perf.yaml
+-rw-r--r--  2.0 unx       46 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v1_5_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1052 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v1_5_quantized/test.py
+-rw-r--r--  2.0 unx      492 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v2_1_quantized/__init__.py
+-rw-r--r--  2.0 unx     1811 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v2_1_quantized/demo.py
+-rw-r--r--  2.0 unx     7724 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v2_1_quantized/export.py
+-rw-r--r--  2.0 unx     1401 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v2_1_quantized/info.yaml
+-rw-r--r--  2.0 unx     3469 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v2_1_quantized/model.py
+-rw-r--r--  2.0 unx       46 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v2_1_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1056 b- defN 24-May-20 18:58 qai_hub_models/models/stable_diffusion_v2_1_quantized/test.py
+-rw-r--r--  2.0 unx      404 b- defN 24-May-20 18:58 qai_hub_models/models/stylegan2/__init__.py
+-rw-r--r--  2.0 unx     4155 b- defN 24-May-20 18:58 qai_hub_models/models/stylegan2/app.py
+-rw-r--r--  2.0 unx     1408 b- defN 24-May-20 18:58 qai_hub_models/models/stylegan2/conftest.py
+-rw-r--r--  2.0 unx     2758 b- defN 24-May-20 18:58 qai_hub_models/models/stylegan2/demo.py
+-rw-r--r--  2.0 unx     8404 b- defN 24-May-20 18:58 qai_hub_models/models/stylegan2/export.py
+-rw-r--r--  2.0 unx     1084 b- defN 24-May-20 18:58 qai_hub_models/models/stylegan2/info.yaml
+-rw-r--r--  2.0 unx     8870 b- defN 24-May-20 18:58 qai_hub_models/models/stylegan2/model.py
+-rw-r--r--  2.0 unx     5595 b- defN 24-May-20 18:58 qai_hub_models/models/stylegan2/perf.yaml
+-rw-r--r--  2.0 unx       13 b- defN 24-May-20 18:58 qai_hub_models/models/stylegan2/requirements.txt
+-rw-r--r--  2.0 unx     2497 b- defN 24-May-20 18:58 qai_hub_models/models/stylegan2/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-May-20 18:58 qai_hub_models/models/swin_base/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-20 18:58 qai_hub_models/models/swin_base/conftest.py
+-rw-r--r--  2.0 unx      531 b- defN 24-May-20 18:58 qai_hub_models/models/swin_base/demo.py
+-rw-r--r--  2.0 unx     8498 b- defN 24-May-20 18:58 qai_hub_models/models/swin_base/export.py
+-rw-r--r--  2.0 unx     1383 b- defN 24-May-20 18:58 qai_hub_models/models/swin_base/info.yaml
+-rw-r--r--  2.0 unx     1241 b- defN 24-May-20 18:58 qai_hub_models/models/swin_base/model.py
+-rw-r--r--  2.0 unx     6053 b- defN 24-May-20 18:58 qai_hub_models/models/swin_base/perf.yaml
+-rw-r--r--  2.0 unx     1358 b- defN 24-May-20 18:58 qai_hub_models/models/swin_base/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-May-20 18:58 qai_hub_models/models/swin_small/__init__.py
+-rw-r--r--  2.0 unx     1315 b- defN 24-May-20 18:58 qai_hub_models/models/swin_small/conftest.py
+-rw-r--r--  2.0 unx      534 b- defN 24-May-20 18:58 qai_hub_models/models/swin_small/demo.py
+-rw-r--r--  2.0 unx     8502 b- defN 24-May-20 18:58 qai_hub_models/models/swin_small/export.py
+-rw-r--r--  2.0 unx     1378 b- defN 24-May-20 18:58 qai_hub_models/models/swin_small/info.yaml
+-rw-r--r--  2.0 unx     1242 b- defN 24-May-20 18:58 qai_hub_models/models/swin_small/model.py
+-rw-r--r--  2.0 unx     6080 b- defN 24-May-20 18:58 qai_hub_models/models/swin_small/perf.yaml
+-rw-r--r--  2.0 unx     1364 b- defN 24-May-20 18:58 qai_hub_models/models/swin_small/test.py
+-rw-r--r--  2.0 unx      471 b- defN 24-May-20 18:58 qai_hub_models/models/swin_tiny/__init__.py
+-rw-r--r--  2.0 unx     1314 b- defN 24-May-20 18:58 qai_hub_models/models/swin_tiny/conftest.py
+-rw-r--r--  2.0 unx      531 b- defN 24-May-20 18:58 qai_hub_models/models/swin_tiny/demo.py
+-rw-r--r--  2.0 unx     8498 b- defN 24-May-20 18:58 qai_hub_models/models/swin_tiny/export.py
+-rw-r--r--  2.0 unx     1376 b- defN 24-May-20 18:58 qai_hub_models/models/swin_tiny/info.yaml
+-rw-r--r--  2.0 unx     1241 b- defN 24-May-20 18:58 qai_hub_models/models/swin_tiny/model.py
+-rw-r--r--  2.0 unx     6025 b- defN 24-May-20 18:58 qai_hub_models/models/swin_tiny/perf.yaml
+-rw-r--r--  2.0 unx     1476 b- defN 24-May-20 18:58 qai_hub_models/models/swin_tiny/test.py
+-rw-r--r--  2.0 unx      396 b- defN 24-May-20 18:58 qai_hub_models/models/trocr/__init__.py
+-rw-r--r--  2.0 unx    10207 b- defN 24-May-20 18:58 qai_hub_models/models/trocr/app.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-May-20 18:58 qai_hub_models/models/trocr/conftest.py
+-rw-r--r--  2.0 unx     1779 b- defN 24-May-20 18:58 qai_hub_models/models/trocr/demo.py
+-rw-r--r--  2.0 unx     9911 b- defN 24-May-20 18:58 qai_hub_models/models/trocr/export.py
+-rw-r--r--  2.0 unx     1370 b- defN 24-May-20 18:58 qai_hub_models/models/trocr/info.yaml
+-rw-r--r--  2.0 unx    10482 b- defN 24-May-20 18:58 qai_hub_models/models/trocr/model.py
+-rw-r--r--  2.0 unx    10503 b- defN 24-May-20 18:58 qai_hub_models/models/trocr/perf.yaml
+-rw-r--r--  2.0 unx       42 b- defN 24-May-20 18:58 qai_hub_models/models/trocr/requirements.txt
+-rw-r--r--  2.0 unx     2357 b- defN 24-May-20 18:58 qai_hub_models/models/trocr/test.py
+-rw-r--r--  2.0 unx      348 b- defN 24-May-20 18:58 qai_hub_models/models/unet_segmentation/__init__.py
+-rw-r--r--  2.0 unx     1305 b- defN 24-May-20 18:58 qai_hub_models/models/unet_segmentation/app.py
+-rw-r--r--  2.0 unx     1322 b- defN 24-May-20 18:58 qai_hub_models/models/unet_segmentation/conftest.py
+-rw-r--r--  2.0 unx     2509 b- defN 24-May-20 18:58 qai_hub_models/models/unet_segmentation/demo.py
+-rw-r--r--  2.0 unx     8870 b- defN 24-May-20 18:58 qai_hub_models/models/unet_segmentation/export.py
+-rw-r--r--  2.0 unx     1310 b- defN 24-May-20 18:58 qai_hub_models/models/unet_segmentation/info.yaml
+-rw-r--r--  2.0 unx     2666 b- defN 24-May-20 18:58 qai_hub_models/models/unet_segmentation/model.py
+-rw-r--r--  2.0 unx     6075 b- defN 24-May-20 18:58 qai_hub_models/models/unet_segmentation/perf.yaml
+-rw-r--r--  2.0 unx     1215 b- defN 24-May-20 18:58 qai_hub_models/models/unet_segmentation/test.py
+-rw-r--r--  2.0 unx      466 b- defN 24-May-20 18:58 qai_hub_models/models/vit/__init__.py
+-rw-r--r--  2.0 unx     1308 b- defN 24-May-20 18:58 qai_hub_models/models/vit/conftest.py
+-rw-r--r--  2.0 unx      515 b- defN 24-May-20 18:58 qai_hub_models/models/vit/demo.py
+-rw-r--r--  2.0 unx     8527 b- defN 24-May-20 18:58 qai_hub_models/models/vit/export.py
+-rw-r--r--  2.0 unx     1342 b- defN 24-May-20 18:58 qai_hub_models/models/vit/info.yaml
+-rw-r--r--  2.0 unx      685 b- defN 24-May-20 18:58 qai_hub_models/models/vit/model.py
+-rw-r--r--  2.0 unx     6035 b- defN 24-May-20 18:58 qai_hub_models/models/vit/perf.yaml
+-rw-r--r--  2.0 unx      807 b- defN 24-May-20 18:58 qai_hub_models/models/vit/test.py
+-rw-r--r--  2.0 unx      444 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_base_en/__init__.py
+-rw-r--r--  2.0 unx     1320 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_base_en/conftest.py
+-rw-r--r--  2.0 unx      483 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_base_en/demo.py
+-rw-r--r--  2.0 unx     9929 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_base_en/export.py
+-rw-r--r--  2.0 unx     1849 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_base_en/info.yaml
+-rw-r--r--  2.0 unx      558 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_base_en/model.py
+-rw-r--r--  2.0 unx    11352 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_base_en/perf.yaml
+-rw-r--r--  2.0 unx       31 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_base_en/requirements.txt
+-rw-r--r--  2.0 unx      696 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_base_en/test.py
+-rw-r--r--  2.0 unx      445 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_small_en/__init__.py
+-rw-r--r--  2.0 unx     1321 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_small_en/conftest.py
+-rw-r--r--  2.0 unx      486 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_small_en/demo.py
+-rw-r--r--  2.0 unx     9933 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_small_en/export.py
+-rw-r--r--  2.0 unx     1848 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_small_en/info.yaml
+-rw-r--r--  2.0 unx      560 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_small_en/model.py
+-rw-r--r--  2.0 unx    11327 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_small_en/perf.yaml
+-rw-r--r--  2.0 unx       38 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_small_en/requirements.txt
+-rw-r--r--  2.0 unx      696 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_small_en/test.py
+-rw-r--r--  2.0 unx      444 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_tiny_en/__init__.py
+-rw-r--r--  2.0 unx     1320 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_tiny_en/conftest.py
+-rw-r--r--  2.0 unx      483 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_tiny_en/demo.py
+-rw-r--r--  2.0 unx     9929 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_tiny_en/export.py
+-rw-r--r--  2.0 unx     1849 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_tiny_en/info.yaml
+-rw-r--r--  2.0 unx      558 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_tiny_en/model.py
+-rw-r--r--  2.0 unx    11252 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_tiny_en/perf.yaml
+-rw-r--r--  2.0 unx       31 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_tiny_en/requirements.txt
+-rw-r--r--  2.0 unx      696 b- defN 24-May-20 18:58 qai_hub_models/models/whisper_tiny_en/test.py
+-rw-r--r--  2.0 unx      475 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50/__init__.py
+-rw-r--r--  2.0 unx     1317 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50/conftest.py
+-rw-r--r--  2.0 unx      542 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50/demo.py
+-rw-r--r--  2.0 unx     8510 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50/export.py
+-rw-r--r--  2.0 unx     1298 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50/info.yaml
+-rw-r--r--  2.0 unx      710 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50/model.py
+-rw-r--r--  2.0 unx     6048 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50/perf.yaml
+-rw-r--r--  2.0 unx      855 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50/test.py
+-rw-r--r--  2.0 unx      582 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50_quantized/__init__.py
+-rw-r--r--  2.0 unx     1327 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50_quantized/conftest.py
+-rw-r--r--  2.0 unx      587 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50_quantized/demo.py
+-rw-r--r--  2.0 unx     8926 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50_quantized/export.py
+-rw-r--r--  2.0 unx     1333 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50_quantized/info.yaml
+-rw-r--r--  2.0 unx     3214 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50_quantized/model.py
+-rw-r--r--  2.0 unx     7729 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50_quantized/perf.yaml
+-rw-r--r--  2.0 unx      932 b- defN 24-May-20 18:58 qai_hub_models/models/wideresnet50_quantized/test.py
+-rw-r--r--  2.0 unx      461 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr/__init__.py
+-rw-r--r--  2.0 unx     1403 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr/conftest.py
+-rw-r--r--  2.0 unx      742 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr/demo.py
+-rw-r--r--  2.0 unx     8675 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr/export.py
+-rw-r--r--  2.0 unx     1156 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr/info.yaml
+-rw-r--r--  2.0 unx     3373 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr/model.py
+-rw-r--r--  2.0 unx     6009 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr/perf.yaml
+-rw-r--r--  2.0 unx     1402 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr/test.py
+-rw-r--r--  2.0 unx      472 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr_quantized/__init__.py
+-rw-r--r--  2.0 unx     1413 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr_quantized/conftest.py
+-rw-r--r--  2.0 unx      956 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr_quantized/demo.py
+-rw-r--r--  2.0 unx     9091 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr_quantized/export.py
+-rw-r--r--  2.0 unx     1191 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr_quantized/info.yaml
+-rw-r--r--  2.0 unx     2816 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr_quantized/model.py
+-rw-r--r--  2.0 unx     7716 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr_quantized/perf.yaml
+-rw-r--r--  2.0 unx     1607 b- defN 24-May-20 18:58 qai_hub_models/models/xlsr_quantized/test.py
+-rw-r--r--  2.0 unx      439 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas/__init__.py
+-rw-r--r--  2.0 unx     2185 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas/app.py
+-rw-r--r--  2.0 unx     1406 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas/conftest.py
+-rw-r--r--  2.0 unx      811 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas/demo.py
+-rw-r--r--  2.0 unx     8490 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas/export.py
+-rw-r--r--  2.0 unx     1270 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas/info.yaml
+-rw-r--r--  2.0 unx     6291 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas/model.py
+-rw-r--r--  2.0 unx     6062 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas/perf.yaml
+-rw-r--r--  2.0 unx      173 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas/requirements.txt
+-rw-r--r--  2.0 unx     1516 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas/test.py
+-rw-r--r--  2.0 unx      450 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas_quantized/__init__.py
+-rw-r--r--  2.0 unx     1416 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas_quantized/conftest.py
+-rw-r--r--  2.0 unx      854 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas_quantized/demo.py
+-rw-r--r--  2.0 unx     8946 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas_quantized/export.py
+-rw-r--r--  2.0 unx     1399 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas_quantized/info.yaml
+-rw-r--r--  2.0 unx     3232 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas_quantized/model.py
+-rw-r--r--  2.0 unx     7205 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas_quantized/perf.yaml
+-rw-r--r--  2.0 unx      173 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1583 b- defN 24-May-20 18:58 qai_hub_models/models/yolonas_quantized/test.py
+-rw-r--r--  2.0 unx      436 b- defN 24-May-20 18:58 qai_hub_models/models/yolov6/__init__.py
+-rw-r--r--  2.0 unx     1071 b- defN 24-May-20 18:58 qai_hub_models/models/yolov6/app.py
+-rw-r--r--  2.0 unx     1405 b- defN 24-May-20 18:58 qai_hub_models/models/yolov6/conftest.py
+-rw-r--r--  2.0 unx     1027 b- defN 24-May-20 18:58 qai_hub_models/models/yolov6/demo.py
+-rw-r--r--  2.0 unx     8486 b- defN 24-May-20 18:58 qai_hub_models/models/yolov6/export.py
+-rw-r--r--  2.0 unx     1168 b- defN 24-May-20 18:58 qai_hub_models/models/yolov6/info.yaml
+-rw-r--r--  2.0 unx     4677 b- defN 24-May-20 18:58 qai_hub_models/models/yolov6/model.py
+-rw-r--r--  2.0 unx     6058 b- defN 24-May-20 18:58 qai_hub_models/models/yolov6/perf.yaml
+-rw-r--r--  2.0 unx     1845 b- defN 24-May-20 18:58 qai_hub_models/models/yolov6/test.py
+-rw-r--r--  2.0 unx      436 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7/__init__.py
+-rw-r--r--  2.0 unx     2188 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7/app.py
+-rw-r--r--  2.0 unx     1405 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7/conftest.py
+-rw-r--r--  2.0 unx      909 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7/demo.py
+-rw-r--r--  2.0 unx     8506 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7/export.py
+-rw-r--r--  2.0 unx     1131 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7/info.yaml
+-rw-r--r--  2.0 unx    12010 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7/model.py
+-rw-r--r--  2.0 unx     5604 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7/perf.yaml
+-rw-r--r--  2.0 unx       98 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7/requirements.txt
+-rw-r--r--  2.0 unx     2322 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7/test.py
+-rw-r--r--  2.0 unx      447 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7_quantized/__init__.py
+-rw-r--r--  2.0 unx     1415 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7_quantized/conftest.py
+-rw-r--r--  2.0 unx      853 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7_quantized/demo.py
+-rw-r--r--  2.0 unx     8942 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7_quantized/export.py
+-rw-r--r--  2.0 unx     1285 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7_quantized/info.yaml
+-rw-r--r--  2.0 unx     3251 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7_quantized/model.py
+-rw-r--r--  2.0 unx     7224 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7_quantized/perf.yaml
+-rw-r--r--  2.0 unx       98 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1558 b- defN 24-May-20 18:58 qai_hub_models/models/yolov7_quantized/test.py
+-rw-r--r--  2.0 unx      415 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det/__init__.py
+-rw-r--r--  2.0 unx      892 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det/app.py
+-rw-r--r--  2.0 unx     1409 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det/conftest.py
+-rw-r--r--  2.0 unx      926 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det/demo.py
+-rw-r--r--  2.0 unx     8540 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det/export.py
+-rw-r--r--  2.0 unx     1171 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det/info.yaml
+-rw-r--r--  2.0 unx     8061 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det/model.py
+-rw-r--r--  2.0 unx     6067 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det/perf.yaml
+-rw-r--r--  2.0 unx      115 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det/requirements.txt
+-rw-r--r--  2.0 unx     2270 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det/test.py
+-rw-r--r--  2.0 unx      459 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det_quantized/__init__.py
+-rw-r--r--  2.0 unx     1419 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det_quantized/conftest.py
+-rw-r--r--  2.0 unx      804 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det_quantized/demo.py
+-rw-r--r--  2.0 unx     8963 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det_quantized/export.py
+-rw-r--r--  2.0 unx     1321 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det_quantized/info.yaml
+-rw-r--r--  2.0 unx     3540 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det_quantized/model.py
+-rw-r--r--  2.0 unx     7225 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det_quantized/perf.yaml
+-rw-r--r--  2.0 unx      115 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det_quantized/requirements.txt
+-rw-r--r--  2.0 unx     1542 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_det_quantized/test.py
+-rw-r--r--  2.0 unx      419 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_seg/__init__.py
+-rw-r--r--  2.0 unx     7698 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_seg/app.py
+-rw-r--r--  2.0 unx     1315 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_seg/conftest.py
+-rw-r--r--  2.0 unx     3155 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_seg/demo.py
+-rw-r--r--  2.0 unx     8563 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_seg/export.py
+-rw-r--r--  2.0 unx     1288 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_seg/info.yaml
+-rw-r--r--  2.0 unx     4663 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_seg/model.py
+-rw-r--r--  2.0 unx     6073 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_seg/perf.yaml
+-rw-r--r--  2.0 unx       64 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_seg/requirements.txt
+-rw-r--r--  2.0 unx     2536 b- defN 24-May-20 18:58 qai_hub_models/models/yolov8_seg/test.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/test/__init__.py
+-rw-r--r--  2.0 unx     1043 b- defN 24-May-20 18:58 qai_hub_models/test/test_async_compile_jobs.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/test/e2e/__init__.py
+-rw-r--r--  2.0 unx     1661 b- defN 24-May-20 18:58 qai_hub_models/test/e2e/test_aimet_compile.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/test/test_utils/__init__.py
+-rw-r--r--  2.0 unx     1493 b- defN 24-May-20 18:58 qai_hub_models/test/test_utils/perf.yaml
+-rw-r--r--  2.0 unx     3229 b- defN 24-May-20 18:58 qai_hub_models/test/test_utils/test_info_specs.py
+-rw-r--r--  2.0 unx     6525 b- defN 24-May-20 18:58 qai_hub_models/test/test_utils/test_perf_summary.py
+-rw-r--r--  2.0 unx     3295 b- defN 24-May-20 18:58 qai_hub_models/test/test_utils/test_qai_hub_helpers.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/utils/__init__.py
+-rw-r--r--  2.0 unx    17968 b- defN 24-May-20 18:58 qai_hub_models/utils/args.py
+-rw-r--r--  2.0 unx    35317 b- defN 24-May-20 18:58 qai_hub_models/utils/asset_loaders.py
+-rw-r--r--  2.0 unx     7243 b- defN 24-May-20 18:58 qai_hub_models/utils/base_model.py
+-rw-r--r--  2.0 unx     9261 b- defN 24-May-20 18:58 qai_hub_models/utils/bounding_box_processing.py
+-rw-r--r--  2.0 unx     1771 b- defN 24-May-20 18:58 qai_hub_models/utils/camera_capture.py
+-rw-r--r--  2.0 unx     5596 b- defN 24-May-20 18:58 qai_hub_models/utils/compare.py
+-rw-r--r--  2.0 unx    33382 b- defN 24-May-20 18:58 qai_hub_models/utils/config_loaders.py
+-rw-r--r--  2.0 unx     3066 b- defN 24-May-20 18:58 qai_hub_models/utils/display.py
+-rw-r--r--  2.0 unx     6359 b- defN 24-May-20 18:58 qai_hub_models/utils/draw.py
+-rw-r--r--  2.0 unx     3259 b- defN 24-May-20 18:58 qai_hub_models/utils/huggingface.py
+-rw-r--r--  2.0 unx    13609 b- defN 24-May-20 18:58 qai_hub_models/utils/image_processing.py
+-rw-r--r--  2.0 unx    13114 b- defN 24-May-20 18:58 qai_hub_models/utils/inference.py
+-rw-r--r--  2.0 unx     1308 b- defN 24-May-20 18:58 qai_hub_models/utils/input_spec.py
+-rw-r--r--  2.0 unx     4593 b- defN 24-May-20 18:58 qai_hub_models/utils/measurement.py
+-rw-r--r--  2.0 unx     1567 b- defN 24-May-20 18:58 qai_hub_models/utils/model_adapters.py
+-rw-r--r--  2.0 unx     1406 b- defN 24-May-20 18:58 qai_hub_models/utils/path_helpers.py
+-rw-r--r--  2.0 unx     5953 b- defN 24-May-20 18:58 qai_hub_models/utils/printing.py
+-rw-r--r--  2.0 unx     5378 b- defN 24-May-20 18:58 qai_hub_models/utils/qai_hub_helpers.py
+-rw-r--r--  2.0 unx     1463 b- defN 24-May-20 18:58 qai_hub_models/utils/qnn_helpers.py
+-rw-r--r--  2.0 unx     2170 b- defN 24-May-20 18:58 qai_hub_models/utils/quantization.py
+-rw-r--r--  2.0 unx    19925 b- defN 24-May-20 18:58 qai_hub_models/utils/quantization_aimet.py
+-rw-r--r--  2.0 unx     2278 b- defN 24-May-20 18:58 qai_hub_models/utils/system_info.py
+-rw-r--r--  2.0 unx      754 b- defN 24-May-20 18:58 qai_hub_models/utils/test_compare.py
+-rw-r--r--  2.0 unx     3173 b- defN 24-May-20 18:58 qai_hub_models/utils/testing.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/utils/aimet/__init__.py
+-rw-r--r--  2.0 unx      876 b- defN 24-May-20 18:58 qai_hub_models/utils/aimet/config_loader.py
+-rw-r--r--  2.0 unx     1233 b- defN 24-May-20 18:58 qai_hub_models/utils/aimet/default_config.json
+-rw-r--r--  2.0 unx      946 b- defN 24-May-20 18:58 qai_hub_models/utils/aimet/default_config_legacy_v1.json
+-rw-r--r--  2.0 unx      955 b- defN 24-May-20 18:58 qai_hub_models/utils/aimet/default_config_legacy_v2.json
+-rw-r--r--  2.0 unx     2725 b- defN 24-May-20 18:58 qai_hub_models/utils/aimet/default_config_llama.json
+-rw-r--r--  2.0 unx      919 b- defN 24-May-20 18:58 qai_hub_models/utils/aimet/default_config_per_channel_qnn.json
+-rw-r--r--  2.0 unx     1187 b- defN 24-May-20 18:58 qai_hub_models/utils/aimet/repo.py
+-rw-r--r--  2.0 unx      259 b- defN 24-May-20 18:58 qai_hub_models/utils/scorecard/__init__.py
+-rw-r--r--  2.0 unx     9556 b- defN 24-May-20 18:58 qai_hub_models/utils/scorecard/common.py
+-rw-r--r--  2.0 unx    12040 b- defN 24-May-20 18:58 qai_hub_models/utils/scorecard/job_summary.py
+-rw-r--r--  2.0 unx    14429 b- defN 24-May-20 18:58 qai_hub_models/utils/scorecard/model_card.py
+-rw-r--r--  2.0 unx    11415 b- defN 24-May-20 18:58 qai_hub_models/utils/scorecard/perf_summary.py
+-rw-r--r--  2.0 unx     1481 b- defN 24-May-20 19:00 qai_hub_models-0.6.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx    48427 b- defN 24-May-20 19:00 qai_hub_models-0.6.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-20 19:00 qai_hub_models-0.6.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       15 b- defN 24-May-20 19:00 qai_hub_models-0.6.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx   104761 b- defN 24-May-20 19:00 qai_hub_models-0.6.0.dist-info/RECORD
+1016 files, 3263227 bytes uncompressed, 1001566 bytes compressed:  69.3%
```

## zipnote {}

```diff
@@ -27,14 +27,17 @@
 
 Filename: qai_hub_models/datasets/coco.py
 Comment: 
 
 Filename: qai_hub_models/datasets/common.py
 Comment: 
 
+Filename: qai_hub_models/datasets/imagenet.py
+Comment: 
+
 Filename: qai_hub_models/datasets/imagenette.py
 Comment: 
 
 Filename: qai_hub_models/datasets/pascal_voc.py
 Comment: 
 
 Filename: qai_hub_models/evaluators/__init__.py
@@ -45,15 +48,15 @@
 
 Filename: qai_hub_models/evaluators/classification_evaluator.py
 Comment: 
 
 Filename: qai_hub_models/evaluators/detection_evaluator.py
 Comment: 
 
-Filename: qai_hub_models/evaluators/image_evaluator.py
+Filename: qai_hub_models/evaluators/segmentation_evaluator.py
 Comment: 
 
 Filename: qai_hub_models/evaluators/superres_evaluator.py
 Comment: 
 
 Filename: qai_hub_models/models/__init__.py
 Comment: 
@@ -84,26 +87,29 @@
 
 Filename: qai_hub_models/models/_shared/cityscapes_segmentation/model.py
 Comment: 
 
 Filename: qai_hub_models/models/_shared/cityscapes_segmentation/patches/move_datasets.diff
 Comment: 
 
+Filename: qai_hub_models/models/_shared/convnext_tiny_quantized/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/_shared/convnext_tiny_quantized/model.py
+Comment: 
+
 Filename: qai_hub_models/models/_shared/deeplab/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/_shared/deeplab/app.py
 Comment: 
 
 Filename: qai_hub_models/models/_shared/deeplab/demo.py
 Comment: 
 
-Filename: qai_hub_models/models/_shared/deeplab/evaluator.py
-Comment: 
-
 Filename: qai_hub_models/models/_shared/deeplab/model.py
 Comment: 
 
 Filename: qai_hub_models/models/_shared/detr/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/_shared/detr/app.py
@@ -327,14 +333,56 @@
 
 Filename: qai_hub_models/models/convnext_tiny/perf.yaml
 Comment: 
 
 Filename: qai_hub_models/models/convnext_tiny/test.py
 Comment: 
 
+Filename: qai_hub_models/models/convnext_tiny_w8a16_quantized/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a16_quantized/conftest.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a16_quantized/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a16_quantized/export.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a16_quantized/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a16_quantized/model.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a16_quantized/test.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a8_quantized/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a8_quantized/conftest.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a8_quantized/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a8_quantized/export.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a8_quantized/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a8_quantized/model.py
+Comment: 
+
+Filename: qai_hub_models/models/convnext_tiny_w8a8_quantized/test.py
+Comment: 
+
 Filename: qai_hub_models/models/ddrnet23_slim/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/ddrnet23_slim/app.py
 Comment: 
 
 Filename: qai_hub_models/models/ddrnet23_slim/conftest.py
@@ -717,14 +765,38 @@
 
 Filename: qai_hub_models/models/fcn_resnet50/perf.yaml
 Comment: 
 
 Filename: qai_hub_models/models/fcn_resnet50/test.py
 Comment: 
 
+Filename: qai_hub_models/models/fcn_resnet50_quantized/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/fcn_resnet50_quantized/conftest.py
+Comment: 
+
+Filename: qai_hub_models/models/fcn_resnet50_quantized/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/fcn_resnet50_quantized/export.py
+Comment: 
+
+Filename: qai_hub_models/models/fcn_resnet50_quantized/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/fcn_resnet50_quantized/model.py
+Comment: 
+
+Filename: qai_hub_models/models/fcn_resnet50_quantized/perf.yaml
+Comment: 
+
+Filename: qai_hub_models/models/fcn_resnet50_quantized/test.py
+Comment: 
+
 Filename: qai_hub_models/models/ffnet_122ns_lowres/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/ffnet_122ns_lowres/conftest.py
 Comment: 
 
 Filename: qai_hub_models/models/ffnet_122ns_lowres/demo.py
@@ -1266,14 +1338,38 @@
 
 Filename: qai_hub_models/models/mediapipe_selfie/test.py
 Comment: 
 
 Filename: qai_hub_models/models/mediapipe_selfie/utils.py
 Comment: 
 
+Filename: qai_hub_models/models/midas/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/midas/app.py
+Comment: 
+
+Filename: qai_hub_models/models/midas/conftest.py
+Comment: 
+
+Filename: qai_hub_models/models/midas/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/midas/export.py
+Comment: 
+
+Filename: qai_hub_models/models/midas/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/midas/model.py
+Comment: 
+
+Filename: qai_hub_models/models/midas/test.py
+Comment: 
+
 Filename: qai_hub_models/models/mnasnet05/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/mnasnet05/conftest.py
 Comment: 
 
 Filename: qai_hub_models/models/mnasnet05/demo.py
@@ -1470,14 +1566,38 @@
 
 Filename: qai_hub_models/models/openpose/requirements.txt
 Comment: 
 
 Filename: qai_hub_models/models/openpose/test.py
 Comment: 
 
+Filename: qai_hub_models/models/posenet_mobilenet/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/posenet_mobilenet/app.py
+Comment: 
+
+Filename: qai_hub_models/models/posenet_mobilenet/conftest.py
+Comment: 
+
+Filename: qai_hub_models/models/posenet_mobilenet/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/posenet_mobilenet/export.py
+Comment: 
+
+Filename: qai_hub_models/models/posenet_mobilenet/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/posenet_mobilenet/model.py
+Comment: 
+
+Filename: qai_hub_models/models/posenet_mobilenet/test.py
+Comment: 
+
 Filename: qai_hub_models/models/quicksrnetlarge/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/quicksrnetlarge/conftest.py
 Comment: 
 
 Filename: qai_hub_models/models/quicksrnetlarge/demo.py
@@ -1908,14 +2028,35 @@
 
 Filename: qai_hub_models/models/resnext50_quantized/perf.yaml
 Comment: 
 
 Filename: qai_hub_models/models/resnext50_quantized/test.py
 Comment: 
 
+Filename: qai_hub_models/models/riffusion_quantized/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/riffusion_quantized/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/riffusion_quantized/export.py
+Comment: 
+
+Filename: qai_hub_models/models/riffusion_quantized/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/riffusion_quantized/model.py
+Comment: 
+
+Filename: qai_hub_models/models/riffusion_quantized/requirements.txt
+Comment: 
+
+Filename: qai_hub_models/models/riffusion_quantized/test.py
+Comment: 
+
 Filename: qai_hub_models/models/sam/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/sam/app.py
 Comment: 
 
 Filename: qai_hub_models/models/sam/conftest.py
@@ -2112,39 +2253,57 @@
 
 Filename: qai_hub_models/models/squeezenet1_1_quantized/perf.yaml
 Comment: 
 
 Filename: qai_hub_models/models/squeezenet1_1_quantized/test.py
 Comment: 
 
-Filename: qai_hub_models/models/stable_diffusion_quantized/__init__.py
+Filename: qai_hub_models/models/stable_diffusion_v1_5_quantized/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/stable_diffusion_v1_5_quantized/demo.py
 Comment: 
 
-Filename: qai_hub_models/models/stable_diffusion_quantized/app.py
+Filename: qai_hub_models/models/stable_diffusion_v1_5_quantized/export.py
 Comment: 
 
-Filename: qai_hub_models/models/stable_diffusion_quantized/demo.py
+Filename: qai_hub_models/models/stable_diffusion_v1_5_quantized/info.yaml
 Comment: 
 
-Filename: qai_hub_models/models/stable_diffusion_quantized/export.py
+Filename: qai_hub_models/models/stable_diffusion_v1_5_quantized/model.py
 Comment: 
 
-Filename: qai_hub_models/models/stable_diffusion_quantized/info.yaml
+Filename: qai_hub_models/models/stable_diffusion_v1_5_quantized/perf.yaml
 Comment: 
 
-Filename: qai_hub_models/models/stable_diffusion_quantized/model.py
+Filename: qai_hub_models/models/stable_diffusion_v1_5_quantized/requirements.txt
 Comment: 
 
-Filename: qai_hub_models/models/stable_diffusion_quantized/perf.yaml
+Filename: qai_hub_models/models/stable_diffusion_v1_5_quantized/test.py
 Comment: 
 
-Filename: qai_hub_models/models/stable_diffusion_quantized/requirements.txt
+Filename: qai_hub_models/models/stable_diffusion_v2_1_quantized/__init__.py
 Comment: 
 
-Filename: qai_hub_models/models/stable_diffusion_quantized/test.py
+Filename: qai_hub_models/models/stable_diffusion_v2_1_quantized/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/stable_diffusion_v2_1_quantized/export.py
+Comment: 
+
+Filename: qai_hub_models/models/stable_diffusion_v2_1_quantized/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/stable_diffusion_v2_1_quantized/model.py
+Comment: 
+
+Filename: qai_hub_models/models/stable_diffusion_v2_1_quantized/requirements.txt
+Comment: 
+
+Filename: qai_hub_models/models/stable_diffusion_v2_1_quantized/test.py
 Comment: 
 
 Filename: qai_hub_models/models/stylegan2/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/stylegan2/app.py
 Comment: 
@@ -2499,14 +2658,71 @@
 
 Filename: qai_hub_models/models/xlsr_quantized/perf.yaml
 Comment: 
 
 Filename: qai_hub_models/models/xlsr_quantized/test.py
 Comment: 
 
+Filename: qai_hub_models/models/yolonas/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas/app.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas/conftest.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas/export.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/yolonas/model.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas/perf.yaml
+Comment: 
+
+Filename: qai_hub_models/models/yolonas/requirements.txt
+Comment: 
+
+Filename: qai_hub_models/models/yolonas/test.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas_quantized/__init__.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas_quantized/conftest.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas_quantized/demo.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas_quantized/export.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas_quantized/info.yaml
+Comment: 
+
+Filename: qai_hub_models/models/yolonas_quantized/model.py
+Comment: 
+
+Filename: qai_hub_models/models/yolonas_quantized/perf.yaml
+Comment: 
+
+Filename: qai_hub_models/models/yolonas_quantized/requirements.txt
+Comment: 
+
+Filename: qai_hub_models/models/yolonas_quantized/test.py
+Comment: 
+
 Filename: qai_hub_models/models/yolov6/__init__.py
 Comment: 
 
 Filename: qai_hub_models/models/yolov6/app.py
 Comment: 
 
 Filename: qai_hub_models/models/yolov6/conftest.py
@@ -2763,14 +2979,17 @@
 
 Filename: qai_hub_models/utils/quantization.py
 Comment: 
 
 Filename: qai_hub_models/utils/quantization_aimet.py
 Comment: 
 
+Filename: qai_hub_models/utils/system_info.py
+Comment: 
+
 Filename: qai_hub_models/utils/test_compare.py
 Comment: 
 
 Filename: qai_hub_models/utils/testing.py
 Comment: 
 
 Filename: qai_hub_models/utils/aimet/__init__.py
@@ -2784,14 +3003,17 @@
 
 Filename: qai_hub_models/utils/aimet/default_config_legacy_v1.json
 Comment: 
 
 Filename: qai_hub_models/utils/aimet/default_config_legacy_v2.json
 Comment: 
 
+Filename: qai_hub_models/utils/aimet/default_config_llama.json
+Comment: 
+
 Filename: qai_hub_models/utils/aimet/default_config_per_channel_qnn.json
 Comment: 
 
 Filename: qai_hub_models/utils/aimet/repo.py
 Comment: 
 
 Filename: qai_hub_models/utils/scorecard/__init__.py
@@ -2805,23 +3027,23 @@
 
 Filename: qai_hub_models/utils/scorecard/model_card.py
 Comment: 
 
 Filename: qai_hub_models/utils/scorecard/perf_summary.py
 Comment: 
 
-Filename: qai_hub_models-0.5.1.dist-info/LICENSE
+Filename: qai_hub_models-0.6.0.dist-info/LICENSE
 Comment: 
 
-Filename: qai_hub_models-0.5.1.dist-info/METADATA
+Filename: qai_hub_models-0.6.0.dist-info/METADATA
 Comment: 
 
-Filename: qai_hub_models-0.5.1.dist-info/WHEEL
+Filename: qai_hub_models-0.6.0.dist-info/WHEEL
 Comment: 
 
-Filename: qai_hub_models-0.5.1.dist-info/top_level.txt
+Filename: qai_hub_models-0.6.0.dist-info/top_level.txt
 Comment: 
 
-Filename: qai_hub_models-0.5.1.dist-info/RECORD
+Filename: qai_hub_models-0.6.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## qai_hub_models/_version.py

```diff
@@ -1,5 +1,5 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
-__version__ = "0.5.1"
+__version__ = "0.6.0"
```

## qai_hub_models/global_requirements.txt

```diff
@@ -1,45 +1,53 @@
 # If you:
 # - Install requirements.txt
 # - Run the aimet installation script
 # - Then install this requirements file
 # That should create an environment that works for every single model.
 
+Deprecated==1.2.11
 PySoundFile; sys_platform == 'win32'
 albumentations==0.5.2
 av==10.0.0
 basicsr==1.4.2
-click==8.0
+click==8.1.7
+data-gradients==0.3.1
 datasets==2.14.5
 diffusers[torch]==0.21.4
 easydict==1.10
+einops==0.3.2
 ffmpeg==1.4
 ftfy==6.1.1
 hydra-core==1.3.0
 imageio[ffmpeg]==2.31.5
+imagesize==1.4.1
 kornia==0.5.0
 librosa==0.10.1
 matplotlib==3.7.4
 mmcv==2.1.0
 mmdet==3.2.0
 mmpose==1.2.0
 object-detection-metrics==0.4.post1
 openai-whisper==20230314
 pycocotools==2.0.7
 pytorch-lightning==1.6.0
+rapidfuzz==3.8.1
 regex==2023.12.25
 scikit-image==0.21.0
 scikit-learn==1.1.3
 scipy==1.8.1
 seaborn==0.11.0
 sentencepiece==0.2.0
+shapely==2.0.3
 soundfile==0.12.1
+stringcase==1.2.0
 tflite==2.10.0
 thop==0.1.1.post2209072238
 timm==0.9.11
 tensorboard==2.13.0
 torchaudio==0.13.1
 transformers==4.27.4
+treelib==1.6.1
 tucker-conv==1.0.1
 ultralytics==8.0.193
 webdataset==0.2.86
 yacs==0.1.8
```

## qai_hub_models/requirements-dev.txt

```diff
@@ -1,10 +1,10 @@
 boto3==1.34.40
 botocore==1.34.40
-coverage==6.5.0
+coverage==5.3.1
 imageio[ffmpeg]==2.31.5
 jinja2==3.0.3
 mypy==0.991
 pre-commit==3.5.0
 pytest-cov==4.1.0
 pytest-xdist==3.3.1
 ruamel-yaml==0.18.6
```

## qai_hub_models/datasets/bsd300.py

```diff
@@ -28,47 +28,44 @@
 class BSD300Dataset(BaseDataset):
     """
     BSD300 published here: https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/
     """
 
     def __init__(self, scaling_factor=4):
         self.bsd_path = BSD300_ASSET.path(extracted=True)
-        self.images_path = os.path.join(self.bsd_path, "images/train")
+        self.images_path = self.bsd_path / "images" / "train"
         BaseDataset.__init__(self, self.bsd_path)
         self.scaling_factor = scaling_factor
 
     def _validate_data(self) -> bool:
-        images_path = os.path.join(self.dataset_path, "images/train")
-
         # Check image path exists
-        if not os.path.exists(images_path):
+        if not self.images_path.exists():
             return False
 
         # Ensure the correct number of images are there
-        files = os.listdir(images_path)
-        images = [f for f in files if ".jpg" in f]
+        images = [f for f in self.images_path.iterdir() if ".jpg" in f.name]
         if len(images) != DATASET_LENGTH:
             return False
 
         return True
 
     def _prepare_data(self):
         # Rename images to be more friendly to enumeration
-        directory = os.path.join(self.dataset_path, "images/train")
-        files = os.listdir(directory)
-        for i, filename in enumerate(files):
-            if filename.endswith(".jpg"):
+        # directory = os.path.join(self.dataset_path, "images/train")
+        # files = os.listdir(directory)
+        for i, filepath in enumerate(self.images_path.iterdir()):
+            if filepath.name.endswith(".jpg"):
                 # Open the image and convert it to png
                 try:
-                    with Image.open(os.path.join(directory, filename)) as img:
-                        img.save(os.path.join(directory, f"img_{i + 1:03d}_HR.jpg"))
+                    with Image.open(filepath) as img:
+                        img.save(self.images_path / f"img_{i + 1:03d}_HR.jpg")
                     # delete the old image
-                    os.remove(os.path.join(directory, filename))
+                    os.remove(filepath)
                 except ValueError:
-                    print(f"File {filename} does not exist!")
+                    print(f"File {filepath} does not exist!")
 
     def __len__(self):
         return DATASET_LENGTH
 
     def __getitem__(self, item) -> Tuple[torch.Tensor, torch.Tensor]:
         # We use the super resolution GT-and-test image preparation from AIMET zoo:
         # https://github.com/quic/aimet-model-zoo/blob/d09d2b0404d10f71a7640a87e9d5e5257b028802/aimet_zoo_torch/quicksrnet/dataloader/utils.py#L51
```

## qai_hub_models/datasets/common.py

```diff
@@ -3,35 +3,36 @@
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import os
 import shutil
 from abc import ABC, abstractmethod
+from pathlib import Path
 from typing import final
 
 from torch.utils.data import Dataset
 
 
 class BaseDataset(Dataset, ABC):
     """
     Base class to be extended by Datasets used in this repo for quantizing models.
     """
 
-    def __init__(self, dataset_path: str):
-        self.dataset_path = dataset_path
+    def __init__(self, dataset_path: str | Path):
+        self.dataset_path = Path(dataset_path)
         self.download_data()
 
     @final
     def download_data(self) -> None:
         if self._validate_data():
             return
-        if os.path.exists(self.dataset_path):
+        if self.dataset_path.exists():
             # Data is corrupted, delete and re-download
-            if os.path.isdir(self.dataset_path):
+            if self.dataset_path.is_dir():
                 shutil.rmtree(self.dataset_path)
             else:
                 os.remove(self.dataset_path)
 
         print("Downloading data")
         self._download_data()
         print("Done downloading")
@@ -45,8 +46,8 @@
         """
         pass
 
     def _validate_data(self) -> bool:
         """
         Validates data downloaded on disk. By default just checks that folder exists.
         """
-        return os.path.exists(self.dataset_path)
+        return self.dataset_path.exists()
```

## qai_hub_models/datasets/imagenette.py

```diff
@@ -5,22 +5,24 @@
 import os
 import stat
 
 from torchvision.datasets import ImageNet
 
 from qai_hub_models.datasets.common import BaseDataset
 from qai_hub_models.utils.asset_loaders import CachedWebDatasetAsset
+from qai_hub_models.utils.image_processing import IMAGENET_TRANSFORM
 
 IMAGENETTE_FOLDER_NAME = "imagenette2-320"
 IMAGENETTE_VERSION = 1
+DEVKIT_NAME = "ILSVRC2012_devkit_t12.tar.gz"
 DEVKIT_ASSET = CachedWebDatasetAsset(
-    "https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz",
+    f"https://image-net.org/data/ILSVRC/2012/{DEVKIT_NAME}",
     IMAGENETTE_FOLDER_NAME,
     IMAGENETTE_VERSION,
-    "ILSVRC2012_devkit_t12.tar.gz",
+    DEVKIT_NAME,
 )
 IMAGENETTE_ASSET = CachedWebDatasetAsset(
     "https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz",
     IMAGENETTE_FOLDER_NAME,
     IMAGENETTE_VERSION,
     "imagenette2-320.tgz",
 )
@@ -47,19 +49,14 @@
         https://github.com/fastai/imagenette
 
     Contains ~4k images spanning 10 of the imagenet classes.
     """
 
     def __init__(self):
         BaseDataset.__init__(self, str(IMAGENETTE_ASSET.path(extracted=True)))
-        # Avoid circular import
-        from qai_hub_models.models._shared.imagenet_classifier.app import (
-            IMAGENET_TRANSFORM,
-        )
-
         ImageNet.__init__(
             self,
             root=IMAGENETTE_ASSET.path(),
             split="val",
             transform=IMAGENET_TRANSFORM,
             target_transform=lambda val: IMAGENETTE_CLASS_MAP[val],
         )
@@ -73,30 +70,30 @@
 
         # Check devkit permissions
         devkit_permissions = os.stat(devkit_path).st_mode
         if devkit_permissions & stat.S_IEXEC != stat.S_IEXEC:
             return False
 
         # Check val data exists
-        val_data_path = os.path.join(self.dataset_path, "val")
-        if not os.path.exists(val_data_path):
+        val_data_path = self.dataset_path / "val"
+        if not val_data_path.exists():
             return False
 
         # Ensure 10 classes
-        subdirs = os.listdir(val_data_path)
+        subdirs = list(val_data_path.iterdir())
         if len(subdirs) != 10:
             return False
 
         # Ensure >= 300 samples per classes
         for subdir in subdirs:
-            if len(os.listdir(os.path.join(val_data_path, subdir))) < 300:
+            if len(list(subdir.iterdir())) < 300:
                 return False
         return True
 
     def _download_data(self) -> None:
         IMAGENETTE_ASSET.fetch(extract=True)
         devkit_path = DEVKIT_ASSET.fetch()
         devkit_st = os.stat(devkit_path)
         os.chmod(devkit_path, devkit_st.st_mode | stat.S_IEXEC)
-        target_path = IMAGENETTE_ASSET.path() / os.path.basename(DEVKIT_ASSET.path())
-        if not os.path.exists(target_path):
+        target_path = IMAGENETTE_ASSET.path() / DEVKIT_NAME
+        if not target_path.exists():
             os.symlink(DEVKIT_ASSET.path(), target_path)
```

## qai_hub_models/datasets/pascal_voc.py

```diff
@@ -1,13 +1,12 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 
-from pathlib import Path
 from typing import Tuple
 
 import numpy as np
 import torch
 from PIL import Image
 from torchvision import transforms
 
@@ -32,15 +31,15 @@
     """
 
     def __init__(self, split: str = "train", image_size: Tuple[int, int] = (224, 224)):
         BaseDataset.__init__(self, str(VOC_ASSET.path().parent / DEVKIT_FOLDER_NAME))
         assert split in ["train", "val", "trainval"]
         self.split = split
 
-        base_path = Path(self.dataset_path) / "VOC2012"
+        base_path = self.dataset_path / "VOC2012"
         image_dir = base_path / "JPEGImages"
         category_dir = base_path / "SegmentationClass"
         splits_dir = base_path / "ImageSets" / "Segmentation"
 
         self.im_ids = []
         self.images = []
         self.categories = []
```

## qai_hub_models/models/protocols.py

```diff
@@ -16,17 +16,17 @@
 
 These are type checked at compile time.
 """
 from __future__ import annotations
 
 from abc import abstractmethod
 from pathlib import Path
-from typing import Any, Protocol, Type, TypeVar, runtime_checkable
+from typing import Any, List, Optional, Protocol, Type, TypeVar, runtime_checkable
 
-from qai_hub.client import DatasetEntries, SourceModel
+from qai_hub.client import DatasetEntries, Device, SourceModel
 
 from qai_hub_models.evaluators.base_evaluators import BaseEvaluator, _DataLoader
 from qai_hub_models.models.common import (
     SampleInputsType,
     SourceModelFormat,
     TargetRuntime,
 )
@@ -196,21 +196,24 @@
 
     def convert_to_hub_source_model(
         self,
         target_runtime: TargetRuntime,
         output_path: str | Path,
         input_spec: InputSpec | None = None,
         check_trace: bool = True,
+        external_onnx_weights: bool = False,
+        output_names: Optional[List[str]] = None,
     ) -> SourceModel:
         ...
 
     def get_hub_compile_options(
         self,
         target_runtime: TargetRuntime,
         other_compile_options: str = "",
+        device: Optional[Device] = None,
     ) -> str:
         """
         AI Hub compile options recommended for the model.
         """
         ...
 
     def preferred_hub_source_model_format(
```

## qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py

```diff
@@ -1,22 +1,18 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 import torch.nn.functional as F
 from torch import Tensor
 
-from qai_hub_models.evaluators.image_evaluator import SegmentationOutputEvaluator
+from qai_hub_models.evaluators.segmentation_evaluator import SegmentationOutputEvaluator
 
 
 class CityscapesSegmentationEvaluator(SegmentationOutputEvaluator):
     """
     Evaluates the output of Cityscapes semantics segmentation.
     """
 
     def add_batch(self, output: Tensor, gt: Tensor):
         output_match_size = F.interpolate(output, gt.shape[1:3], mode="bilinear")
-        output_class = output_match_size.argmax(1).cpu()
-        return super().add_batch(output_class, gt)
-
-    def get_accuracy_score(self) -> float:
-        return super().Mean_Intersection_over_Union()
+        return super().add_batch(output_match_size, gt)
```

## qai_hub_models/models/_shared/deeplab/model.py

```diff
@@ -1,15 +1,15 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 import torch
 
 from qai_hub_models.evaluators.base_evaluators import BaseEvaluator
-from qai_hub_models.models._shared.deeplab.evaluator import DeepLabV3Evaluator
+from qai_hub_models.evaluators.segmentation_evaluator import SegmentationOutputEvaluator
 from qai_hub_models.utils.base_model import BaseModel
 from qai_hub_models.utils.image_processing import normalize_image_torchvision
 from qai_hub_models.utils.input_spec import InputSpec
 
 NUM_CLASSES = 21
 
 
@@ -20,15 +20,15 @@
         normalize_input: bool = True,
     ) -> None:
         super().__init__()
         self.model = deeplabv3_model
         self.normalize_input = normalize_input
 
     def get_evaluator(self) -> BaseEvaluator:
-        return DeepLabV3Evaluator(NUM_CLASSES)
+        return SegmentationOutputEvaluator(NUM_CLASSES)
 
     def forward(self, image):
         """
         Run DeepLabV3_Plus_Mobilenet on `image`, and produce a tensor of classes for segmentation
 
         Parameters:
             image: Pixel values pre-processed for model consumption.
```

## qai_hub_models/models/_shared/fastsam/demo.py

```diff
@@ -1,27 +1,30 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import os
-import tempfile
 from typing import Type
 
 from PIL import Image
 
 from qai_hub_models.models._shared.fastsam.app import FastSAMApp
 from qai_hub_models.utils.args import (
     demo_model_from_cli_args,
     get_model_cli_parser,
     get_on_device_demo_parser,
     validate_on_device_demo_args,
 )
-from qai_hub_models.utils.asset_loaders import CachedWebAsset, load_image
+from qai_hub_models.utils.asset_loaders import (
+    CachedWebAsset,
+    load_image,
+    qaihm_temp_dir,
+)
 from qai_hub_models.utils.base_model import BaseModel
 from qai_hub_models.utils.display import display_or_save_image
 
 
 def fastsam_demo(
     model_type: Type[BaseModel],
     model_id: str,
@@ -42,15 +45,15 @@
     validate_on_device_demo_args(args, model_id)
 
     model = demo_model_from_cli_args(model_type, model_id, args)
     app = FastSAMApp(model)
 
     image = load_image(args.image)
 
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         image_path = os.path.join(tmpdir, "inp_image.jpg")
         image.save(image_path)
         pred, prompt_process = app.segment_image(image_path)
 
         # Store the output image
         output_path = os.path.join(args.output_dir or tmpdir, "output.jpg")
```

## qai_hub_models/models/_shared/imagenet_classifier/app.py

```diff
@@ -2,26 +2,19 @@
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import torch
 from PIL.Image import Image
-from torchvision import transforms
 
-from qai_hub_models.models._shared.imagenet_classifier.model import IMAGENET_DIM
 from qai_hub_models.models.protocols import ExecutableModelProtocol
-from qai_hub_models.utils.image_processing import normalize_image_transform
-
-IMAGENET_TRANSFORM = transforms.Compose(
-    [
-        transforms.Resize(256),
-        transforms.CenterCrop(IMAGENET_DIM),
-        transforms.ToTensor(),
-    ]
+from qai_hub_models.utils.image_processing import (
+    IMAGENET_TRANSFORM,
+    normalize_image_transform,
 )
 
 
 def preprocess_image(image: Image, normalize: bool = False) -> torch.Tensor:
     """
     Preprocesses images to be run through torch imagenet classifiers
     as prescribed here:
```

## qai_hub_models/models/_shared/imagenet_classifier/model.py

```diff
@@ -8,21 +8,23 @@
 
 import numpy as np
 import torch
 
 from qai_hub_models.evaluators.base_evaluators import BaseEvaluator
 from qai_hub_models.evaluators.classification_evaluator import ClassificationEvaluator
 from qai_hub_models.utils.base_model import BaseModel
-from qai_hub_models.utils.image_processing import normalize_image_torchvision
+from qai_hub_models.utils.image_processing import (
+    IMAGENET_DIM,
+    normalize_image_torchvision,
+)
 from qai_hub_models.utils.input_spec import InputSpec
 from qai_hub_models.utils.quantization import get_image_quantization_samples
 
 MODEL_ASSET_VERSION = 1
 MODEL_ID = __name__.split(".")[-2]
-IMAGENET_DIM = 224
 
 
 class ImagenetClassifier(BaseModel):
     """
     Base class for all Imagenet Classifier models within QAI Hub Models.
     """
```

## qai_hub_models/models/_shared/mediapipe/app.py

```diff
@@ -562,15 +562,15 @@
         Returns
             Nothing; drawing is done on input frame.
         """
         for roi, box, kp in zip(roi_4corners, selected_boxes, selected_keypoints):
             # Draw detector bounding box
             draw_box_from_xyxy(NHWC_int_numpy_frame, box[0], box[1], (255, 0, 0), 1)
             # Draw detector keypoints
-            draw_points(NHWC_int_numpy_frame, kp)
+            draw_points(NHWC_int_numpy_frame, kp, size=30)
             # Draw region of interest box computed from the detector box & keypoints
             # (this is the input to the landmark detector)
             draw_box_from_corners(NHWC_int_numpy_frame, roi, (0, 255, 0))
 
     def _draw_landmarks(
         self,
         NHWC_int_numpy_frame: np.ndarray,
```

## qai_hub_models/models/_shared/video_classifier/demo.py

```diff
@@ -1,20 +1,19 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
-import tempfile
 from typing import Type
 
 from qai_hub_models.models._shared.video_classifier.app import KineticsClassifierApp
 from qai_hub_models.models._shared.video_classifier.model import KineticsClassifier
 from qai_hub_models.utils.args import get_model_cli_parser, model_from_cli_args
-from qai_hub_models.utils.asset_loaders import CachedWebAsset, load_path
+from qai_hub_models.utils.asset_loaders import CachedWebAsset, load_path, qaihm_temp_dir
 
 
 #
 # Run KineticsClassifierApp end-to-end on a sample video.
 # The demo will display top classification predictions for the video.
 #
 def kinetics_classifier_demo(
@@ -31,13 +30,13 @@
 
     args = parser.parse_args([] if is_test else None)
 
     # Load image & model
     model = model_from_cli_args(model_type, args)
     app = KineticsClassifierApp(model)
     print("Model Loaded")
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         dst_path = load_path(args.video, tmpdir)
         predictions = app.predict(path=str(dst_path))
     top5_classes = ", ".join(predictions)
     if not is_test:
         print(f"Top 5 predictions: {top5_classes}")
```

## qai_hub_models/models/_shared/whisper/app.py

```diff
@@ -32,15 +32,15 @@
 
     def __init__(self, whisper: Whisper):
         decoder = whisper.decoder.to("cpu")
         encoder = whisper.encoder.to("cpu")
         self.num_decoder_blocks = whisper.num_decoder_blocks
         self.num_decoder_heads = whisper.num_decoder_heads
         self.attention_dim = whisper.attention_dim
-        self.max_decode_len = whisper.max_decode_len
+        self.mean_decode_len = whisper.mean_decode_len
 
         # Wraps torch Module so it takes np ndarray as input and outputs
         if isinstance(encoder, torch.nn.Module):
             self.encoder = TorchNumpyAdapter(encoder)
         else:
             self.encoder = encoder
         if isinstance(decoder, torch.nn.Module):
@@ -60,39 +60,51 @@
 
         - mel_input: of shape (1, 80, 3000). Mel spectrogram of 30s audio.
 
         Returns:
 
         - transcribed texts
         """
-        cross_attn_cache = self.encoder(mel_input)
+        k_cache_cross, v_cache_cross = self.encoder(mel_input)
         # Start decoding
         # coreml only takes float tensors
         x = np.array([[TOKEN_SOT]])
         decoded_tokens = [TOKEN_SOT]
-        sample_len = self.max_decode_len  # max # of tokens to sample
-        cache_tensor = np.zeros((1, sample_len, self.attention_dim)).astype(np.float32)
-        self_attn_cache = [cache_tensor] * 2 * self.num_decoder_blocks
+        sample_len = self.mean_decode_len  # mean # of tokens to sample
+        k_cache_self = np.zeros(
+            (
+                self.num_decoder_blocks,
+                self.num_decoder_heads,
+                self.attention_dim // self.num_decoder_heads,
+                sample_len,
+            )
+        ).astype(np.float32)
+        v_cache_self = np.zeros(
+            (
+                self.num_decoder_blocks,
+                self.num_decoder_heads,
+                sample_len,
+                self.attention_dim // self.num_decoder_heads,
+            )
+        ).astype(np.float32)
 
         sum_logprobs = 0
         for i in range(sample_len):
             # Using i to index inside the decoder model hurts the
             # the model performance.
             # index - used to get positional embedding correctly.
             index = torch.zeros([1, 1], dtype=torch.int32)
             index[0, 0] = i
-            # Use mask to get the k_cache updated with new key
-            mask = torch.zeros(1, sample_len, self.attention_dim, dtype=torch.bool)
-            mask[:, i, :] = 1
             decoder_out = self.decoder(
-                x, index, mask, *cross_attn_cache, *self_attn_cache
+                x, index, k_cache_cross, v_cache_cross, k_cache_self, v_cache_self
             )
             # logit has shape (1, decoded_len, 51864)
             logits = decoder_out[0]
-            self_attn_cache = decoder_out[1:]  # type: ignore
+            k_cache_self = decoder_out[1]
+            v_cache_self = decoder_out[2]
 
             # logit has shape (51864,)
             logits = logits[0, -1]  # consider only the last token
 
             # Filters
             # SuppressBlank
             if i == 0:
```

## qai_hub_models/models/_shared/whisper/model.py

```diff
@@ -9,15 +9,22 @@
 import torch
 import whisper  # type: ignore
 
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 from qai_hub_models.utils.base_model import BaseModel, CollectionModel, TargetRuntime
 from qai_hub_models.utils.input_spec import InputSpec
 
-MAX_DECODE_LEN = 224
+# The official default max decoded length is 448. We use mean decoded length 224 for benchmarking purpose
+MEAN_DECODE_LEN = 224
+
+# The number of 20ms audio contexts in 30 seconds of audio
+AUDIO_EMB_LEN = 1500
+
+# The number of Mel features per audio context
+N_MELS = 80
 
 MODEL_ID = "whisper_asr_shared"
 MODEL_ASSET_VERSION = 1
 MEL_FILTER_PATH = CachedWebModelAsset.from_asset_store(
     MODEL_ID, MODEL_ASSET_VERSION, "openai_assets/mel_filters.npz"
 )
 
@@ -32,15 +39,15 @@
         num_heads: int,
     ):
         self.encoder = encoder
         self.decoder = decoder
         self.num_decoder_blocks = num_decoder_blocks
         self.attention_dim = attention_dim
         self.num_decoder_heads = num_heads
-        self.max_decode_len = MAX_DECODE_LEN
+        self.mean_decode_len = MEAN_DECODE_LEN
 
     @classmethod
     def from_pretrained(cls, model: str = "tiny.en"):
         # For other model sizes, see https://github.com/openai/whisper/blob/main/whisper/__init__.py#L17
         return cls.from_source_model(whisper.load_model(model))
 
     @classmethod
@@ -59,197 +66,302 @@
 
     It takes audio input (mel) and directly produce cross attention
     kv-cache.
     """
 
     def __init__(self, model: whisper.model.Whisper):
         super().__init__()
-        self.model = model
+        self.encoder = model.encoder
+        dims = model.dims
+
+        states_per_head = dims.n_audio_state // dims.n_audio_head
+        scale = states_per_head**-0.25
+
+        self.cross_attn_key_list = torch.nn.ModuleList(
+            [
+                SplitLinear(block.cross_attn.key, dims.n_audio_head, scale)
+                for block in model.decoder.blocks
+            ]
+        )
+        self.cross_attn_value_list = torch.nn.ModuleList(
+            [
+                SplitLinear(block.cross_attn.value, dims.n_audio_head)
+                for block in model.decoder.blocks
+            ]
+        )
 
     def forward(self, audio: torch.Tensor) -> List[torch.Tensor]:
-        # Return 2 * self.num_blocks tensors (k, v for each block)
-        encoder_out = self.model.encoder(audio)
-        res = []
-        for residual_block in self.model.decoder.blocks:
-            res.append(residual_block.cross_attn.key(encoder_out))
-            res.append(residual_block.cross_attn.value(encoder_out))
-        return res
+        # Return cross attention key and value cache tensors
+        encoder_out = self.encoder(audio)
+        k_cache = torch.cat(
+            [
+                key(encoder_out, transpose=True).unsqueeze(0)
+                for key in self.cross_attn_key_list
+            ],
+            dim=0,
+        )
+        v_cache = torch.cat(
+            [value(encoder_out).unsqueeze(0) for value in self.cross_attn_value_list],
+            dim=0,
+        )
+        return k_cache, v_cache
 
     @staticmethod
     def get_input_spec() -> InputSpec:
         """
         Returns the input specification (name -> (shape, type). This can be
         used to submit profiling job on Qualcomm AI Hub.
         """
-        return dict(audio=((1, 80, 3000), "float32"))
+        return dict(audio=((1, N_MELS, AUDIO_EMB_LEN * 2), "float32"))
 
     @classmethod
     def from_pretrained(cls):
         return Whisper.from_pretrained().encoder
 
     def get_hub_profile_options(
         self, target_runtime: TargetRuntime, other_profile_options: str = ""
     ) -> str:
         profile_options = super().get_hub_profile_options(
             target_runtime, other_profile_options
         )
-        return profile_options + " --max_profiler_iterations 10" + " --compute_unit gpu"
+        if (
+            target_runtime == TargetRuntime.TFLITE
+            and "--compute_unit" not in profile_options
+        ):
+            profile_options = profile_options + " --compute_unit gpu"
+        return profile_options + " --max_profiler_iterations 10"
 
 
 class WhisperDecoderInf(BaseModel):
     """
     whisper.model.TextDecoder optimized for export and inference:
 
     Wraps `whisper.model.TextDecoder` to facilitate export:
 
     1. kv cache inputs are individual tensors instead of a list of tensors
     2. kv cache inputs are required, not optional
     """
 
-    def __init__(self, model: whisper.model.TextDecoder):
+    def __init__(
+        self, model: whisper.model.TextDecoder, max_decode_len: int = MEAN_DECODE_LEN
+    ):
         super().__init__()
         assert isinstance(model, whisper.model.TextDecoder)
 
+        self.max_decode_len = max_decode_len
+
         # Wraps `ResidualAttentionBlock` in
         # `ResidualAttentionBlockWrapper`
         self.blocks = torch.nn.ModuleList(
             [ResidualAttentionBlockWrapper(b) for b in model.blocks]
         )
-        self.num_blocks = len(self.blocks)
+
         for m in ["token_embedding", "ln"]:
             self.add_module(m, getattr(model, m))
-        for p in ["positional_embedding"]:
-            self.register_parameter(p, getattr(model, p))
+
+        # Replace `whisper.model.TextDecoder.positional_embedding` (nn.Parameter) with nn.Embedding for easier lookup
+        self.positional_embedding = torch.nn.Embedding(
+            max_decode_len, self.token_embedding.weight.shape[1]
+        )
+        self.positional_embedding.weight = torch.nn.Parameter(
+            model.positional_embedding[:max_decode_len, :]
+        )
+
+        self.logits = torch.nn.Linear(
+            self.token_embedding.weight.shape[1],
+            self.token_embedding.weight.shape[0],
+            bias=False,
+        )
+        self.logits.weight = self.token_embedding.weight
+
+        # Since kv cache is a fixed size, mask out elements
+        # that correspond to not yet used entries.
+        # The kv cache for current token is inserted at the last
+        # index, with the previous cache shifted down by one element.
+        self.mask = torch.nn.Embedding(max_decode_len, max_decode_len)
+        mask = torch.zeros([max_decode_len, max_decode_len], dtype=torch.float32)
+        for c_idx in range(0, max_decode_len):
+            mask[c_idx, 0 : max_decode_len - c_idx - 1] = -100
+        self.mask.weight = torch.nn.Parameter(mask)
 
     @property
     def attention_dim(self):
         return self.blocks[0].attn_ln.weight.shape[0]
 
     @property
     def num_heads(self):
         return self.blocks[0].attn.n_head
 
+    @property
+    def num_blocks(self):
+        return len(self.blocks)
+
     def forward(
         self,
         x: torch.Tensor,
         index: torch.Tensor,
-        mask: torch.Tensor,
-        *kv_cache_args,
-        **kv_cache_kwargs,
+        k_cache_cross: torch.Tensor,
+        v_cache_cross: torch.Tensor,
+        k_cache_self: torch.Tensor,
+        v_cache_self: torch.Tensor,
     ):
         """
         Args:
 
         - x: torch.LongTensor, shape = (batch_size, <= n_ctx)
             the text tokens
 
         - index: torch.tensor, shape = (1, 1)
             index to get the positional encoding for x.
 
-        - mask: torch.tensor, shape = (1, max_sample_length, attn_dim)
-            Mask helps create kv_cache while keeping the size consistent.
-
-        - kv_cache_args: Tuple of length 4 * num_decoder_blocks. Elements are:
-
-            b{i}_cross_attn_k: [1, 1500, attn_dim]
-            b{i}_cross_attn_v: [1, 1500, attn_dim]
-
-            for i = 0, ..., num_blocks
-
-            followed by
+        - k_cache_cross: key cache for cross attention:
+          [num_blocks, num_heads, attn_dim/num_heads, AUDIO_EMB_LEN]
 
-            b{i}_self_attn_k: [1, max_sample_length, attn_dim]
-            b{i}_self_attn_v: [1, max_sample_length, attn_dim]
+        - v_cache_cross: value cache for cross attention:
+          [num_blocks, num_heads, AUDIO_EMB_LEN, attn_dim/num_heads]
 
-            for i = 0, ..., num_blocks
+        - k_cache_self: key cache for self attention:
+          [num_blocks, num_heads, attn_dim/num_heads, self.max_decode_len]
+          pass zeros for first call (index 0), otherwise pass in
+          previous decoder output
+
+        - v_cache_self: value cache for self attention:
+          [num_blocks, num_heads, self.max_decode_len, attn_dim/num_heads]
+          pass zeros for first call (index 0), otherwise pass in
+          previous decoder output
 
         Returns:
 
         - logits: of shape [1, 1, 51864]
-        - b0_self_attn_k, b0_self_attn_v, b1_self_attn_k, ...: Updated self attn cache.
-          2*num_decoder_blocks
+        - k_cache_self_new: updated key cache for self attention
+        - v_cache_self_new: updated value cache for self attention
         """
 
-        if not kv_cache_args:
-            kv_cache_args = list(kv_cache_kwargs.values())
-
         assert isinstance(self.token_embedding, torch.nn.Module)  # for mypy
         assert isinstance(self.ln, torch.nn.Module)  # for mypy
-        assert isinstance(self.positional_embedding, torch.nn.Parameter)  # for mypy
+        assert isinstance(self.positional_embedding, torch.nn.Embedding)  # for mypy
         # Set up kv_cache
         kv_cache = {}  # torch.nn.Module -> torch.Tensor
         for i, block in enumerate(self.blocks):
             kv_cache.update(
                 {
-                    block.attn.key: kv_cache_args[2 * self.num_blocks + i * 2],
-                    block.attn.value: kv_cache_args[2 * self.num_blocks + i * 2 + 1],
-                    block.cross_attn.key: kv_cache_args[i * 2],
-                    block.cross_attn.value: kv_cache_args[i * 2 + 1],
+                    block.attn.key: k_cache_self[i : i + 1],
+                    block.attn.value: v_cache_self[i : i + 1],
+                    block.cross_attn.key: k_cache_cross[i : i + 1],
+                    block.cross_attn.value: v_cache_cross[i : i + 1],
                 }
             )
 
-        x = self.token_embedding(x) + self.positional_embedding[index.long()]
+        x = self.token_embedding(x) + self.positional_embedding(index)
+        mask = self.mask(index)
 
         # x shape: (1, 1, 384)
-        kv_cache_new = []
-        for block in self.blocks:
-            x, k_cache, v_cache = block(x, index, mask, kv_cache=kv_cache)
-            kv_cache_new.append(k_cache.float())
-            kv_cache_new.append(v_cache.float())
+        k_cache_new = []
+        v_cache_new = []
+        for block_idx in range(self.num_blocks):
+            x, k_cache, v_cache = self.blocks[block_idx](x, mask, kv_cache=kv_cache)
+            k_cache_new.append(k_cache.float())
+            v_cache_new.append(v_cache.float())
 
         x = self.ln(x)
         logits = (
             x
             @ torch.transpose(
                 self.token_embedding.weight.to(x.dtype), 0, 1  # type: ignore
             )
         ).float()
+        logits = self.logits(x).float()
 
-        # shape: [1, 1, 51864]
-        return (logits,) + tuple(kv_cache_new)
+        return logits, torch.cat(k_cache_new), torch.cat(v_cache_new)
 
     @staticmethod
     def get_input_spec(
         num_blocks: int, attention_dim: int, num_heads: int
     ) -> InputSpec:
         """
         Returns the input specification (name -> (shape, type). This can be
         used to submit profiling job on Qualcomm AI Hub.
         """
         specs = dict(
             x=((1, 1), "int32"),
             index=((1, 1), "int32"),
-            mask=((1, MAX_DECODE_LEN, attention_dim), "int32"),
-        )
-        for i in range(num_blocks):
-            specs[f"b{i}_cross_attn_k"] = ((1, 1500, attention_dim), "float32")
-            specs[f"b{i}_cross_attn_v"] = ((1, 1500, attention_dim), "float32")
-
-        for i in range(num_blocks):
-            specs[f"b{i}_self_attn_k"] = (
-                (1, MAX_DECODE_LEN, attention_dim),
+            k_cache_cross=(
+                (num_blocks, num_heads, attention_dim // num_heads, AUDIO_EMB_LEN),
                 "float32",
-            )
-            specs[f"b{i}_self_attn_v"] = (
-                (1, MAX_DECODE_LEN, attention_dim),
+            ),
+            v_cache_cross=(
+                (num_blocks, num_heads, AUDIO_EMB_LEN, attention_dim // num_heads),
                 "float32",
-            )
+            ),
+            k_cache_self=(
+                (num_blocks, num_heads, attention_dim // num_heads, MEAN_DECODE_LEN),
+                "float32",
+            ),
+            v_cache_self=(
+                (num_blocks, num_heads, MEAN_DECODE_LEN, attention_dim // num_heads),
+                "float32",
+            ),
+        )
 
         return specs
 
     def _get_input_spec_for_instance(self) -> InputSpec:
         return self.__class__.get_input_spec(
             len(self.blocks), self.attention_dim, self.num_heads
         )
 
     @classmethod
     def from_pretrained(cls):
         return Whisper.from_pretrained().decoder
 
 
+class SplitLinear(torch.nn.Module):
+    def __init__(self, linear: torch.nn.Module, num_splits: int, scale: float = 1.0):
+        """
+        Split Linear operation into multiple instances
+        Multi-head cross attention
+        Uses pre-computed cross kv cache passed as input to the
+        decoder model
+        """
+        super().__init__()
+        weight = linear.weight
+        has_bias = False if linear.bias is None else True
+        if has_bias:
+            bias = linear.bias.reshape(num_splits, -1) * scale
+        split_weight = weight.reshape(num_splits, -1, weight.shape[1]) * scale
+        self.split_linears = torch.nn.ModuleList(
+            [
+                torch.nn.Linear(
+                    split_weight.shape[1], split_weight.shape[2], bias=has_bias
+                )
+                for split_idx in range(num_splits)
+            ]
+        )
+        for split_idx in range(num_splits):
+            self.split_linears[split_idx].weight = torch.nn.Parameter(
+                split_weight[split_idx, :, :]
+            )
+            if has_bias:
+                self.split_linears[split_idx].bias = torch.nn.Parameter(bias[split_idx])
+
+    def forward(self, x: torch.Tensor, transpose: bool = False):
+        """
+        produces output with dimension
+        [num_splits, input rows, output_features / num_splits]
+        If transpose is True, will transpose last two indices
+        """
+        if transpose:
+            x = torch.cat(
+                [spl(x).transpose(-1, -2) for spl in self.split_linears], dim=-3
+            )
+        else:
+            x = torch.cat([spl(x) for spl in self.split_linears], dim=-3)
+        return x
+
+
 class MHAWrapper(torch.nn.Module):
     """
     Wrapper around whisper.model.MultiHeadAttention to leverage kv cache for
     efficient inference. The original whisper.model.MultiHeadAttention doesn't
     returns the updated kv cache but relies on pytorch hook which
     cannot be exported for on-device inference. This wrapper fixes that.
 
@@ -271,48 +383,53 @@
         self.n_head = model.n_head
         for m in ["query", "key", "value", "out"]:
             self.add_module(m, getattr(model, m))
 
     def forward(
         self,
         x: torch.Tensor,
-        index: torch.Tensor,
         mask: torch.Tensor,
         kv_cache: Dict[torch.nn.Module, torch.Tensor],
     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
         """
         Args:
 
         - x: shape [1, 1, attention_dim]. Input feature.
 
         - kv_cache: 4 * num_decoder_blocks entries representing self attention
-          and cross attention from all attention blocks. Each entry of shape
-          [1, decoded_len, attention_dim]. We'd only use cache relevant to this
-          particular attention layer and ignore other entries in the dict.
+          and cross attention from all attention blocks. Each k entry of shape
+          [1, num_heads, attention_dim // num_heads, context_len] and
+          each v entry of shape
+          [1, num_heads, context_len, attention_dim // num_heads].
+          We'd only use cache relevant to this particular attention layer
+          and ignore other entries in the dict.
 
         Returns:
 
         - x_out: attention output
 
-        - updated k, v cache: of shape [1, decoded_len+1, attention_dim]
+        - updated k, v cache: with same shape as input
         """
         assert isinstance(self.query, torch.nn.Module)  # for mypy
         assert isinstance(self.key, torch.nn.Module)  # for mypy
         assert isinstance(self.value, torch.nn.Module)  # for mypy
         assert isinstance(self.out, torch.nn.Module)  # for mypy
         q = self.query(x)
+        q = q.view(q.shape[0], self.n_head, 1, -1)
         if self.attn_type == "self_attention":
             k_cache = kv_cache[self.key]
             v_cache = kv_cache[self.value]
-            k = torch.zeros(k_cache.shape)
-            v = torch.zeros(v_cache.shape)
-            k = mask * self.key(x) + k_cache
-            v = mask * self.value(x) + v_cache
-            new_index = torch.tensor([index[0, 0] + 1]).long()
-            wv = qkv_attention(q, k[:, :new_index], v[:, :new_index], self.n_head)
+            k = self.key(x).unsqueeze(3)
+            k = k.view(k.shape[0], self.n_head, -1, 1)
+            v = self.value(x).unsqueeze(2)
+            v = v.view(k.shape[0], self.n_head, 1, -1)
+            # shift kv cache and insert new k and v entries
+            k = torch.cat((k_cache[:, :, :, 1:], k), dim=-1)
+            v = torch.cat((v_cache[:, :, 1:, :], v), dim=-2)
+            wv = qkv_attention(q, k, v, self.n_head, mask=mask)
         else:  # cross_attention
             k, v = kv_cache[self.key], kv_cache[self.value]
             wv = qkv_attention(q, k, v, self.n_head)
 
         # Return updated kv cache
         return self.out(wv), k.detach(), v.detach()
 
@@ -323,29 +440,25 @@
     v: torch.Tensor,
     n_head: int,
     mask: Optional[torch.Tensor] = None,
 ) -> torch.Tensor:
     """
     Adapted from whisper.model.MultiHeadAttention.qkv_attention
     """
-    n_batch, n_ctx, n_state = q.shape
-
-    scale = (n_state // n_head) ** -0.25
-    q = q.view(*q.shape[:2], n_head, -1).permute(0, 2, 1, 3) * scale
-    k = k.view(*k.shape[:2], n_head, -1).permute(0, 2, 3, 1) * scale
-    v = v.view(*v.shape[:2], n_head, -1).permute(0, 2, 1, 3)
-
-    qk = q @ k
-    if mask is not None:
-        qk = qk + mask
-    # Use negative infinity to mask the zeros when doing the softmax.
-    qk = qk.float()
-
-    w = torch.nn.functional.softmax(qk, dim=-1).to(q.dtype)
-    return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2)
+    wv_list = []
+    # Split heads in qkv calculation
+    for h in range(n_head):
+        qk = q[:, h : h + 1, :, :] @ k[:, h : h + 1, :, :]
+        if mask is not None:
+            qk = qk + mask
+        w = torch.nn.functional.softmax(qk, dim=-1)
+        wv_list.append(w @ v[:, h : h + 1, :, :])
+    wv = torch.cat(wv_list, dim=1)
+    wv = wv.view(wv.shape[0], 1, -1)
+    return wv
 
 
 class ResidualAttentionBlockWrapper(torch.nn.Module):
     """
     Wrapper around whisper.model.ResidualAttentionBlock to leverage kv cache
     for efficient inference. The original whisper.model.ResidiualAttentionBlock
     doesn't returns the updated kv cache but relies on pytorch hook which
@@ -353,22 +466,34 @@
     """
 
     def __init__(self, model: whisper.model.ResidualAttentionBlock):
         super().__init__()
         assert isinstance(model, whisper.model.ResidualAttentionBlock)
         # Wraps `MultiheadAttention` to `MultiheadAttentionWrapper`
         self.attn = MHAWrapper(model.attn, "self_attention")
+
+        states_per_head = model.attn.query.weight.shape[0] // model.attn.n_head
+        scale = states_per_head**-0.25
         self.cross_attn = MHAWrapper(model.cross_attn, "cross_attention")
+
+        # Apply scale for qkv to parameters
+        with torch.no_grad():
+            self.attn.query.weight *= scale
+            self.attn.query.bias *= scale
+            self.attn.key.weight *= scale
+            self.cross_attn.query.weight *= scale
+            self.cross_attn.query.bias *= scale
+            self.cross_attn.key.weight *= scale
+
         for m in ["attn_ln", "cross_attn_ln", "mlp", "mlp_ln"]:
             self.add_module(m, getattr(model, m))
 
     def forward(
         self,
         x: torch.Tensor,
-        index: torch.Tensor,
         mask: torch.Tensor,
         kv_cache: Dict[torch.nn.Module, torch.Tensor],
     ):
         """
         Args: Same as MHAWrapper
         Returns: Same as MHAWrapper
         """
@@ -376,19 +501,19 @@
         assert isinstance(self.attn, torch.nn.Module)  # for mypy
         assert isinstance(self.attn_ln, torch.nn.Module)  # for mypy
         assert isinstance(self.cross_attn_ln, torch.nn.Module)  # for mypy
         assert isinstance(self.cross_attn, torch.nn.Module)  # for mypy
         assert isinstance(self.mlp, torch.nn.Module)  # for mypy
         assert isinstance(self.mlp_ln, torch.nn.Module)  # for mypy
         x_attn, k_cache, v_cache = self.attn(
-            self.attn_ln(x), index=index, mask=mask, kv_cache=kv_cache
+            self.attn_ln(x), mask=mask, kv_cache=kv_cache
         )
         x = x + x_attn
         if self.cross_attn:
             # Ignore cross attn kv cache which is constant (pre-computed in
             # `WhisperCrossAttnKVCacheTorch`)
             x_cross_attn, _, _ = self.cross_attn(
-                self.cross_attn_ln(x), index=index, mask=mask, kv_cache=kv_cache
+                self.cross_attn_ln(x), mask=mask, kv_cache=kv_cache
             )
             x = x + x_cross_attn
         x = x + self.mlp(self.mlp_ln(x))
         return x, k_cache, v_cache
```

## qai_hub_models/models/_shared/whisper/test_utils.py

```diff
@@ -9,15 +9,15 @@
 from qai_hub_models.models._shared.whisper.app import (
     WhisperApp,
     load_audio,
     load_mel_filter,
 )
 from qai_hub_models.models._shared.whisper.demo import TEST_AUDIO_PATH
 from qai_hub_models.models._shared.whisper.model import (
-    MAX_DECODE_LEN,
+    MEAN_DECODE_LEN,
     MEL_FILTER_PATH,
     Whisper,
     WhisperDecoderInf,
     WhisperEncoderInf,
 )
 
 
@@ -45,27 +45,44 @@
         tokens = torch.LongTensor([[50257]])
         logits_orig = model.decoder(tokens, audio_features).detach().numpy()
 
     # QAIHM
     encoder = WhisperEncoderInf(model)
     decoder = WhisperDecoderInf(model.decoder)
 
-    cross_attn_cache = encoder(mel_input)
-    sample_len = MAX_DECODE_LEN
-    cache_tensor = np.zeros([1, sample_len, decoder.attention_dim]).astype(np.float32)
+    k_cache_cross, v_cache_cross = encoder(mel_input)
+    sample_len = MEAN_DECODE_LEN
+
+    k_cache_self = torch.zeros(
+        (
+            decoder.num_blocks,
+            decoder.num_heads,
+            decoder.attention_dim // decoder.num_heads,
+            sample_len,
+        ),
+        dtype=torch.float32,
+    )
+    v_cache_self = torch.zeros(
+        (
+            decoder.num_blocks,
+            decoder.num_heads,
+            sample_len,
+            decoder.attention_dim // decoder.num_heads,
+        ),
+        dtype=torch.float32,
+    )
     index = torch.zeros([1, 1], dtype=torch.int32)
     index[0, 0] = 0
-    mask = torch.zeros(1, sample_len, decoder.attention_dim, dtype=torch.bool)
-    mask[:, 0, :] = 1
-    self_attn_cache = [cache_tensor] * 2 * decoder.num_blocks
     with torch.no_grad():
-        decoder_out = decoder(tokens, index, mask, *cross_attn_cache, *self_attn_cache)
+        decoder_out = decoder(
+            tokens, index, k_cache_cross, v_cache_cross, k_cache_self, v_cache_self
+        )
         logits = decoder_out[0].detach().numpy()
 
-    np.testing.assert_allclose(logits_orig, logits)
+    np.testing.assert_allclose(logits_orig, logits, rtol=5e-3)
 
 
 def run_test_transcribe(whisper_version):
     """
     Test that WhisperApp produces end to end transcription results that
     matches with the original model
     """
```

## qai_hub_models/models/_shared/yolo/demo.py

```diff
@@ -25,26 +25,27 @@
 def yolo_detection_demo(
     model_type: Type[HubModel],
     model_id: str,
     app_type: Callable[..., YoloObjectDetectionApp],
     default_image: str | CachedWebAsset,
     stride_multiple: int | None = None,
     is_test: bool = False,
+    default_score_threshold: float = 0.45,
 ):
     # Demo parameters
     parser = get_model_cli_parser(model_type)
     parser = get_on_device_demo_parser(parser, add_output_dir=True)
     image_help = "image file path or URL."
     if stride_multiple:
         image_help = f"{image_help} Image spatial dimensions (x and y) must be multiples of {stride_multiple}."
     parser.add_argument("--image", type=str, default=default_image, help=image_help)
     parser.add_argument(
         "--score-threshold",
         type=float,
-        default=0.45,
+        default=default_score_threshold,
         help="Score threshold for NonMaximumSuppression",
     )
     parser.add_argument(
         "--iou-threshold",
         type=float,
         default=0.7,
         help="Intersection over Union (IoU) threshold for NonMaximumSuppression",
```

## qai_hub_models/models/aotgan/export.py

```diff
@@ -116,20 +116,25 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image,mask"
+        + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image,mask"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +164,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image,mask", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image,mask", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,16 +197,20 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
```

## qai_hub_models/models/aotgan/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: AOT-GAN
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 172218.0
-      throughput: 5.806593968110186
+      inference_time: 164598.0
+      throughput: 6.075407963644759
       estimated_peak_memory_range:
-        min: 3301376
-        max: 6608312
+        min: 4349952
+        max: 7789760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 235
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 235
-      job_id: jlpeelxop
+      job_id: jmg9werlp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 162527.0
-      throughput: 6.15282383850068
+      inference_time: 164540.0
+      throughput: 6.077549532028686
       estimated_peak_memory_range:
-        min: 4247552
-        max: 34036840
+        min: 4341760
+        max: 36913480
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 275
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 275
-      job_id: jz5w21z35
+      job_id: jz57x3mlg
       job_status: Passed
     torchscript_onnx_ort:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jnp1yv18p
+      job_id: jegn384r5
       job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.302216Z'
+    timestamp: '2024-05-20T16:35:27.553176Z'
   - torchscript_onnx_tflite:
-      inference_time: 126778.0
-      throughput: 7.887803877644386
+      inference_time: 120809.0
+      throughput: 8.277528992045294
       estimated_peak_memory_range:
-        min: 2174976
-        max: 256099504
+        min: 2879488
+        max: 222384800
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 235
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 235
-      job_id: jygzo4yo5
+      job_id: jnp1ex92g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 119306.0
-      throughput: 8.381808123648433
+      inference_time: 121163.0
+      throughput: 8.253344667926678
       estimated_peak_memory_range:
-        min: 3887104
-        max: 166111904
+        min: 3252224
+        max: 144610800
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 275
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 275
-      job_id: jmg9jx2w5
+      job_id: jqp4v07vp
       job_status: Passed
     torchscript_onnx_ort:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jvgdez4r5
+      job_id: joprejr95
       job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.302592Z'
+    timestamp: '2024-05-20T16:35:27.684430Z'
   - torchscript_onnx_tflite:
-      inference_time: 171670.0
-      throughput: 5.825129609133803
+      inference_time: 161130.0
+      throughput: 6.206168931918326
       estimated_peak_memory_range:
-        min: 3219456
-        max: 6614600
+        min: 3170304
+        max: 13340440
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 235
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 235
-      job_id: jopr8dz05
+      job_id: jvgdolkep
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 162527.0
-      throughput: 6.15282383850068
+      inference_time: 164457.0
+      throughput: 6.080616817769995
       estimated_peak_memory_range:
-        min: 4337664
-        max: 32953256
+        min: 4214784
+        max: 29715440
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 275
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 275
-      job_id: j1p80rlkg
+      job_id: jo5m3y7wg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.302905Z'
+    timestamp: '2024-05-20T16:35:27.816239Z'
+  - torchscript_onnx_qnn:
+      inference_time: 145454.0
+      throughput: 6.87502578134668
+      estimated_peak_memory_range:
+        min: 4202496
+        max: 4202496
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 275
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 275
+      job_id: j0pxy2q1g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jep2ln14g
+      job_status: Failed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jqpy60l75
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:27.949164Z'
```

## qai_hub_models/models/controlnet_quantized/info.yaml

```diff
@@ -25,15 +25,15 @@
   ControlNet Number of parameters: 361M
   Model size: 1.4GB
 applicable_scenarios:
   - Image Generation
   - Image Editing
   - Content Creation
 related_models:
-  - stable_diffusion_quantized
+  - stable_diffusion_v1_5_quantized
 form_factors:
   - Phone
   - Tablet
 has_static_banner: yes
 has_animated_banner: no
 dataset: []
 license_type: apache-2.0
```

## qai_hub_models/models/controlnet_quantized/test.py

```diff
@@ -1,28 +1,27 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
-import tempfile
-
 import pytest
 
 from qai_hub_models.models.controlnet_quantized.demo import main as demo_main
 from qai_hub_models.models.controlnet_quantized.export import export_model
 from qai_hub_models.models.controlnet_quantized.model import ControlNetQuantized
+from qai_hub_models.utils.asset_loaders import qaihm_temp_dir
 
 
 def test_from_precompiled():
     ControlNetQuantized.from_precompiled()
 
 
 @pytest.mark.skip("#105 move slow_cloud and slow tests to nightly.")
 @pytest.mark.slow_cloud
 def test_export():
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         exported_jobs = export_model(
             # Testing text_encoder as it's smallest model in
             # ControlNet pipeline
             components=["text_encoder"],
             skip_inferencing=True,
             skip_downloading=True,
             skip_summary=True,
```

## qai_hub_models/models/convnext_tiny/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/convnext_tiny/perf.yaml

```diff
@@ -4,106 +4,231 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
+  - QCS8550 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
+  - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ConvNext-Tiny
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 11504.0
-      throughput: 86.92628650904034
+      inference_time: 5710.0
+      throughput: 175.13134851138355
+      estimated_peak_memory_range:
+        min: 49152
+        max: 2555016
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 328
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 328
+      job_id: j2p0l7w6p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 3790.0
+      throughput: 263.85224274406335
+      estimated_peak_memory_range:
+        min: 86016
+        max: 170428944
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 223
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 223
+      job_id: jn5q3on4p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 16263.0
+      throughput: 61.48927012236365
       estimated_peak_memory_range:
         min: 32768
-        max: 2493040
+        max: 155815696
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 380
+        layers_on_npu: 189
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 380
-      job_id: jlpeoyo7g
+        total_layers: 189
+      job_id: jwgo3qxxg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-02T15:30:19.195043Z'
+    timestamp: '2024-05-20T16:35:27.983972Z'
+  - torchscript_onnx_tflite:
+      inference_time: 3967.0
+      throughput: 252.07965717166624
+      estimated_peak_memory_range:
+        min: 16384
+        max: 210597920
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 328
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 328
+      job_id: j1p8zvnxp
+      job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 2727.0
+      throughput: 366.70333700036673
       estimated_peak_memory_range:
         min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        max: 88673616
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 223
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: ''
-      job_status: Skipped
-  - torchscript_onnx_tflite:
-      inference_time: 8139.0
-      throughput: 122.86521685710775
+        total_layers: 223
+      job_id: j1gl3rd8g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 11672.0
+      throughput: 85.67511994516792
       estimated_peak_memory_range:
-        min: 20480
-        max: 209217264
+        min: 618496
+        max: 58874592
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 380
+        layers_on_npu: 189
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 380
-      job_id: jygz2n2zg
+        total_layers: 189
+      job_id: j1pvvx8jp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-02T15:30:19.195057Z'
+    timestamp: '2024-05-20T16:35:27.984000Z'
+  - torchscript_onnx_tflite:
+      inference_time: 5754.0
+      throughput: 173.79214459506431
+      estimated_peak_memory_range:
+        min: 24576
+        max: 2610064
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 328
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 328
+      job_id: jogk3m125
+      job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 3773.0
+      throughput: 265.041081367612
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 90112
+        max: 202074560
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 223
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: ''
-      job_status: Skipped
+        total_layers: 223
+      job_id: j1p3e2dl5
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:27.984018Z'
+  - torchscript_onnx_qnn:
+      inference_time: 3953.0
+      throughput: 252.97242600556538
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 223
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 223
+      job_id: jw56nlx0g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 17160.0
+      throughput: 58.27505827505828
+      estimated_peak_memory_range:
+        min: 441618432
+        max: 441618432
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 189
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 189
+      job_id: j7gje49x5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 36452.0
+      throughput: 27.4333369911116
+      estimated_peak_memory_range:
+        min: 1425408
+        max: 1425408
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 202
+        total_layers: 202
+      job_id: jlpek3q1p
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:27.984040Z'
```

## qai_hub_models/models/ddrnet23_slim/export.py

```diff
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ddrnet23_slim/perf.yaml

```diff
@@ -18,117 +18,157 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: DDRNet23-Slim
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 6651.0
-      throughput: 150.35333032626673
+      inference_time: 6617.0
+      throughput: 151.1258878645912
       estimated_peak_memory_range:
-        min: 1007616
-        max: 2683032
+        min: 212992
+        max: 2249480
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 131
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 131
-      job_id: j0pxndrl5
+      job_id: jogk3mj25
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 9555.0
+      throughput: 104.65724751439038
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 11808768
+        max: 48661000
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 155
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jegnl7qq5
-      job_status: Failed
+        total_layers: 155
+      job_id: jw56nlk0g
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.356614Z'
+    timestamp: '2024-05-20T16:35:28.530240Z'
   - torchscript_onnx_tflite:
-      inference_time: 4569.0
-      throughput: 218.8662727073758
+      inference_time: 4661.0
+      throughput: 214.5462347135808
       estimated_peak_memory_range:
-        min: 16384
-        max: 71802832
+        min: 40960
+        max: 74706752
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 131
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 131
-      job_id: jo5mqdk9p
+      job_id: jn5q3oj4p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 6060.0
+      throughput: 165.01650165016503
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 2203648
+        max: 39944288
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 155
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jopr8nd75
-      job_status: Failed
+        total_layers: 155
+      job_id: j1p3e2yl5
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.356678Z'
+    timestamp: '2024-05-20T16:35:28.530261Z'
   - torchscript_onnx_tflite:
-      inference_time: 6682.0
-      throughput: 149.655791679138
+      inference_time: 6700.0
+      throughput: 149.2537313432836
       estimated_peak_memory_range:
-        min: 1011712
-        max: 3063152
+        min: 1019904
+        max: 2922360
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 131
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 131
-      job_id: jnp1y108p
+      job_id: j1gl3rj8g
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.356703Z'
+    timestamp: '2024-05-20T16:35:28.530273Z'
+  - torchscript_onnx_ort:
+      inference_time: 9528.0
+      throughput: 104.95382031905962
+      estimated_peak_memory_range:
+        min: 9854976
+        max: 9854976
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 155
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 155
+      job_id: jwgo3qjxg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 38162.0
+      throughput: 26.20407735443635
+      estimated_peak_memory_range:
+        min: 104890368
+        max: 104890368
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 105
+        total_layers: 105
+      job_id: j1pvvxjjp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.530292Z'
```

## qai_hub_models/models/deeplabv3_plus_mobilenet/export.py

```diff
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/deeplabv3_plus_mobilenet/perf.yaml

```diff
@@ -18,132 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: DeepLabV3-Plus-MobileNet
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 13206.0
-      throughput: 75.72315614114797
+      inference_time: 13090.0
+      throughput: 76.39419404125286
       estimated_peak_memory_range:
-        min: 21012480
-        max: 37177032
+        min: 20566016
+        max: 22394640
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 98
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 98
-      job_id: jz57xlxvg
+      job_id: j7gje4jx5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 12804.0
-      throughput: 78.10059356451109
+      inference_time: 12915.0
+      throughput: 77.42934572202864
       estimated_peak_memory_range:
-        min: 1888256
-        max: 20259784
+        min: 2191360
+        max: 18354728
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 124
-      job_id: jo5m363dg
+      job_id: jz5wqno65
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 18188.0
+      throughput: 54.98130635583902
+      estimated_peak_memory_range:
+        min: 46182400
+        max: 80384080
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: jz57x3zlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-30T00:18:21.085559Z'
+    timestamp: '2024-05-20T16:35:28.552289Z'
   - torchscript_onnx_tflite:
-      inference_time: 9587.0
-      throughput: 104.30791697089809
+      inference_time: 9659.0
+      throughput: 103.53038616834041
       estimated_peak_memory_range:
-        min: 49152
-        max: 66968480
+        min: 45056
+        max: 67351648
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 98
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 98
-      job_id: jqp4vdv8p
+      job_id: jlpek3j1p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 9430.0
-      throughput: 106.04453870625663
+      inference_time: 9460.0
+      throughput: 105.70824524312897
       estimated_peak_memory_range:
-        min: 3174400
-        max: 56418592
+        min: 3194880
+        max: 56272272
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 124
-      job_id: jegn3m3k5
+      job_id: jmg9wevlp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 14020.0
+      throughput: 71.32667617689016
+      estimated_peak_memory_range:
+        min: 51036160
+        max: 82578320
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: jqp4v0qvp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-30T00:18:21.085754Z'
+    timestamp: '2024-05-20T16:35:28.552316Z'
   - torchscript_onnx_tflite:
-      inference_time: 13237.0
-      throughput: 75.54581853894386
+      inference_time: 13152.0
+      throughput: 76.03406326034063
       estimated_peak_memory_range:
-        min: 22167552
-        max: 24453856
+        min: 19988480
+        max: 21886552
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 98
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 98
-      job_id: j0pxy6y3g
+      job_id: jygzrk1k5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 12986.0
-      throughput: 77.00600646850454
+      inference_time: 12898.0
+      throughput: 77.53140021708792
       estimated_peak_memory_range:
-        min: 3194880
-        max: 26458984
+        min: 3207168
+        max: 24171704
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 124
-      job_id: jopre2e05
+      job_id: jvgdolwep
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-30T00:18:21.085930Z'
+    timestamp: '2024-05-20T16:35:28.552334Z'
+  - torchscript_onnx_qnn:
+      inference_time: 16530.0
+      throughput: 60.49606775559589
+      estimated_peak_memory_range:
+        min: 3170304
+        max: 3170304
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 124
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 124
+      job_id: jnp1ex02g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 16738.0
+      throughput: 59.7442944198829
+      estimated_peak_memory_range:
+        min: 107229184
+        max: 107229184
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: j0pxy2v1g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 192375.0
+      throughput: 5.198180636777128
+      estimated_peak_memory_range:
+        min: 387981312
+        max: 387981312
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jo5m3yrwg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.552356Z'
```

## qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/export.py

```diff
@@ -120,20 +120,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -167,16 +171,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -196,27 +202,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/perf.yaml

```diff
@@ -22,210 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: DeepLabV3-Plus-MobileNet-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 3523.0
-      throughput: 283.84899233607723
+      inference_time: 3349.0
+      throughput: 298.59659599880564
       estimated_peak_memory_range:
         min: 12288
-        max: 2061128
+        max: 1753112
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 99
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 99
-      job_id: j2p0l2l9p
+      job_id: jegn382r5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 5308.0
-      throughput: 188.39487565938205
+      inference_time: 5370.0
+      throughput: 186.21973929236498
       estimated_peak_memory_range:
         min: 806912
-        max: 9579664
+        max: 8194984
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 100
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 100
-      job_id: jw56nzn6g
+      job_id: jqpy60e75
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 18506.0
+      throughput: 54.03652869339674
+      estimated_peak_memory_range:
+        min: 102789120
+        max: 122435512
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 122
+        layers_on_gpu: 0
+        layers_on_cpu: 51
+        total_layers: 173
+      job_id: jn5q3o84p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-30T00:18:21.431467Z'
+    timestamp: '2024-05-20T16:35:28.582800Z'
   - torchscript_onnx_tflite:
-      inference_time: 2623.0
-      throughput: 381.2428516965307
+      inference_time: 2567.0
+      throughput: 389.5597974289053
       estimated_peak_memory_range:
         min: 12288
-        max: 58004960
+        max: 57529696
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 99
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 99
-      job_id: j1p8zmzkp
+      job_id: joprejk95
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 3894.0
-      throughput: 256.8053415511043
+      inference_time: 3971.0
+      throughput: 251.82573659027952
       estimated_peak_memory_range:
-        min: 802816
-        max: 58260400
+        min: 962560
+        max: 56354464
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 100
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 100
-      job_id: j1p3e1e35
+      job_id: j2p0l7y6p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 13687.0
+      throughput: 73.06202966318405
+      estimated_peak_memory_range:
+        min: 80236544
+        max: 138756336
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 122
+        layers_on_gpu: 0
+        layers_on_cpu: 51
+        total_layers: 173
+      job_id: j1gl3rn8g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-30T00:18:21.431668Z'
+    timestamp: '2024-05-20T16:35:28.582825Z'
   - torchscript_onnx_tflite:
-      inference_time: 15123.0
-      throughput: 66.12444620776301
+      inference_time: 3337.0
+      throughput: 299.6703626011387
       estimated_peak_memory_range:
-        min: 40960
-        max: 41498720
+        min: 12288
+        max: 2058944
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 99
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 99
-      job_id: jn5q3r3np
+      job_id: jep2ln84g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 19868.0
-      throughput: 50.33219247030401
+      inference_time: 5351.0
+      throughput: 186.88095683049897
       estimated_peak_memory_range:
-        min: 802816
-        max: 50369568
+        min: 0
+        max: 6063744
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 100
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 100
-      job_id: j1pvvrwkp
+      job_id: jogk3mz25
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
+      name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs6490
-    timestamp: '2024-04-30T00:18:21.431848Z'
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:28.582842Z'
   - torchscript_onnx_tflite:
-      inference_time: 124831.0
-      throughput: 8.010830643029376
+      inference_time: 15025.0
+      throughput: 66.55574043261231
       estimated_peak_memory_range:
-        min: 11464704
-        max: 28637488
+        min: 5541888
+        max: 47370848
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 96
-        layers_on_gpu: 3
+        layers_on_npu: 99
+        layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 99
-      job_id: j1gl323jg
+      job_id: jnp1e1wkg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 20149.0
+      throughput: 49.63025460320611
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 12288
+        max: 49872128
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 100
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: j7gje2lv5
-      job_status: Failed
+        total_layers: 100
+      job_id: jegn3q3v5
+      job_status: Passed
     reference_device_info:
-      name: RB5 (Proxy)
+      name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8250
-    timestamp: '2024-04-30T00:18:21.432019Z'
+      chipset: Qcs6490
+    timestamp: '2024-05-20T16:35:28.582857Z'
   - torchscript_onnx_tflite:
-      inference_time: 3534.0
-      throughput: 282.9654782116582
+      inference_time: 125926.0
+      throughput: 7.941171799310706
       estimated_peak_memory_range:
-        min: 12288
-        max: 17568944
+        min: 11571200
+        max: 17936624
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 99
-        layers_on_gpu: 0
+        layers_on_npu: 96
+        layers_on_gpu: 3
         layers_on_cpu: 0
         total_layers: 99
-      job_id: jogk3q3w5
+      job_id: jvgdo4qkp
       job_status: Passed
-    torchscript_onnx_qnn:
-      inference_time: 5297.0
-      throughput: 188.78610534264678
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-05-20T16:35:28.582868Z'
+  - torchscript_onnx_qnn:
+      inference_time: 5343.0
+      throughput: 187.16077110237694
       estimated_peak_memory_range:
-        min: 831488
-        max: 14169232
+        min: 790528
+        max: 790528
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 100
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 100
-      job_id: jwgo3n3qg
+      job_id: j1p8zvoxp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 50376.0
+      throughput: 19.850722566301414
+      estimated_peak_memory_range:
+        min: 130891776
+        max: 130891776
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 122
+        layers_on_gpu: 0
+        layers_on_cpu: 51
+        total_layers: 173
+      job_id: jw56nl60g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 472873.0
+      throughput: 2.114732708359327
+      estimated_peak_memory_range:
+        min: 248066048
+        max: 248066048
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j1p3e2kl5
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-30T00:18:21.432205Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.582890Z'
```

## qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/test.py

```diff
@@ -1,13 +1,12 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 import os
-import tempfile
 import zipfile
 
 import torch
 
 from qai_hub_models.models._shared.deeplab.app import DeepLabV3App
 from qai_hub_models.models._shared.deeplab.model import NUM_CLASSES
 from qai_hub_models.models.deeplabv3_plus_mobilenet.test import INPUT_IMAGE_ADDRESS
@@ -19,14 +18,15 @@
     MODEL_ID,
     DeepLabV3PlusMobilenetQuantizable,
 )
 from qai_hub_models.utils.asset_loaders import (
     CachedWebModelAsset,
     load_image,
     load_numpy,
+    qaihm_temp_dir,
 )
 from qai_hub_models.utils.testing import skip_clone_repo_check
 
 OUTPUT_IMAGE_MASK = CachedWebModelAsset.from_asset_store(
     MODEL_ID, MODEL_ASSET_VERSION, "deeplab_output_mask.npy"
 )
 
@@ -44,15 +44,15 @@
     assert (output_mask == output_mask_gt).mean() > 0.95
 
 
 @skip_clone_repo_check
 def test_aimet_export():
     model = DeepLabV3PlusMobilenetQuantizable.from_pretrained()
     name = model.__class__.__name__
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         output_zip = model.convert_to_onnx_and_aimet_encodings(
             tmpdir,
         )
         assert os.path.exists(output_zip)
         with zipfile.ZipFile(output_zip, "r") as zip:
             assert zip.namelist() == [
                 f"{name}.aimet/",
```

## qai_hub_models/models/deeplabv3_resnet50/export.py

```diff
@@ -116,20 +116,25 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        + " --force_channel_last_output output_0,output_1"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0,output_1",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +164,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +195,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0,output_1", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0,output_1", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/deeplabv3_resnet50/model.py

```diff
@@ -1,14 +1,17 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
+from typing import Optional
+
 import torchvision.models as tv_models
+from qai_hub.client import Device
 
 from qai_hub_models.models._shared.deeplab.model import DeepLabV3Model
 from qai_hub_models.utils.base_model import TargetRuntime
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 2
 DEFAULT_WEIGHTS = "COCO_WITH_VOC_LABELS_V1"
@@ -19,24 +22,40 @@
 
     @classmethod
     def from_pretrained(cls, weights: str = DEFAULT_WEIGHTS) -> DeepLabV3_ResNet50:
         model = tv_models.segmentation.deeplabv3_resnet50(weights=weights).eval()
         return cls(model)
 
     def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
+        self,
+        target_runtime: TargetRuntime,
+        other_compile_options: str = "",
+        device: Optional[Device] = None,
     ) -> str:
         compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
+            target_runtime, other_compile_options, device
         )
-        return compile_options + " --compute_unit gpu"
+        if (
+            target_runtime == TargetRuntime.TFLITE
+            and "--compute_unit" not in compile_options
+        ):
+            compile_options = compile_options + " --compute_unit gpu"
+        return compile_options
 
     def get_hub_profile_options(
-        self, target_runtime: TargetRuntime, other_profile_options: str = ""
+        self,
+        target_runtime: TargetRuntime,
+        other_profile_options: str = "",
     ) -> str:
         profile_options = super().get_hub_profile_options(
-            target_runtime, other_profile_options
+            target_runtime,
+            other_profile_options,
         )
-        return profile_options + " --compute_unit gpu"
+        if (
+            target_runtime == TargetRuntime.TFLITE
+            and "--compute_unit" not in profile_options
+        ):
+            profile_options = profile_options + " --compute_unit gpu"
+        return profile_options
 
     def forward(self, image):
         return super().forward(image)["out"]
```

## qai_hub_models/models/deeplabv3_resnet50/perf.yaml

```diff
@@ -18,132 +18,172 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: DeepLabV3-ResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 290847.0
-      throughput: 3.4382338480369405
+      inference_time: 295509.0
+      throughput: 3.383991688916412
       estimated_peak_memory_range:
-        min: 32768
-        max: 223952912
+        min: 12288
+        max: 211050624
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 95
         layers_on_cpu: 0
         total_layers: 95
-      job_id: jz5wq3935
+      job_id: jwgo3qyxg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 810711.0
-      throughput: 1.23348517535842
+      inference_time: 'null'
+      throughput: 'null'
       estimated_peak_memory_range:
-        min: 3481600
-        max: 11830488
-      primary_compute_unit: GPU
-      precision: fp16
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 83
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 83
-      job_id: jvgdoqvrp
-      job_status: Passed
+        total_layers: 0
+      job_id: jlpek391p
+      job_status: Failed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jmg9we1lp
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-30T00:18:21.450422Z'
+    timestamp: '2024-05-20T16:35:28.622222Z'
   - torchscript_onnx_tflite:
-      inference_time: 228363.0
-      throughput: 4.37899309432789
+      inference_time: 227563.0
+      throughput: 4.394387488299944
       estimated_peak_memory_range:
-        min: 102400
-        max: 31114256
+        min: 69632
+        max: 30257776
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 95
         layers_on_cpu: 0
         total_layers: 95
-      job_id: jmg9wy4wp
+      job_id: j1pvvx3jp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 588856.0
-      throughput: 1.6982080508647275
+      inference_time: 'null'
+      throughput: 'null'
       estimated_peak_memory_range:
-        min: 3207168
-        max: 37364864
-      primary_compute_unit: GPU
-      precision: fp16
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 83
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 83
-      job_id: jz57xldvg
-      job_status: Passed
+        total_layers: 0
+      job_id: jygzrkek5
+      job_status: Failed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jnp1exl2g
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-30T00:18:21.450461Z'
+    timestamp: '2024-05-20T16:35:28.622250Z'
   - torchscript_onnx_tflite:
+      inference_time: 292688.0
+      throughput: 3.416607445470945
+      estimated_peak_memory_range:
+        min: 1380352
+        max: 149690448
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 95
+        layers_on_cpu: 0
+        total_layers: 95
+      job_id: j7gje4xx5
+      job_status: Passed
+    torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jnp1ew88g
+      job_id: jz5wqnv65
       job_status: Failed
-    torchscript_onnx_qnn:
-      inference_time: 821173.0
-      throughput: 1.217770189716418
-      estimated_peak_memory_range:
-        min: 3436544
-        max: 12462344
-      primary_compute_unit: GPU
-      precision: fp16
-      layer_info:
-        layers_on_npu: 0
-        layers_on_gpu: 83
-        layers_on_cpu: 0
-        total_layers: 83
-      job_id: jqp4vdw8p
-      job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-30T00:18:21.450490Z'
+    timestamp: '2024-05-20T16:35:28.622267Z'
+  - reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.622274Z'
```

## qai_hub_models/models/densenet121/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/densenet121/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: DenseNet-121
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1945.0
-      throughput: 514.1388174807198
+      inference_time: 1948.0
+      throughput: 513.347022587269
       estimated_peak_memory_range:
         min: 16384
-        max: 2306688
+        max: 2162632
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 312
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 312
-      job_id: jvgdezmz5
+      job_id: jvgdol9ep
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2005.0
-      throughput: 498.75311720698255
+      inference_time: 1981.0
+      throughput: 504.79555779909134
       estimated_peak_memory_range:
         min: 12288
-        max: 40807680
+        max: 18832024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 372
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 372
-      job_id: jqp4k921g
+      job_id: jnp1exl8g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 1971.0
+      throughput: 507.35667174023337
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 12288
+        max: 46477632
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 374
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jo5mqdl9p
-      job_status: Failed
+        total_layers: 374
+      job_id: j0pxy2j3g
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.432252Z'
+    timestamp: '2024-05-20T16:35:28.650430Z'
   - torchscript_onnx_tflite:
-      inference_time: 1282.0
-      throughput: 780.0312012480499
+      inference_time: 1321.0
+      throughput: 757.002271006813
       estimated_peak_memory_range:
-        min: 12288
-        max: 95228096
+        min: 16384
+        max: 95688784
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 312
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 312
-      job_id: jz570789g
+      job_id: jz5wqnv35
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1330.0
-      throughput: 751.8796992481203
+      inference_time: 1319.0
+      throughput: 758.1501137225171
       estimated_peak_memory_range:
         min: 618496
-        max: 155690704
+        max: 162806592
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 372
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 372
-      job_id: j0pxndzl5
+      job_id: jvgdol9rp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 1355.0
+      throughput: 738.0073800738007
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 618496
+        max: 49577808
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 374
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jegnl7wq5
-      job_status: Failed
+        total_layers: 374
+      job_id: jo5m3y2dg
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.432349Z'
+    timestamp: '2024-05-20T16:35:28.650456Z'
   - torchscript_onnx_tflite:
-      inference_time: 1944.0
-      throughput: 514.40329218107
+      inference_time: 1948.0
+      throughput: 513.347022587269
       estimated_peak_memory_range:
-        min: 20480
-        max: 2194800
+        min: 28672
+        max: 2603520
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 312
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 312
-      job_id: j0pxnrjl5
+      job_id: jmg9we1wp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2008.0
-      throughput: 498.00796812749
+      inference_time: 1983.0
+      throughput: 504.2864346949067
       estimated_peak_memory_range:
-        min: 12288
-        max: 40726728
+        min: 622592
+        max: 6049752
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 372
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 372
-      job_id: jep20d6qg
+      job_id: jqp4v0o8p
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.432464Z'
+    timestamp: '2024-05-20T16:35:28.650472Z'
+  - torchscript_onnx_qnn:
+      inference_time: 2255.0
+      throughput: 443.4589800443459
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 372
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 372
+      job_id: jz57x3wvg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 2014.0
+      throughput: 496.52432969215494
+      estimated_peak_memory_range:
+        min: 606208
+        max: 606208
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 374
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 374
+      job_id: jegn38yk5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 17596.0
+      throughput: 56.83109797681291
+      estimated_peak_memory_range:
+        min: 856064
+        max: 856064
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 311
+        total_layers: 311
+      job_id: joprejq05
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.650494Z'
```

## qai_hub_models/models/detr_resnet101/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +202,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/detr_resnet101/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: DETR-ResNet101
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 47978.0
-      throughput: 20.842886322897996
+      inference_time: 24664.0
+      throughput: 40.5449237755433
       estimated_peak_memory_range:
-        min: 94208
-        max: 9060976
+        min: 438272
+        max: 3728248
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 910
-        layers_on_gpu: 2
+        layers_on_npu: 839
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 839
+      job_id: jep2ln6rg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 21040.0
+      throughput: 47.52851711026616
+      estimated_peak_memory_range:
+        min: 2801664
+        max: 31885224
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1084
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 912
-      job_id: jep20vzqg
+        total_layers: 1084
+      job_id: j1p8zv9kp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 26243.0
-      throughput: 38.105399535114124
+      inference_time: 22542.0
+      throughput: 44.36163605713779
       estimated_peak_memory_range:
-        min: 0
-        max: 299546600
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 16384
+        max: 296984832
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 3
+        layers_on_npu: 856
         layers_on_gpu: 0
-        layers_on_cpu: 5
-        total_layers: 8
-      job_id: j2p03vxnp
+        layers_on_cpu: 0
+        total_layers: 856
+      job_id: jw56nlj6g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.456092Z'
+    timestamp: '2024-05-20T16:35:28.680620Z'
   - torchscript_onnx_tflite:
-      inference_time: 35573.0
-      throughput: 28.111207938605123
+      inference_time: 17307.0
+      throughput: 57.78008898133703
       estimated_peak_memory_range:
-        min: 28672
-        max: 261178736
+        min: 106496
+        max: 282048208
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 910
-        layers_on_gpu: 2
+        layers_on_npu: 839
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 912
-      job_id: jqpyr7yl5
+        total_layers: 839
+      job_id: jqpy60w85
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 15126.0
+      throughput: 66.11133148221606
+      estimated_peak_memory_range:
+        min: 2797568
+        max: 330730224
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1084
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1084
+      job_id: jogk3mnw5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 19779.0
-      throughput: 50.558673340411545
+      inference_time: 15844.0
+      throughput: 63.11537490532694
       estimated_peak_memory_range:
-        min: 3723264
-        max: 90043392
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 2781184
+        max: 113431904
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 3
+        layers_on_npu: 856
         layers_on_gpu: 0
-        layers_on_cpu: 5
-        total_layers: 8
-      job_id: j1p804kog
+        layers_on_cpu: 0
+        total_layers: 856
+      job_id: j1p3e2335
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.456228Z'
+    timestamp: '2024-05-20T16:35:28.680646Z'
   - torchscript_onnx_tflite:
-      inference_time: 48057.0
-      throughput: 20.80862309340991
+      inference_time: 24760.0
+      throughput: 40.38772213247173
       estimated_peak_memory_range:
-        min: 1380352
-        max: 12433288
+        min: 405504
+        max: 3265984
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 910
-        layers_on_gpu: 2
+        layers_on_npu: 839
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 912
-      job_id: j1gl68rmg
+        total_layers: 839
+      job_id: j2p0l7q9p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 21118.0
+      throughput: 47.35296903115825
+      estimated_peak_memory_range:
+        min: 2813952
+        max: 31273000
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1084
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1084
+      job_id: j1gl3rzjg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.456328Z'
+    timestamp: '2024-05-20T16:35:28.680664Z'
+  - torchscript_onnx_qnn:
+      inference_time: 31213.0
+      throughput: 32.03793291256848
+      estimated_peak_memory_range:
+        min: 2768896
+        max: 2768896
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1084
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1084
+      job_id: jn5q3oknp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 23126.0
+      throughput: 43.24137334601747
+      estimated_peak_memory_range:
+        min: 117997568
+        max: 117997568
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 856
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 856
+      job_id: jwgo3q0qg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 1815598.0
+      throughput: 0.5507827173195828
+      estimated_peak_memory_range:
+        min: 280969216
+        max: 280969216
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 628
+        total_layers: 628
+      job_id: j1pvvxxkp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.680687Z'
```

## qai_hub_models/models/detr_resnet101_dc5/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +202,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/detr_resnet101_dc5/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: DETR-ResNet101-DC5
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 407929.0
-      throughput: 2.451406985039063
+      inference_time: 146017.0
+      throughput: 6.848517638357178
       estimated_peak_memory_range:
-        min: 7622656
-        max: 15500416
+        min: 1216512
+        max: 4088024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 911
-        layers_on_gpu: 2
+        layers_on_npu: 840
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 840
+      job_id: j7gje44v5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 142673.0
+      throughput: 7.009034645658254
+      estimated_peak_memory_range:
+        min: 2891776
+        max: 63987360
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1084
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 913
-      job_id: jn5qemdo5
+        total_layers: 1084
+      job_id: jz5wqnn35
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 179129.0
-      throughput: 5.582568986596252
+      inference_time: 135442.0
+      throughput: 7.383234151887893
       estimated_peak_memory_range:
-        min: 2637824
-        max: 309754336
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 2297856
+        max: 306707784
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 3
+        layers_on_npu: 856
         layers_on_gpu: 0
-        layers_on_cpu: 5
-        total_layers: 8
-      job_id: jw56ed9yg
+        layers_on_cpu: 0
+        total_layers: 856
+      job_id: jz57x33vg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.473830Z'
+    timestamp: '2024-05-20T16:35:28.711241Z'
   - torchscript_onnx_tflite:
-      inference_time: 311354.0
-      throughput: 3.2117782331365583
+      inference_time: 107206.0
+      throughput: 9.327836128574893
       estimated_peak_memory_range:
-        min: 90112
-        max: 447334464
+        min: 790528
+        max: 492355520
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 911
-        layers_on_gpu: 2
+        layers_on_npu: 840
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 913
-      job_id: j1gl619mg
+        total_layers: 840
+      job_id: jlpek33op
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 100534.0
+      throughput: 9.946883641355164
+      estimated_peak_memory_range:
+        min: 460566528
+        max: 811388336
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1084
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1084
+      job_id: jmg9weewp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 135318.0
-      throughput: 7.3899998522000026
+      inference_time: 95212.0
+      throughput: 10.502877788514052
       estimated_peak_memory_range:
-        min: 10055680
-        max: 190681632
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 4116480
+        max: 168196992
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 3
+        layers_on_npu: 856
         layers_on_gpu: 0
-        layers_on_cpu: 5
-        total_layers: 8
-      job_id: j1p3vwlng
+        layers_on_cpu: 0
+        total_layers: 856
+      job_id: jqp4v008p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.473979Z'
+    timestamp: '2024-05-20T16:35:28.711267Z'
   - torchscript_onnx_tflite:
-      inference_time: 405436.0
-      throughput: 2.4664805296026993
+      inference_time: 141747.0
+      throughput: 7.054823029764298
       estimated_peak_memory_range:
-        min: 6467584
-        max: 13861952
+        min: 184320
+        max: 5835464
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 911
-        layers_on_gpu: 2
+        layers_on_npu: 840
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 913
-      job_id: jlpeex3vp
+        total_layers: 840
+      job_id: jygzrkko5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 144502.0
+      throughput: 6.92031944194544
+      estimated_peak_memory_range:
+        min: 2871296
+        max: 58689696
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1084
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1084
+      job_id: jvgdollrp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.474090Z'
+    timestamp: '2024-05-20T16:35:28.711285Z'
+  - torchscript_onnx_qnn:
+      inference_time: 172453.0
+      throughput: 5.798681379854221
+      estimated_peak_memory_range:
+        min: 2772992
+        max: 2772992
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1084
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1084
+      job_id: jnp1exx8g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 125853.0
+      throughput: 7.945778010853933
+      estimated_peak_memory_range:
+        min: 119799808
+        max: 119799808
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 856
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 856
+      job_id: j0pxy223g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 2208720.0
+      throughput: 0.4527509145568474
+      estimated_peak_memory_range:
+        min: 280973312
+        max: 280973312
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 628
+        total_layers: 628
+      job_id: jo5m3yydg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.711307Z'
```

## qai_hub_models/models/detr_resnet50/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +202,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/detr_resnet50/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: DETR-ResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 39035.0
-      throughput: 25.618035096708084
+      inference_time: 20791.0
+      throughput: 48.0977345967005
       estimated_peak_memory_range:
-        min: 1327104
-        max: 9193440
+        min: 57344
+        max: 3249616
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 842
-        layers_on_gpu: 2
+        layers_on_npu: 771
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 771
+      job_id: jegn388k5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 19328.0
+      throughput: 51.73841059602649
+      estimated_peak_memory_range:
+        min: 2805760
+        max: 23254680
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 863
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 844
-      job_id: j1pv09yr5
+        total_layers: 863
+      job_id: jqpy60085
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 22280.0
-      throughput: 44.88330341113106
+      inference_time: 16790.0
+      throughput: 59.55926146515783
       estimated_peak_memory_range:
-        min: 1789952
-        max: 205559344
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 536576
+        max: 208713080
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 3
+        layers_on_npu: 737
         layers_on_gpu: 0
-        layers_on_cpu: 5
-        total_layers: 8
-      job_id: jlpeel0vp
+        layers_on_cpu: 0
+        total_layers: 737
+      job_id: jn5q3oonp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.491676Z'
+    timestamp: '2024-05-20T16:35:28.741938Z'
   - torchscript_onnx_tflite:
-      inference_time: 28469.0
-      throughput: 35.12592644631002
+      inference_time: 14384.0
+      throughput: 69.52169076751946
       estimated_peak_memory_range:
-        min: 1241088
-        max: 215942624
+        min: 409600
+        max: 231124128
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 842
-        layers_on_gpu: 2
+        layers_on_npu: 771
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 844
-      job_id: j7gjzw6e5
+        total_layers: 771
+      job_id: joprejj05
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 13592.0
+      throughput: 73.57268981753973
+      estimated_peak_memory_range:
+        min: 2801664
+        max: 247117184
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 863
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 863
+      job_id: j2p0l779p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 17238.0
-      throughput: 58.0113702285648
+      inference_time: 11524.0
+      throughput: 86.77542519958348
       estimated_peak_memory_range:
-        min: 3723264
-        max: 80445392
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 4878336
+        max: 99183200
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 3
+        layers_on_npu: 737
         layers_on_gpu: 0
-        layers_on_cpu: 5
-        total_layers: 8
-      job_id: jygzo4qx5
+        layers_on_cpu: 0
+        total_layers: 737
+      job_id: j1gl3rrjg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.491805Z'
+    timestamp: '2024-05-20T16:35:28.741963Z'
   - torchscript_onnx_tflite:
-      inference_time: 38866.0
-      throughput: 25.729429321257655
+      inference_time: 20731.0
+      throughput: 48.23693984853601
       estimated_peak_memory_range:
-        min: 1429504
-        max: 8463712
+        min: 405504
+        max: 3824656
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 842
-        layers_on_gpu: 2
+        layers_on_npu: 771
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 844
-      job_id: jz570n39g
+        total_layers: 771
+      job_id: jep2lnnrg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 19426.0
+      throughput: 51.47740142077628
+      estimated_peak_memory_range:
+        min: 40960
+        max: 25594136
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 863
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 863
+      job_id: jogk3mmw5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.491909Z'
+    timestamp: '2024-05-20T16:35:28.741980Z'
+  - torchscript_onnx_qnn:
+      inference_time: 22410.0
+      throughput: 44.62293618920125
+      estimated_peak_memory_range:
+        min: 2768896
+        max: 2768896
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 863
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 863
+      job_id: j1p8zvvkp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 17039.0
+      throughput: 58.68889019308645
+      estimated_peak_memory_range:
+        min: 33472512
+        max: 33472512
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 737
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 737
+      job_id: jw56nll6g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 68004.0
+      throughput: 14.705017351920475
+      estimated_peak_memory_range:
+        min: 3866624
+        max: 3866624
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 577
+        total_layers: 577
+      job_id: j1p3e2235
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.742002Z'
```

## qai_hub_models/models/detr_resnet50_dc5/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +202,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/detr_resnet50_dc5/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: DETR-ResNet50-DC5
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 405395.0
-      throughput: 2.4667299794028046
+      inference_time: 135457.0
+      throughput: 7.382416560236828
       estimated_peak_memory_range:
-        min: 339968
-        max: 8125832
+        min: 1200128
+        max: 4621488
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 843
-        layers_on_gpu: 2
+        layers_on_npu: 772
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 772
+      job_id: jwgo3qqqg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 121332.0
+      throughput: 8.2418488115254
+      estimated_peak_memory_range:
+        min: 65536
+        max: 55100088
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 863
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 845
-      job_id: jmg9jx785
+        total_layers: 863
+      job_id: jlpevmmo5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 174726.0
-      throughput: 5.723246683378547
+      inference_time: 119137.0
+      throughput: 8.39369801153294
       estimated_peak_memory_range:
-        min: 7774208
-        max: 210473208
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 3
-        layers_on_gpu: 0
-        layers_on_cpu: 5
-        total_layers: 8
-      job_id: jvgdezyz5
+        min: 679936
+        max: 229172048
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 737
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 737
+      job_id: jnp18zz8g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.509285Z'
+    timestamp: '2024-05-20T16:35:28.772381Z'
   - torchscript_onnx_tflite:
-      inference_time: 306266.0
-      throughput: 3.26513553577609
+      inference_time: 102211.0
+      throughput: 9.78368277386974
       estimated_peak_memory_range:
-        min: 16384
-        max: 412400848
+        min: 1204224
+        max: 442913328
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 843
-        layers_on_gpu: 2
+        layers_on_npu: 772
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 845
-      job_id: jnp1yvk7p
+        total_layers: 772
+      job_id: j1pvwkkkg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 92508.0
+      throughput: 10.809875902624638
+      estimated_peak_memory_range:
+        min: 2818048
+        max: 287246416
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 863
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 863
+      job_id: jygz7ddop
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 130531.0
-      throughput: 7.66101539097992
+      inference_time: 90890.0
+      throughput: 11.002310485201892
       estimated_peak_memory_range:
-        min: 10014720
-        max: 184574640
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 3
-        layers_on_gpu: 0
-        layers_on_cpu: 5
-        total_layers: 8
-      job_id: jz570719g
+        min: 4927488
+        max: 146881408
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 737
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 737
+      job_id: jvgdv11rg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.509384Z'
+    timestamp: '2024-05-20T16:35:28.772406Z'
   - torchscript_onnx_tflite:
-      inference_time: 400391.0
-      throughput: 2.497558636432887
+      inference_time: 134542.0
+      throughput: 7.432623270056934
       estimated_peak_memory_range:
-        min: 7581696
-        max: 16235952
+        min: 1204224
+        max: 4576992
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 843
-        layers_on_gpu: 2
+        layers_on_npu: 772
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 845
-      job_id: jegnlq8q5
+        total_layers: 772
+      job_id: j7gjlnnvp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 133524.0
+      throughput: 7.4892903148497645
+      estimated_peak_memory_range:
+        min: 16384
+        max: 52330520
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 863
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 863
+      job_id: jmg94nnw5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.509493Z'
+    timestamp: '2024-05-20T16:35:28.772424Z'
+  - torchscript_onnx_qnn:
+      inference_time: 165859.0
+      throughput: 6.029217588433549
+      estimated_peak_memory_range:
+        min: 2772992
+        max: 2772992
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 863
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 863
+      job_id: jz5w9663p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 119044.0
+      throughput: 8.40025536776318
+      estimated_peak_memory_range:
+        min: 31268864
+        max: 31268864
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 737
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 737
+      job_id: jz57drrv5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jqp4wrr8g
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.772448Z'
```

## qai_hub_models/models/efficientnet_b0/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/efficientnet_b0/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: EfficientNet-B0
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1637.0
-      throughput: 610.8735491753207
+      inference_time: 1623.0
+      throughput: 616.1429451632779
       estimated_peak_memory_range:
         min: 24576
-        max: 18330576
+        max: 2090224
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 245
-      job_id: j0pxnd8l5
+      job_id: j0px1oo3g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1692.0
-      throughput: 591.016548463357
+      inference_time: 1678.0
+      throughput: 595.9475566150179
       estimated_peak_memory_range:
-        min: 16384
-        max: 89136624
+        min: 12288
+        max: 88022416
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 243
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 243
-      job_id: jegnl7dq5
+      job_id: jopry330g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1847.0
-      throughput: 541.4185165132648
+      inference_time: 1575.0
+      throughput: 634.9206349206349
       estimated_peak_memory_range:
         min: 12288
-        max: 80485720
+        max: 80602048
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jep20vqqg
+        total_layers: 245
+      job_id: j1p87yyk5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.527091Z'
+    timestamp: '2024-05-20T16:35:28.802791Z'
   - torchscript_onnx_tflite:
-      inference_time: 1177.0
-      throughput: 849.6176720475786
+      inference_time: 1162.0
+      throughput: 860.5851979345955
       estimated_peak_memory_range:
         min: 16384
-        max: 70869408
+        max: 71535472
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 245
-      job_id: jo5mqd19p
+      job_id: jo5mzxxdp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1180.0
-      throughput: 847.457627118644
+      inference_time: 1182.0
+      throughput: 846.0236886632825
       estimated_peak_memory_range:
-        min: 0
-        max: 70362624
+        min: 618496
+        max: 69430064
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 243
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 243
-      job_id: jopr8nm75
+      job_id: jep2myyr5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1299.0
-      throughput: 769.8229407236336
+      inference_time: 1137.0
+      throughput: 879.5074758135444
       estimated_peak_memory_range:
-        min: 761856
-        max: 28745360
+        min: 0
+        max: 34872096
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jqpyr7kl5
+        total_layers: 245
+      job_id: jogkyxxwp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.527166Z'
+    timestamp: '2024-05-20T16:35:28.802817Z'
   - torchscript_onnx_tflite:
-      inference_time: 1635.0
-      throughput: 611.6207951070336
+      inference_time: 1626.0
+      throughput: 615.0061500615006
       estimated_peak_memory_range:
-        min: 28672
-        max: 2553520
+        min: 24576
+        max: 2679392
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 245
-      job_id: j1gl6qmmg
+      job_id: jegnevvkg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1694.0
-      throughput: 590.318772136954
+      inference_time: 1668.0
+      throughput: 599.5203836930456
       estimated_peak_memory_range:
-        min: 622592
-        max: 68146216
+        min: 16384
+        max: 14848360
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 243
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 243
-      job_id: j1pv0nkr5
+      job_id: j2p0r009p
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.527250Z'
+    timestamp: '2024-05-20T16:35:28.802834Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1835.0
+      throughput: 544.9591280653951
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 243
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 243
+      job_id: jqpyd338p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1670.0
+      throughput: 598.8023952095808
+      estimated_peak_memory_range:
+        min: 34729984
+        max: 34729984
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 245
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 245
+      job_id: jn5q2qqn5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 10374.0
+      throughput: 96.3948332369385
+      estimated_peak_memory_range:
+        min: 36884480
+        max: 36884480
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j1glkmmjp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.802858Z'
```

## qai_hub_models/models/esrgan/export.py

```diff
@@ -115,20 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -158,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -187,24 +193,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/esrgan/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ESRGAN
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 65051.0
-      throughput: 15.372553842369832
+      inference_time: 68602.0
+      throughput: 14.576834494621147
       estimated_peak_memory_range:
-        min: 3252224
-        max: 6824744
+        min: 4915200
+        max: 8401176
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1024
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1024
-      job_id: j1p804dog
+      job_id: jw561446p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 65381.0
-      throughput: 15.294963368562732
+      inference_time: 67537.0
+      throughput: 14.806698550424212
       estimated_peak_memory_range:
-        min: 102400
-        max: 104823816
+        min: 122880
+        max: 105180416
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1026
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1026
-      job_id: jn5qemxo5
+      job_id: j1pvwk6kg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 70770.0
-      throughput: 14.130281192595733
+      inference_time: 70574.0
+      throughput: 14.169524187377787
       estimated_peak_memory_range:
-        min: 3174400
-        max: 141778696
+        min: 6324224
+        max: 153237392
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 1028
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jw56edxyg
+        total_layers: 1028
+      job_id: jz5w96e3p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.551433Z'
+    timestamp: '2024-05-20T16:35:28.833041Z'
   - torchscript_onnx_tflite:
-      inference_time: 51233.0
-      throughput: 19.518669607479556
+      inference_time: 51332.0
+      throughput: 19.48102548118133
       estimated_peak_memory_range:
-        min: 94208
-        max: 579142256
+        min: 3239936
+        max: 585991072
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1024
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1024
-      job_id: jogk79wnp
+      job_id: j1p3m003g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 50830.0
-      throughput: 19.673421207948063
+      inference_time: 50345.0
+      throughput: 19.86294567484358
       estimated_peak_memory_range:
-        min: 102400
-        max: 255173680
+        min: 12288
+        max: 260077888
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1026
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1026
-      job_id: j1gl61dmg
+      job_id: j7gjlnvvp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 51607.0
-      throughput: 19.37721626911078
+      inference_time: 51390.0
+      throughput: 19.45903872348706
       estimated_peak_memory_range:
-        min: 6688768
-        max: 197563712
+        min: 6324224
+        max: 192683632
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 1028
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1p3vwdng
+        total_layers: 1028
+      job_id: jmg94nlw5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.551673Z'
+    timestamp: '2024-05-20T16:35:28.833066Z'
   - torchscript_onnx_tflite:
-      inference_time: 71702.0
-      throughput: 13.946612367855847
+      inference_time: 71946.0
+      throughput: 13.899313373919329
       estimated_peak_memory_range:
-        min: 3293184
-        max: 6629192
+        min: 0
+        max: 3606600
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1024
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1024
-      job_id: jmg9jqn85
+      job_id: jwgov66q5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 68263.0
-      throughput: 14.649224323572067
+      inference_time: 70208.0
+      throughput: 14.243391066545122
       estimated_peak_memory_range:
-        min: 118784
-        max: 62391352
+        min: 196608
+        max: 104068704
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1026
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1026
-      job_id: jqp4k2r1g
+      job_id: jygz7d3op
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.551903Z'
+    timestamp: '2024-05-20T16:35:28.833084Z'
+  - torchscript_onnx_qnn:
+      inference_time: 73168.0
+      throughput: 13.667176907937897
+      estimated_peak_memory_range:
+        min: 204800
+        max: 204800
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1026
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1026
+      job_id: jlpevmdo5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 65764.0
+      throughput: 15.205887719725078
+      estimated_peak_memory_range:
+        min: 1138688
+        max: 1138688
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1028
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1028
+      job_id: jnp18z48g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 641039.0
+      throughput: 1.5599674902775027
+      estimated_peak_memory_range:
+        min: 554172416
+        max: 554172416
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jvgdv1xrg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.833106Z'
```

## qai_hub_models/models/facebook_denoiser/demo.py

```diff
@@ -1,13 +1,12 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 import os
-import tempfile
 from pathlib import Path
 from typing import List
 
 import torchaudio
 
 from qai_hub_models.models.facebook_denoiser.app import FacebookDenoiserApp
 from qai_hub_models.models.facebook_denoiser.model import (
@@ -19,15 +18,19 @@
 )
 from qai_hub_models.utils.args import (
     demo_model_from_cli_args,
     get_model_cli_parser,
     get_on_device_demo_parser,
     validate_on_device_demo_args,
 )
-from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, load_path
+from qai_hub_models.utils.asset_loaders import (
+    CachedWebModelAsset,
+    load_path,
+    qaihm_temp_dir,
+)
 
 EXAMPLE_RECORDING = CachedWebModelAsset.from_asset_store(
     MODEL_ID, ASSET_VERSION, "icsi_meeting_recording.wav"
 )
 
 
 def main(is_test: bool = False):
@@ -53,15 +56,15 @@
     validate_on_device_demo_args(args, MODEL_ID)
 
     app = FacebookDenoiserApp(model, args.sample_rate)
 
     # Download data
     audio_files: List[str] = args.audio
     audio_tensors = []
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         for idx, file in enumerate(audio_files):
             audio_file = load_path(file, tmpdir)
             audio, sample_rate = torchaudio.load(audio_file)
             # By default, cut audio to the default sequence length
             # since by default, model is compiled with this input size
             audio_tensor = audio[0, :DEFAULT_SEQUENCE_LENGTH].unsqueeze(0).unsqueeze(0)
             assert sample_rate == SAMPLE_RATE
```

## qai_hub_models/models/facebook_denoiser/export.py

```diff
@@ -116,15 +116,15 @@
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options
+        target_runtime, compile_options, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -188,14 +188,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/facebook_denoiser/perf.yaml

```diff
@@ -18,117 +18,157 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Facebook-Denoiser
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 683713.0
-      throughput: 1.4626019982068499
+      inference_time: 727870.0
+      throughput: 1.37387170785992
       estimated_peak_memory_range:
-        min: 380928
-        max: 375423608
+        min: 45551616
+        max: 416715824
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 209
         total_layers: 209
-      job_id: j1pv098r5
+      job_id: jz57dryv5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 14433398.0
-      throughput: 0.0692837542483066
+      inference_time: 14547237.0
+      throughput: 0.06874157614947773
       estimated_peak_memory_range:
-        min: 1519616
-        max: 86092704
+        min: 143360
+        max: 92274744
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 4
+        layers_on_npu: 175
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 7
-      job_id: jlpeelqvp
+        total_layers: 178
+      job_id: jo5mzxndp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.576051Z'
+    timestamp: '2024-05-20T16:35:28.863583Z'
   - torchscript_onnx_tflite:
-      inference_time: 677141.0
-      throughput: 1.476797299233099
+      inference_time: 779484.0
+      throughput: 1.2828999697235608
       estimated_peak_memory_range:
-        min: 363802624
-        max: 387318224
+        min: 430981120
+        max: 452244496
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 209
         total_layers: 209
-      job_id: j7gjzw9e5
+      job_id: jqp4wrl8g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 10716749.0
-      throughput: 0.09331188030997087
+      inference_time: 10691874.0
+      throughput: 0.09352897349893947
       estimated_peak_memory_range:
-        min: 19521536
-        max: 273877616
+        min: 17801216
+        max: 224185136
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 4
+        layers_on_npu: 175
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 7
-      job_id: jygzo46x5
+        total_layers: 178
+      job_id: jegnev6kg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.576099Z'
+    timestamp: '2024-05-20T16:35:28.863604Z'
   - torchscript_onnx_tflite:
-      inference_time: 704020.0
-      throughput: 1.4204141927786142
+      inference_time: 727753.0
+      throughput: 1.3740925836100986
       estimated_peak_memory_range:
-        min: 321875968
-        max: 538203832
+        min: 235909120
+        max: 447833184
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 209
         total_layers: 209
-      job_id: j1p80kyog
+      job_id: j0px1ok3g
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.576158Z'
+    timestamp: '2024-05-20T16:35:28.863616Z'
+  - torchscript_onnx_ort:
+      inference_time: 15602048.0
+      throughput: 0.06409414969111747
+      estimated_peak_memory_range:
+        min: 450560
+        max: 450560
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 175
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 178
+      job_id: jopry3v0g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 138131.0
+      throughput: 7.239504528310082
+      estimated_peak_memory_range:
+        min: 139943936
+        max: 139943936
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 107
+        total_layers: 107
+      job_id: jep2mykr5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.863636Z'
```

## qai_hub_models/models/fastsam_s/export.py

```diff
@@ -118,20 +118,25 @@
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(
         model.to("cpu"), make_torch_inputs(input_spec), check_trace=False
     )
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        + " --force_channel_last_output output_1,output_2,output_3,output_5"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_1,output_2,output_3,output_5",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -161,16 +166,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -190,27 +197,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_1,output_2,output_3,output_5", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_1,output_2,output_3,output_5", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/fastsam_s/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: FastSam-S
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 8729.0
-      throughput: 114.56065986940085
+      inference_time: 8636.0
+      throughput: 115.7943492357573
       estimated_peak_memory_range:
-        min: 7823360
-        max: 10576056
+        min: 8404992
+        max: 26145480
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 288
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 288
-      job_id: jmg9jxr85
+      job_id: jqpyd318p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 8361.0
+      throughput: 119.60291831120679
+      estimated_peak_memory_range:
+        min: 4947968
+        max: 19891312
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 286
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 286
+      job_id: jogkyxewp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 10386.0
-      throughput: 96.28345850182939
+      inference_time: 10837.0
+      throughput: 92.27646027498385
       estimated_peak_memory_range:
-        min: 20791296
-        max: 84541352
+        min: 21467136
+        max: 77311024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 289
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jvgdezkz5
+        total_layers: 289
+      job_id: j1p3m0j3g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.594002Z'
+    timestamp: '2024-05-20T16:35:28.890936Z'
   - torchscript_onnx_tflite:
-      inference_time: 6438.0
-      throughput: 155.32774153463808
+      inference_time: 6531.0
+      throughput: 153.11590874291838
       estimated_peak_memory_range:
-        min: 6541312
-        max: 77737344
+        min: 5767168
+        max: 76610048
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 288
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 288
-      job_id: jnp1yv97p
+      job_id: j2p0r0z9p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 6171.0
+      throughput: 162.04829039053638
+      estimated_peak_memory_range:
+        min: 4952064
+        max: 91897808
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 286
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 286
+      job_id: jn5q2q6n5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 7468.0
-      throughput: 133.9046598821639
+      inference_time: 7948.0
+      throughput: 125.81781580271766
       estimated_peak_memory_range:
-        min: 24322048
-        max: 63913008
+        min: 28004352
+        max: 71806784
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 289
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jz5707m9g
+        total_layers: 289
+      job_id: jwgov62q5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.594052Z'
+    timestamp: '2024-05-20T16:35:28.890962Z'
   - torchscript_onnx_tflite:
-      inference_time: 8739.0
-      throughput: 114.42956860052638
+      inference_time: 8645.0
+      throughput: 115.6737998843262
       estimated_peak_memory_range:
-        min: 7802880
-        max: 25345168
+        min: 7819264
+        max: 25353920
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 288
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 288
-      job_id: jw56e0yyg
+      job_id: j1p87yqk5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 8210.0
+      throughput: 121.8026796589525
+      estimated_peak_memory_range:
+        min: 4984832
+        max: 19259848
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 286
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 286
+      job_id: jw5614y6p
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.594089Z'
+    timestamp: '2024-05-20T16:35:28.890979Z'
+  - torchscript_onnx_qnn:
+      inference_time: 9182.0
+      throughput: 108.90873448050533
+      estimated_peak_memory_range:
+        min: 4935680
+        max: 4935680
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 286
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 286
+      job_id: j1glkmvjp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 10779.0
+      throughput: 92.77298450691158
+      estimated_peak_memory_range:
+        min: 67710976
+        max: 67710976
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 289
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 289
+      job_id: j1pvwkqkg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 62697.0
+      throughput: 15.949726462191174
+      estimated_peak_memory_range:
+        min: 70156288
+        max: 70156288
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 201
+        total_layers: 201
+      job_id: j7gjlndvp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.891004Z'
```

## qai_hub_models/models/fastsam_x/export.py

```diff
@@ -118,20 +118,25 @@
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(
         model.to("cpu"), make_torch_inputs(input_spec), check_trace=False
     )
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        + " --force_channel_last_output output_1,output_2,output_3,output_5"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_1,output_2,output_3,output_5",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -161,16 +166,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -190,27 +197,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_1,output_2,output_3,output_5", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_1,output_2,output_3,output_5", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/fastsam_x/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: FastSam-X
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 50012.0
-      throughput: 19.995201151723585
+      inference_time: 49665.0
+      throughput: 20.13490385583409
       estimated_peak_memory_range:
-        min: 9154560
-        max: 13813200
+        min: 9117696
+        max: 14327728
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 420
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 420
-      job_id: j0pxndql5
+      job_id: jlpevmoo5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 46166.0
+      throughput: 21.66096261317853
+      estimated_peak_memory_range:
+        min: 4935680
+        max: 20646312
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 418
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 418
+      job_id: jmg94n0w5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 50171.0
-      throughput: 19.93183313069303
+      inference_time: 50328.0
+      throughput: 19.86965506278811
       estimated_peak_memory_range:
-        min: 24637440
-        max: 351124872
+        min: 25731072
+        max: 346581656
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 421
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jegnl74q5
+        total_layers: 421
+      job_id: jmg94n085
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.611861Z'
+    timestamp: '2024-05-20T16:35:28.921155Z'
   - torchscript_onnx_tflite:
-      inference_time: 36802.0
-      throughput: 27.172436280636923
+      inference_time: 36007.0
+      throughput: 27.772377593245757
       estimated_peak_memory_range:
-        min: 8462336
-        max: 149995872
+        min: 73728
+        max: 135466464
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 420
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 420
-      job_id: jo5mqd79p
+      job_id: jygz7d2op
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 34949.0
+      throughput: 28.61312197773899
+      estimated_peak_memory_range:
+        min: 4096000
+        max: 127015584
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 418
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 418
+      job_id: jnp18z28g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 36880.0
-      throughput: 27.114967462039047
+      inference_time: 36890.0
+      throughput: 27.107617240444565
       estimated_peak_memory_range:
-        min: 26107904
-        max: 93739104
+        min: 29392896
+        max: 93988544
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 421
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jopr8nr75
+        total_layers: 421
+      job_id: jnp18z27g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.611951Z'
+    timestamp: '2024-05-20T16:35:28.921181Z'
   - torchscript_onnx_tflite:
-      inference_time: 52081.0
-      throughput: 19.200860198536894
+      inference_time: 50541.0
+      throughput: 19.785916384717357
       estimated_peak_memory_range:
-        min: 9240576
-        max: 13789008
+        min: 9220096
+        max: 14009928
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 420
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 420
-      job_id: jmg9jql85
+      job_id: jz5w96w3p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 45832.0
+      throughput: 21.81881654739047
+      estimated_peak_memory_range:
+        min: 4988928
+        max: 21102120
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 418
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 418
+      job_id: jz5w96wmp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.612007Z'
+    timestamp: '2024-05-20T16:35:28.921198Z'
+  - torchscript_onnx_qnn:
+      inference_time: 57556.0
+      throughput: 17.374383209396065
+      estimated_peak_memory_range:
+        min: 4939776
+        max: 4939776
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 418
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 418
+      job_id: jvgdv1nrg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 49642.0
+      throughput: 20.144232706176222
+      estimated_peak_memory_range:
+        min: 36737024
+        max: 36737024
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 421
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 421
+      job_id: jvgdv1nzg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 2190810.0
+      throughput: 0.45645217978738456
+      estimated_peak_memory_range:
+        min: 582156288
+        max: 582156288
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jz57dr295
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.921219Z'
```

## qai_hub_models/models/fcn_resnet50/app.py

```diff
@@ -11,36 +11,29 @@
 import PIL
 import torch
 from PIL.Image import Image
 from torchvision import transforms
 
 from qai_hub_models.models.fcn_resnet50.model import NUM_CLASSES
 from qai_hub_models.utils.draw import create_color_map
-from qai_hub_models.utils.image_processing import normalize_image_transform
 
 
 def preprocess_image(image: Image) -> torch.Tensor:
     """
     Preprocesses images to be run through torch FCN segmenter
     as prescribed here:
     https://pytorch.org/hub/pytorch_vision_resnet/
 
     Parameters:
         image: Input image to be run through the classifier model.
 
     Returns:
         torch tensor to be directly passed to the model.
     """
-    transform = transforms.Compose(
-        [
-            transforms.ToTensor(),
-            normalize_image_transform(),
-        ]
-    )
-    out_tensor: torch.Tensor = transform(image)  # type: ignore
+    out_tensor: torch.Tensor = transforms.ToTensor()(image)  # type: ignore
     return out_tensor.unsqueeze(0)
 
 
 class FCN_ResNet50App:
     """
     This class consists of light-weight "app code" that is required to
     perform end to end inference with FCN_ResNet50.
```

## qai_hub_models/models/fcn_resnet50/demo.py

```diff
@@ -1,11 +1,13 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
+from typing import Type
+
 from qai_hub_models.models.fcn_resnet50.app import FCN_ResNet50App
 from qai_hub_models.models.fcn_resnet50.model import (
     MODEL_ASSET_VERSION,
     MODEL_ID,
     FCN_ResNet50,
 )
 from qai_hub_models.utils.args import (
@@ -22,41 +24,45 @@
 # and has had alpha channel removed for use as input
 INPUT_IMAGE_LOCAL_PATH = "fcn_demo.png"
 INPUT_IMAGE_ADDRESS = CachedWebModelAsset.from_asset_store(
     MODEL_ID, MODEL_ASSET_VERSION, INPUT_IMAGE_LOCAL_PATH
 )
 
 
-def main(is_test: bool = False):
+def fcn_resnet50_demo(model_cls: Type[FCN_ResNet50], is_test: bool = False):
     # Demo parameters
-    parser = get_model_cli_parser(FCN_ResNet50)
+    parser = get_model_cli_parser(model_cls)
     parser = get_on_device_demo_parser(parser, add_output_dir=True)
     parser.add_argument(
         "--image",
         type=str,
         default=INPUT_IMAGE_ADDRESS,
         help="image file path or URL.",
     )
 
     args = parser.parse_args([] if is_test else None)
     validate_on_device_demo_args(args, MODEL_ID)
-    model = demo_model_from_cli_args(FCN_ResNet50, MODEL_ID, args)
+    model = demo_model_from_cli_args(model_cls, MODEL_ID, args)
 
     # This FCN ResNet 50 demo comes from
     # https://pytorch.org/hub/pytorch_vision_fcn_resnet101/
     # load image
-    (_, _, height, width) = FCN_ResNet50.get_input_spec()["image"][0]
+    (_, _, height, width) = model_cls.get_input_spec()["image"][0]
     orig_image = load_image(args.image)
     image, scale, padding = pil_resize_pad(orig_image, (height, width))
     input_image = image.convert("RGB")
 
     app = FCN_ResNet50App(model)
     output = app.predict(input_image, False)
 
     if not is_test:
         # Resize / unpad annotated image
         image_annotated = pil_undo_resize_pad(output, orig_image.size, scale, padding)
         display_or_save_image(image_annotated, args.output_dir, "fcn_demo_output.png")
 
 
+def main(is_test: bool = False):
+    return fcn_resnet50_demo(FCN_ResNet50, is_test=is_test)
+
+
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/fcn_resnet50/export.py

```diff
@@ -94,15 +94,15 @@
     if chipset:
         hub_device = hub.Device(attributes=f"chipset:{chipset}")
     else:
         hub_device = hub.Device(name=device)
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "fcn_resnet50",
-            "FCN_ResNet50",
+            "FCN-ResNet50",
             device,
             skip_profiling,
             skip_inferencing,
             skip_downloading,
             skip_summary,
             output_path,
             target_runtime,
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/fcn_resnet50/info.yaml

```diff
@@ -1,8 +1,8 @@
-name: FCN_ResNet50
+name: FCN-ResNet50
 # id must match with the model dir name in qai_hub_models
 id: fcn_resnet50
 status: public
 headline: Fully-convolutional network model for image segmentation.
 domain: Computer Vision
 use_case: Semantic Segmentation
 description: FCN_ResNet50 is a machine learning model that can segment images from
@@ -20,19 +20,20 @@
   Number of parameters: 32.9M
   Model size: 126 MB
 applicable_scenarios:
   - Anomaly Detection
   - Inventory Management
 related_models:
   - sam
-  - unet_segmentation
+  - deeplabv3_plus_mobilenet
   - ddrnet23_slim
 form_factors:
   - Phone
   - Tablet
   - IoT
   - XR
 has_static_banner: yes
 has_animated_banner: no
 license_type: bsd-3-clause
 deploy_license_type: AI Model Hub License
-dataset: []
+dataset:
+  - coco
```

## qai_hub_models/models/fcn_resnet50/model.py

```diff
@@ -3,15 +3,18 @@
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import torch
 import torchvision.models as tv_models
 
+from qai_hub_models.evaluators.base_evaluators import BaseEvaluator
+from qai_hub_models.evaluators.segmentation_evaluator import SegmentationOutputEvaluator
 from qai_hub_models.utils.base_model import BaseModel
+from qai_hub_models.utils.image_processing import normalize_image_torchvision
 from qai_hub_models.utils.input_spec import InputSpec
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 1
 DEFAULT_WEIGHTS = "COCO_WITH_VOC_LABELS_V1"
 NUM_CLASSES = 21
 
@@ -25,35 +28,39 @@
     ) -> None:
         super().__init__()
         self.model = fcn_model
 
     @classmethod
     def from_pretrained(cls, weights: str = DEFAULT_WEIGHTS) -> FCN_ResNet50:
         model = tv_models.segmentation.fcn_resnet50(weights=weights).eval()
+        model.aux_classifier = None
         return cls(model)
 
-    def forward(self, image: torch.Tensor) -> torch.Tensor:
+    def get_evaluator(self) -> BaseEvaluator:
+        return SegmentationOutputEvaluator(NUM_CLASSES)
+
+    def forward(self, image):
         """
         Run FCN_ResNet50 on `image`, and produce a tensor of classes for segmentation
 
         Parameters:
             image: Pixel values pre-processed for model consumption.
                    Range: float[0, 1]
                    3-channel Color Space: RGB
 
         Returns:
             tensor: 1x21xHxW tensor of class logits per pixel
         """
-        return self.model(image)["out"]
+        return self.model(normalize_image_torchvision(image))["out"]
 
     @staticmethod
     def get_input_spec(
         batch_size: int = 1,
         num_channels: int = 3,
-        height: int = 224,
-        width: int = 224,
+        height: int = 512,
+        width: int = 512,
     ) -> InputSpec:
         # Get the input specification ordered (name -> (shape, type)) pairs for this model.
         #
         # This can be used with the qai_hub python API to declare
         # the model input specification upon submitting a profile job.
         return {"image": ((batch_size, num_channels, height, width), "float32")}
```

## qai_hub_models/models/fcn_resnet50/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
-- name: FCN_ResNet50
+- name: FCN-ResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 8481.0
-      throughput: 117.91062374719962
+      inference_time: 42451.0
+      throughput: 23.55657110550988
       estimated_peak_memory_range:
-        min: 4251648
-        max: 6673424
+        min: 22093824
+        max: 24844120
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 84
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 84
-      job_id: jqpyr7ll5
+        total_layers: 86
+      job_id: jqp4wrn1g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 7915.0
-      throughput: 126.34238787113077
+      inference_time: 42160.0
+      throughput: 23.719165085388994
       estimated_peak_memory_range:
-        min: 32768
-        max: 14371224
+        min: 3166208
+        max: 20971816
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 125
+        layers_on_npu: 127
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 125
-      job_id: j1p804nog
+        total_layers: 127
+      job_id: jegnev0qg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 434382.0
-      throughput: 2.3021211744501384
+      inference_time: 42833.0
+      throughput: 23.346485186655148
       estimated_peak_memory_range:
-        min: 229376
-        max: 157385104
+        min: 46034944
+        max: 200591552
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 129
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jn5qemno5
+        total_layers: 129
+      job_id: j2p0r04np
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.629765Z'
+    timestamp: '2024-05-20T16:35:28.951494Z'
   - torchscript_onnx_tflite:
-      inference_time: 6385.0
-      throughput: 156.61707126076743
+      inference_time: 30899.0
+      throughput: 32.363506909608724
       estimated_peak_memory_range:
-        min: 4259840
-        max: 81999104
+        min: 20209664
+        max: 155788144
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 84
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 84
-      job_id: j2p03vwnp
+        total_layers: 86
+      job_id: j0px1o9lg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 5804.0
-      throughput: 172.2949689869056
+      inference_time: 31911.0
+      throughput: 31.337156466422236
       estimated_peak_memory_range:
-        min: 618496
-        max: 57524672
+        min: 2564096
+        max: 76317072
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 125
+        layers_on_npu: 127
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 125
-      job_id: jogk791np
+        total_layers: 127
+      job_id: jopry367g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 334126.0
-      throughput: 2.9928829244057633
+      inference_time: 32386.0
+      throughput: 30.877539677638485
       estimated_peak_memory_range:
-        min: 3608576
-        max: 48710400
+        min: 43917312
+        max: 112401296
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 129
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1gl61jmg
+        total_layers: 129
+      job_id: j1p87y2o5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.629842Z'
+    timestamp: '2024-05-20T16:35:28.951519Z'
   - torchscript_onnx_tflite:
-      inference_time: 8533.0
-      throughput: 117.19207781553968
+      inference_time: 42178.0
+      throughput: 23.709042628858647
       estimated_peak_memory_range:
-        min: 4243456
-        max: 6395552
+        min: 18853888
+        max: 20525048
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 84
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 84
-      job_id: jvgdemx65
+        total_layers: 86
+      job_id: jo5mzxe9p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 7887.0
-      throughput: 126.79092177000126
+      inference_time: 42067.0
+      throughput: 23.77160244372073
       estimated_peak_memory_range:
-        min: 16384
-        max: 14326120
+        min: 3178496
+        max: 20597416
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 125
+        layers_on_npu: 127
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 125
-      job_id: jo5mqln7p
+        total_layers: 127
+      job_id: jqpyd3zlp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.629884Z'
+    timestamp: '2024-05-20T16:35:28.951537Z'
+  - torchscript_onnx_qnn:
+      inference_time: 68578.0
+      throughput: 14.581935897809792
+      estimated_peak_memory_range:
+        min: 3153920
+        max: 3153920
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 127
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 127
+      job_id: jep2myxq5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 42426.0
+      throughput: 23.57045208127092
+      estimated_peak_memory_range:
+        min: 40243200
+        max: 40243200
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 129
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 129
+      job_id: jogkyxvnp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 340971.0
+      throughput: 2.932800736719545
+      estimated_peak_memory_range:
+        min: 278179840
+        max: 278179840
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 59
+        total_layers: 59
+      job_id: jn5q2q0o5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:28.951559Z'
```

## qai_hub_models/models/ffnet_122ns_lowres/export.py

```diff
@@ -115,20 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -158,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -187,24 +193,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_122ns_lowres/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: FFNet-122NS-LowRes
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 9669.0
-      throughput: 103.42331161443789
+      inference_time: 9717.0
+      throughput: 102.91242152927859
       estimated_peak_memory_range:
-        min: 675840
-        max: 2991672
+        min: 651264
+        max: 3155872
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 216
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 216
-      job_id: j1p3vwyng
+      job_id: jvgdv1ezg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 10768.0
-      throughput: 92.86775631500743
+      inference_time: 10869.0
+      throughput: 92.00478424878094
       estimated_peak_memory_range:
-        min: 6320128
-        max: 41702576
+        min: 8364032
+        max: 43265120
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 348
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 348
-      job_id: j1pv09jr5
+      job_id: j0px1onlg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 7374.0
-      throughput: 135.61160835367508
+      inference_time: 7858.0
+      throughput: 127.25884448969204
       estimated_peak_memory_range:
-        min: 1433600
-        max: 142206056
+        min: 2232320
+        max: 141084128
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 350
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jlpeeljvp
+        total_layers: 350
+      job_id: jep2my0q5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.654178Z'
+    timestamp: '2024-05-20T16:35:29.021584Z'
   - torchscript_onnx_tflite:
-      inference_time: 6839.0
-      throughput: 146.22020763269484
+      inference_time: 6794.0
+      throughput: 147.18869590815424
       estimated_peak_memory_range:
-        min: 569344
-        max: 59671696
+        min: 303104
+        max: 60447344
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 216
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 216
-      job_id: jwgok4jkp
+      job_id: jz57dr095
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 7605.0
-      throughput: 131.49243918474687
+      inference_time: 7585.0
+      throughput: 131.83915622940015
       estimated_peak_memory_range:
         min: 6307840
-        max: 88354272
+        max: 88988128
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 348
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 348
-      job_id: j7gjzwje5
+      job_id: jo5mzxq9p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 5809.0
-      throughput: 172.14666896195558
+      inference_time: 5761.0
+      throughput: 173.58097552508247
       estimated_peak_memory_range:
-        min: 61464576
-        max: 106276496
+        min: 5238784
+        max: 60652944
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 350
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jygzo41x5
+        total_layers: 350
+      job_id: jqpyd3rlp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.654276Z'
+    timestamp: '2024-05-20T16:35:29.021611Z'
   - torchscript_onnx_tflite:
-      inference_time: 9658.0
-      throughput: 103.54110581901014
+      inference_time: 9668.0
+      throughput: 103.4340091021928
       estimated_peak_memory_range:
-        min: 0
-        max: 4034800
+        min: 651264
+        max: 2883976
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 216
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 216
-      job_id: jqpyry105
+      job_id: jqp4wrk1g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 10822.0
-      throughput: 92.40436148586214
+      inference_time: 10900.0
+      throughput: 91.74311926605505
       estimated_peak_memory_range:
-        min: 6328320
-        max: 38539008
+        min: 6332416
+        max: 40664968
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 348
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 348
-      job_id: jn5qed0e5
+      job_id: jopry387g
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.654354Z'
+    timestamp: '2024-05-20T16:35:29.021627Z'
+  - torchscript_onnx_qnn:
+      inference_time: 17551.0
+      throughput: 56.976810438151674
+      estimated_peak_memory_range:
+        min: 6303744
+        max: 6303744
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 348
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 348
+      job_id: jegnevlqg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 7536.0
+      throughput: 132.6963906581741
+      estimated_peak_memory_range:
+        min: 6365184
+        max: 6365184
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 350
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 350
+      job_id: j2p0r03np
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 38423.0
+      throughput: 26.026078130286546
+      estimated_peak_memory_range:
+        min: 6307840
+        max: 6307840
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 151
+        total_layers: 151
+      job_id: j1p87y0o5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.021650Z'
```

## qai_hub_models/models/ffnet_40s/export.py

```diff
@@ -115,20 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -158,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -187,24 +193,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_40s/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: FFNet-40S
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 23048.0
-      throughput: 43.38771259979174
+      inference_time: 23181.0
+      throughput: 43.138777447047154
       estimated_peak_memory_range:
-        min: 0
-        max: 30911488
+        min: 2527232
+        max: 5196976
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 92
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 92
-      job_id: jmg9jx685
+      job_id: jogkyx7np
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 17363.0
-      throughput: 57.59373380176237
+      inference_time: 17245.0
+      throughput: 57.98782255726297
       estimated_peak_memory_range:
-        min: 25214976
-        max: 44166488
+        min: 1662976
+        max: 17190312
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 140
-      job_id: jvgdezjz5
+      job_id: jw5614zyp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 28590.0
-      throughput: 34.97726477789437
+      inference_time: 27135.0
+      throughput: 36.852773171181134
       estimated_peak_memory_range:
-        min: 30191616
-        max: 118917360
+        min: 33619968
+        max: 118794368
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 142
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jmg9jx6m5
+        total_layers: 142
+      job_id: j7gjln2ep
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:31.678643Z'
+    timestamp: '2024-05-20T16:35:29.051849Z'
   - torchscript_onnx_tflite:
-      inference_time: 16867.0
-      throughput: 59.28736586233474
+      inference_time: 16628.0
+      throughput: 60.13952369497233
       estimated_peak_memory_range:
-        min: 32768
-        max: 105460576
+        min: 65536
+        max: 96903808
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 92
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 92
-      job_id: jnp1yvr7p
+      job_id: jn5q2qeo5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 12552.0
-      throughput: 79.66857871255577
+      inference_time: 12571.0
+      throughput: 79.54816641476414
       estimated_peak_memory_range:
-        min: 25202688
-        max: 84533840
+        min: 25198592
+        max: 80803488
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 140
-      job_id: jz5w21j45
+      job_id: j1p3m01ng
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 20354.0
-      throughput: 49.13039206052864
+      inference_time: 19730.0
+      throughput: 50.68423720223011
       estimated_peak_memory_range:
-        min: 352256
-        max: 45279760
+        min: 32903168
+        max: 79929760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 142
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jnp1yvrnp
+        total_layers: 142
+      job_id: jlpevmwv5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:31.678725Z'
+    timestamp: '2024-05-20T16:35:29.051876Z'
   - torchscript_onnx_tflite:
-      inference_time: 22456.0
-      throughput: 44.53152832205201
+      inference_time: 23514.0
+      throughput: 42.527855745513314
       estimated_peak_memory_range:
-        min: 32768
-        max: 1647568
+        min: 2555904
+        max: 4820560
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 92
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 92
-      job_id: jygzo0245
+      job_id: j1glkm2mp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 17241.0
-      throughput: 58.00127602807262
+      inference_time: 17349.0
+      throughput: 57.64020981036371
       estimated_peak_memory_range:
-        min: 25214976
-        max: 52246888
+        min: 25227264
+        max: 46301352
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 140
-      job_id: jvgdemn65
+      job_id: j1pvwkrrg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:31.678770Z'
+    timestamp: '2024-05-20T16:35:29.051893Z'
+  - torchscript_onnx_qnn:
+      inference_time: 23285.0
+      throughput: 42.94610264118531
+      estimated_peak_memory_range:
+        min: 25214976
+        max: 25214976
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 140
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 140
+      job_id: jwgov6nk5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 26395.0
+      throughput: 37.885963250615646
+      estimated_peak_memory_range:
+        min: 25223168
+        max: 25223168
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 142
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 142
+      job_id: jygz7djxp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 143723.0
+      throughput: 6.957828600850247
+      estimated_peak_memory_range:
+        min: 208834560
+        max: 208834560
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 67
+        total_layers: 67
+      job_id: jz5w963mp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.051916Z'
```

## qai_hub_models/models/ffnet_40s_quantized/export.py

```diff
@@ -119,20 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -166,16 +170,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -195,24 +201,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_40s_quantized/perf.yaml

```diff
@@ -22,180 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: FFNet-40S-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 6424.0
-      throughput: 155.6662515566625
+      inference_time: 6448.0
+      throughput: 155.08684863523573
       estimated_peak_memory_range:
-        min: 651264
-        max: 25140680
+        min: 823296
+        max: 2440760
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 97
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 97
-      job_id: jz5707qng
+      job_id: jmg94ny85
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 4328.0
+      throughput: 231.0536044362292
+      estimated_peak_memory_range:
+        min: 6303744
+        max: 20703816
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 89
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 89
+      job_id: jz57drl95
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 50173.0
-      throughput: 19.93103860642178
+      inference_time: 11529.0
+      throughput: 86.73779165582444
       estimated_peak_memory_range:
-        min: 29384704
-        max: 58656168
+        min: 25239552
+        max: 52880320
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 94
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j0pxndw85
+        total_layers: 94
+      job_id: jegnevmqg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.012713Z'
+    timestamp: '2024-05-20T16:35:29.082174Z'
   - torchscript_onnx_tflite:
       inference_time: 4623.0
       throughput: 216.3097555699762
       estimated_peak_memory_range:
-        min: 20480
-        max: 67550048
+        min: 36864
+        max: 67842848
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 97
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 97
-      job_id: jqp4k9z2g
+      job_id: jnp18zw7g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 3154.0
+      throughput: 317.0577045022194
+      estimated_peak_memory_range:
+        min: 6311936
+        max: 56225968
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 89
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 89
+      job_id: jqp4wrd1g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 31095.0
-      throughput: 32.15951117543013
+      inference_time: 8449.0
+      throughput: 118.35720203574388
       estimated_peak_memory_range:
-        min: 31465472
-        max: 65073664
+        min: 29212672
+        max: 64461296
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 94
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jo5mqdj7p
+        total_layers: 94
+      job_id: jopry327g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.012866Z'
+    timestamp: '2024-05-20T16:35:29.082201Z'
   - torchscript_onnx_tflite:
-      inference_time: 46106.0
-      throughput: 21.68915108662647
+      inference_time: 6431.0
+      throughput: 155.49681231534754
       estimated_peak_memory_range:
-        min: 12288
-        max: 52922016
+        min: 651264
+        max: 2546568
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 99
+        layers_on_npu: 97
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 99
-      job_id: jegnlw0j5
+        total_layers: 97
+      job_id: jvgdv1qzg
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 362244.0
-      throughput: 2.7605702233853426
+    torchscript_onnx_qnn:
+      inference_time: 4293.0
+      throughput: 232.93733985557884
       estimated_peak_memory_range:
-        min: 159432704
-        max: 207613904
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 6332416
+        max: 23257352
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 89
         layers_on_gpu: 0
-        layers_on_cpu: 92
-        total_layers: 92
-      job_id: jegnl7jj5
+        layers_on_cpu: 0
+        total_layers: 89
+      job_id: jo5mzx69p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
+      name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:32.013041Z'
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:29.082219Z'
   - torchscript_onnx_tflite:
-      inference_time: 206934.0
-      throughput: 4.832458658316178
+      inference_time: 35053.0
+      throughput: 28.528228682281117
+      estimated_peak_memory_range:
+        min: 147456
+        max: 42857344
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 97
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 97
+      job_id: j1gl3818g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 28024.0
+      throughput: 35.68369968598344
       estimated_peak_memory_range:
-        min: 2678784
-        max: 4932640
+        min: 6324224
+        max: 55642400
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 99
+        layers_on_npu: 89
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 99
-      job_id: jep29omxg
+        total_layers: 89
+      job_id: jlpekxl1p
       job_status: Passed
     reference_device_info:
-      name: RB5 (Proxy)
+      name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:32.013136Z'
+      chipset: Qcs6490
+    timestamp: '2024-05-20T16:35:29.082235Z'
   - torchscript_onnx_tflite:
-      inference_time: 8927.0
-      throughput: 112.0197154699227
+      inference_time: 186982.0
+      throughput: 5.348108374068092
       estimated_peak_memory_range:
-        min: 2711552
-        max: 19152008
+        min: 774144
+        max: 11267904
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 99
+        layers_on_npu: 97
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 99
-      job_id: jopr876k5
+        total_layers: 97
+      job_id: jw56nmd0g
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
+      name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.013229Z'
+      chipset: Qcs8250
+    timestamp: '2024-05-20T16:35:29.082246Z'
+  - torchscript_onnx_qnn:
+      inference_time: 5258.0
+      throughput: 190.1863826550019
+      estimated_peak_memory_range:
+        min: 6303744
+        max: 6303744
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 89
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 89
+      job_id: j0px1o6lg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 10886.0
+      throughput: 91.86110600771633
+      estimated_peak_memory_range:
+        min: 25223168
+        max: 25223168
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 94
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 94
+      job_id: jep2my9q5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 551323.0
+      throughput: 1.813818759601903
+      estimated_peak_memory_range:
+        min: 204230656
+        max: 204230656
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jqpyd3jlp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.082269Z'
```

## qai_hub_models/models/ffnet_54s/export.py

```diff
@@ -115,20 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -158,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -187,24 +193,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_54s/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: FFNet-54S
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 25024.0
-      throughput: 39.9616368286445
+      inference_time: 25556.0
+      throughput: 39.12975426514321
       estimated_peak_memory_range:
-        min: 2580480
-        max: 5287928
+        min: 2527232
+        max: 5075256
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 113
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 113
-      job_id: jopr8nzk5
+      job_id: j2p0r02np
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 19758.0
-      throughput: 50.61241016297196
+      inference_time: 20540.0
+      throughput: 48.685491723466406
       estimated_peak_memory_range:
-        min: 25214976
-        max: 48724312
+        min: 25178112
+        max: 46235888
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 175
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 175
-      job_id: jqpyr7905
+      job_id: jn5q2qro5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 30799.0
-      throughput: 32.46858664242345
+      inference_time: 30453.0
+      throughput: 32.837487275473684
       estimated_peak_memory_range:
-        min: 30203904
-        max: 103625272
+        min: 33370112
+        max: 130933960
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 177
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1p804oqg
+        total_layers: 177
+      job_id: jwgov63k5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.053359Z'
+    timestamp: '2024-05-20T16:35:29.121526Z'
   - torchscript_onnx_tflite:
-      inference_time: 18446.0
-      throughput: 54.21229534858506
+      inference_time: 18475.0
+      throughput: 54.12719891745602
       estimated_peak_memory_range:
-        min: 1429504
-        max: 120768592
+        min: 2248704
+        max: 109217248
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 113
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 113
-      job_id: jep20v26g
+      job_id: j1p87ymo5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 14552.0
-      throughput: 68.71907641561297
+      inference_time: 14482.0
+      throughput: 69.05123601712471
       estimated_peak_memory_range:
-        min: 180420608
-        max: 252953088
+        min: 24494080
+        max: 90410912
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 175
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 175
-      job_id: j2p03vy0p
+      job_id: j1glkm3mp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 23498.0
-      throughput: 42.556813345816664
+      inference_time: 23113.0
+      throughput: 43.265694630727296
       estimated_peak_memory_range:
-        min: 30953472
-        max: 85531952
+        min: 29417472
+        max: 74020448
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 177
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jogk79zvp
+        total_layers: 177
+      job_id: j1pvwkvrg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.053423Z'
+    timestamp: '2024-05-20T16:35:29.121553Z'
   - torchscript_onnx_tflite:
-      inference_time: 25045.0
-      throughput: 39.92812936713915
+      inference_time: 25895.0
+      throughput: 38.61749372465727
       estimated_peak_memory_range:
-        min: 2555904
-        max: 5156288
+        min: 2523136
+        max: 5051104
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 113
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 113
-      job_id: jn5qedee5
+      job_id: jogkyxqnp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 19986.0
-      throughput: 50.035024517162014
+      inference_time: 20155.0
+      throughput: 49.61548002976929
       estimated_peak_memory_range:
         min: 25214976
-        max: 55043864
+        max: 43633496
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 175
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 175
-      job_id: jwgok9k1p
+      job_id: j1p3m0eng
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.053476Z'
+    timestamp: '2024-05-20T16:35:29.121570Z'
+  - torchscript_onnx_qnn:
+      inference_time: 25810.0
+      throughput: 38.74467260751646
+      estimated_peak_memory_range:
+        min: 25219072
+        max: 25219072
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 175
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 175
+      job_id: jw5614nyp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 29548.0
+      throughput: 33.84323812102342
+      estimated_peak_memory_range:
+        min: 25219072
+        max: 25219072
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 177
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 177
+      job_id: j7gjlneep
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 176490.0
+      throughput: 5.666043401892458
+      estimated_peak_memory_range:
+        min: 414695424
+        max: 414695424
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 81
+        total_layers: 81
+      job_id: jlpevmkv5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.121592Z'
```

## qai_hub_models/models/ffnet_54s_quantized/export.py

```diff
@@ -119,20 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -166,16 +170,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -195,24 +201,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_54s_quantized/perf.yaml

```diff
@@ -22,180 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: FFNet-54S-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 7125.0
-      throughput: 140.35087719298247
+      inference_time: 7101.0
+      throughput: 140.8252358822701
       estimated_peak_memory_range:
-        min: 647168
-        max: 2562192
+        min: 692224
+        max: 2279272
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 118
-      job_id: j1gl61n2g
+      job_id: jygz7drxp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 4974.0
+      throughput: 201.04543626859672
+      estimated_peak_memory_range:
+        min: 6311936
+        max: 20048864
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 110
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 110
+      job_id: jnp18ze7g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 51385.0
-      throughput: 19.46093217865136
+      inference_time: 11814.0
+      throughput: 84.64533604198408
       estimated_peak_memory_range:
-        min: 29982720
-        max: 70964288
+        min: 30167040
+        max: 62607768
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 115
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1p3vwkmg
+        total_layers: 115
+      job_id: j0px1oylg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.077877Z'
+    timestamp: '2024-05-20T16:35:29.151840Z'
   - torchscript_onnx_tflite:
-      inference_time: 5099.0
-      throughput: 196.11688566385567
+      inference_time: 5164.0
+      throughput: 193.64833462432222
       estimated_peak_memory_range:
         min: 16384
-        max: 75082320
+        max: 74278720
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 118
-      job_id: jw56ed6ng
+      job_id: jz5w96qmp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 3622.0
+      throughput: 276.09055770292656
+      estimated_peak_memory_range:
+        min: 6307840
+        max: 63588464
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 110
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 110
+      job_id: jvgdv1ozg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 31008.0
-      throughput: 32.24974200206398
+      inference_time: 9025.0
+      throughput: 110.80332409972299
       estimated_peak_memory_range:
-        min: 15433728
-        max: 55696624
+        min: 675840
+        max: 35809952
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 115
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jwgok4y1p
+        total_layers: 115
+      job_id: jo5mzx39p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.077918Z'
+    timestamp: '2024-05-20T16:35:29.151867Z'
   - torchscript_onnx_tflite:
-      inference_time: 49684.0
-      throughput: 20.127203928830205
+      inference_time: 7134.0
+      throughput: 140.17381553125875
       estimated_peak_memory_range:
-        min: 126976
-        max: 56138256
+        min: 643072
+        max: 3436240
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 120
+        layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 120
-      job_id: jygzo0o45
+        total_layers: 118
+      job_id: jmg94nw85
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 420355.0
-      throughput: 2.3789416088782103
+    torchscript_onnx_qnn:
+      inference_time: 4965.0
+      throughput: 201.4098690835851
       estimated_peak_memory_range:
-        min: 187011072
-        max: 248380464
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 6307840
+        max: 20582560
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 110
         layers_on_gpu: 0
-        layers_on_cpu: 113
-        total_layers: 113
-      job_id: j1pv093z5
+        layers_on_cpu: 0
+        total_layers: 110
+      job_id: jqp4wrv1g
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
+      name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:32.077964Z'
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:29.151884Z'
   - torchscript_onnx_tflite:
-      inference_time: 216291.0
-      throughput: 4.623400881220208
+      inference_time: 39060.0
+      throughput: 25.60163850486431
+      estimated_peak_memory_range:
+        min: 40960
+        max: 43989968
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 118
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 118
+      job_id: jn5q31v4p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 31116.0
+      throughput: 32.13780691605605
       estimated_peak_memory_range:
-        min: 2650112
-        max: 4899184
+        min: 6332416
+        max: 62882080
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 120
+        layers_on_npu: 110
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 120
-      job_id: jqpyj8drp
+        total_layers: 110
+      job_id: j7gjeyqx5
       job_status: Passed
     reference_device_info:
-      name: RB5 (Proxy)
+      name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:32.077990Z'
+      chipset: Qcs6490
+    timestamp: '2024-05-20T16:35:29.151900Z'
   - torchscript_onnx_tflite:
-      inference_time: 10210.0
-      throughput: 97.94319294809011
+      inference_time: 200139.0
+      throughput: 4.996527413447654
       estimated_peak_memory_range:
-        min: 2527232
-        max: 4340680
+        min: 765952
+        max: 3306112
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 120
+        layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 120
-      job_id: jvgdeme65
+        total_layers: 118
+      job_id: j1gl38l8g
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
+      name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.078022Z'
+      chipset: Qcs8250
+    timestamp: '2024-05-20T16:35:29.151911Z'
+  - torchscript_onnx_qnn:
+      inference_time: 6006.0
+      throughput: 166.5001665001665
+      estimated_peak_memory_range:
+        min: 6303744
+        max: 6303744
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 110
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 110
+      job_id: jz57drx95
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 11424.0
+      throughput: 87.53501400560224
+      estimated_peak_memory_range:
+        min: 25227264
+        max: 25227264
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 115
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 115
+      job_id: jegnev3qg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 870655.0
+      throughput: 1.1485605664700713
+      estimated_peak_memory_range:
+        min: 241315840
+        max: 241315840
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 113
+        total_layers: 113
+      job_id: jopry3e7g
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.151933Z'
```

## qai_hub_models/models/ffnet_78s/export.py

```diff
@@ -115,20 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -158,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -187,24 +193,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_78s/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: FFNet-78S
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 29177.0
-      throughput: 34.27357164890153
+      inference_time: 29391.0
+      throughput: 34.02402095879691
       estimated_peak_memory_range:
-        min: 2576384
-        max: 5205816
+        min: 2580480
+        max: 4887232
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 149
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 149
-      job_id: j7gjzwx15
+      job_id: jep2mylq5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 23420.0
-      throughput: 42.69854824935952
+      inference_time: 23544.0
+      throughput: 42.473666326877336
       estimated_peak_memory_range:
-        min: 24846336
-        max: 48603008
+        min: 25210880
+        max: 46779104
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 235
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 235
-      job_id: jygzo4e45
+      job_id: j1p87yzo5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 35439.0
-      throughput: 28.21750049380626
+      inference_time: 34349.0
+      throughput: 29.1129290517919
       estimated_peak_memory_range:
-        min: 30183424
-        max: 150703648
+        min: 30216192
+        max: 174827344
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 237
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jmg9jxvm5
+        total_layers: 237
+      job_id: jw56141yp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.105190Z'
+    timestamp: '2024-05-20T16:35:29.191393Z'
   - torchscript_onnx_tflite:
-      inference_time: 21728.0
-      throughput: 46.02356406480118
+      inference_time: 21206.0
+      throughput: 47.15646515137225
       estimated_peak_memory_range:
-        min: 0
-        max: 133794256
+        min: 794624
+        max: 119306480
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 149
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 149
-      job_id: jlpeel98p
+      job_id: jqpyd36lp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 17745.0
-      throughput: 56.353902507748664
+      inference_time: 17482.0
+      throughput: 57.201693170117835
       estimated_peak_memory_range:
-        min: 25317376
-        max: 101665296
+        min: 20983808
+        max: 100170336
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 235
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 235
-      job_id: jz5w21o45
+      job_id: jogkyx3np
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 26731.0
-      throughput: 37.40974898058434
+      inference_time: 26382.0
+      throughput: 37.904631946023805
       estimated_peak_memory_range:
         min: 29417472
-        max: 90195264
+        max: 78554720
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 237
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jnp1yv0np
+        total_layers: 237
+      job_id: j1p3m0mng
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.105263Z'
+    timestamp: '2024-05-20T16:35:29.191421Z'
   - torchscript_onnx_tflite:
-      inference_time: 29631.0
-      throughput: 33.748439134690024
+      inference_time: 29621.0
+      throughput: 33.759832551230545
       estimated_peak_memory_range:
-        min: 499712
-        max: 1916448
+        min: 2560000
+        max: 5156816
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 149
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 149
-      job_id: jegnlwlj5
+      job_id: j2p0r0lnp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 23601.0
-      throughput: 42.371085970933436
+      inference_time: 23548.0
+      throughput: 42.466451503312385
       estimated_peak_memory_range:
-        min: 25165824
-        max: 55560608
+        min: 25202688
+        max: 46491072
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 235
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 235
-      job_id: j2p03x20p
+      job_id: j1glkmkmp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.105324Z'
+    timestamp: '2024-05-20T16:35:29.191438Z'
+  - torchscript_onnx_qnn:
+      inference_time: 32624.0
+      throughput: 30.65228052967141
+      estimated_peak_memory_range:
+        min: 25214976
+        max: 25214976
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 235
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 235
+      job_id: jn5q2q3o5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 33277.0
+      throughput: 30.050785828049403
+      estimated_peak_memory_range:
+        min: 26583040
+        max: 26583040
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 237
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 237
+      job_id: jwgov6vk5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 207214.0
+      throughput: 4.825928749987935
+      estimated_peak_memory_range:
+        min: 139489280
+        max: 139489280
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 105
+        total_layers: 105
+      job_id: j1pvwkwrg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.191460Z'
```

## qai_hub_models/models/ffnet_78s_lowres/export.py

```diff
@@ -115,20 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -158,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -187,24 +193,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_78s_lowres/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: FFNet-78S-LowRes
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 10805.0
-      throughput: 92.5497454881999
+      inference_time: 10832.0
+      throughput: 92.31905465288035
       estimated_peak_memory_range:
         min: 667648
-        max: 2943392
+        max: 2444712
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 149
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 149
-      job_id: jz5707zng
+      job_id: j7gjlnlep
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 11389.0
-      throughput: 87.80402142418123
+      inference_time: 11360.0
+      throughput: 88.02816901408451
       estimated_peak_memory_range:
-        min: 32768
-        max: 63143120
+        min: 135168
+        max: 63213296
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 236
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 236
-      job_id: j0pxndv85
+      job_id: jz5w969mp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 7820.0
-      throughput: 127.8772378516624
+      inference_time: 8961.0
+      throughput: 111.59468809284678
       estimated_peak_memory_range:
-        min: 2232320
-        max: 124968440
+        min: 2129920
+        max: 131892976
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 238
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jegnl72j5
+        total_layers: 238
+      job_id: jz5w9694p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.129479Z'
+    timestamp: '2024-05-20T16:35:29.221675Z'
   - torchscript_onnx_tflite:
-      inference_time: 7620.0
-      throughput: 131.23359580052494
+      inference_time: 7598.0
+      throughput: 131.61358252171624
       estimated_peak_memory_range:
-        min: 299008
-        max: 53659920
+        min: 32768
+        max: 51441440
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 149
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 149
-      job_id: jqp4k9q2g
+      job_id: jlpevmvv5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 7996.0
-      throughput: 125.06253126563281
+      inference_time: 7919.0
+      throughput: 126.27857052658165
       estimated_peak_memory_range:
-        min: 6324224
-        max: 70041552
+        min: 6307840
+        max: 73605024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 236
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 236
-      job_id: jo5mqdr7p
+      job_id: jmg94n485
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 5925.0
-      throughput: 168.77637130801688
+      inference_time: 6622.0
+      throughput: 151.01177891875565
       estimated_peak_memory_range:
-        min: 6332416
-        max: 48029072
+        min: 6012928
+        max: 45766784
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 238
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jopr8nkk5
+        total_layers: 238
+      job_id: jmg94n4m5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.129559Z'
+    timestamp: '2024-05-20T16:35:29.221702Z'
   - torchscript_onnx_tflite:
-      inference_time: 10747.0
-      throughput: 93.04922303898762
+      inference_time: 10817.0
+      throughput: 92.44707405010631
       estimated_peak_memory_range:
-        min: 655360
-        max: 2972672
+        min: 692224
+        max: 2481904
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 149
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 149
-      job_id: jw56e0zng
+      job_id: jygz7d7xp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 11414.0
-      throughput: 87.61170492377782
+      inference_time: 11402.0
+      throughput: 87.70391159445711
       estimated_peak_memory_range:
-        min: 6336512
-        max: 38367920
+        min: 1359872
+        max: 53966200
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 236
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 236
-      job_id: j7gjz8215
+      job_id: jvgdv1vzg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.129627Z'
+    timestamp: '2024-05-20T16:35:29.221720Z'
+  - torchscript_onnx_qnn:
+      inference_time: 20470.0
+      throughput: 48.85197850512946
+      estimated_peak_memory_range:
+        min: 6303744
+        max: 6303744
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 236
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 236
+      job_id: jnp18z87g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 8747.0
+      throughput: 114.32491139819366
+      estimated_peak_memory_range:
+        min: 42668032
+        max: 42668032
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 238
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 238
+      job_id: jnp18z8ng
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 64289.0
+      throughput: 15.554760534461572
+      estimated_peak_memory_range:
+        min: 42369024
+        max: 42369024
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 106
+        total_layers: 106
+      job_id: jvgdv1v6g
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.221743Z'
```

## qai_hub_models/models/ffnet_78s_quantized/export.py

```diff
@@ -119,20 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -166,16 +170,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -195,24 +201,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/ffnet_78s_quantized/perf.yaml

```diff
@@ -22,180 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: FFNet-78S-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 8382.0
-      throughput: 119.30326890956812
+      inference_time: 8341.0
+      throughput: 119.88970147464333
       estimated_peak_memory_range:
-        min: 688128
-        max: 2625256
+        min: 684032
+        max: 2437040
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 154
-      job_id: jqpyr7e05
+      job_id: jz57drdn5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 5952.0
+      throughput: 168.01075268817203
+      estimated_peak_memory_range:
+        min: 8372224
+        max: 27369456
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jo5mzxz7p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 53059.0
-      throughput: 18.846943968035582
+      inference_time: 12352.0
+      throughput: 80.95854922279793
       estimated_peak_memory_range:
-        min: 30326784
-        max: 75211072
+        min: 30101504
+        max: 79464896
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 151
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1p8049qg
+        total_layers: 151
+      job_id: jqpyd3d0p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.154340Z'
+    timestamp: '2024-05-20T16:35:29.251992Z'
   - torchscript_onnx_tflite:
-      inference_time: 5988.0
-      throughput: 167.000668002672
+      inference_time: 5972.0
+      throughput: 167.44809109176154
       estimated_peak_memory_range:
-        min: 20480
-        max: 87117952
+        min: 12288
+        max: 88653408
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 154
-      job_id: j2p03vq0p
+      job_id: jqp4wrw2g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 4317.0
+      throughput: 231.6423442205235
+      estimated_peak_memory_range:
+        min: 6307840
+        max: 75240272
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jegnevejg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 31534.0
-      throughput: 31.71180313312615
+      inference_time: 9441.0
+      throughput: 105.92098294672175
       estimated_peak_memory_range:
-        min: 31961088
-        max: 77114832
+        min: 31965184
+        max: 81051088
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 151
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jogk79nvp
+        total_layers: 151
+      job_id: j2p0r010p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.154385Z'
+    timestamp: '2024-05-20T16:35:29.252019Z'
   - torchscript_onnx_tflite:
-      inference_time: 57755.0
-      throughput: 17.31451822353043
+      inference_time: 8351.0
+      throughput: 119.74613818704347
       estimated_peak_memory_range:
-        min: 319488
-        max: 58248928
+        min: 696320
+        max: 3185352
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 156
+        layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 156
-      job_id: jz5708lng
+        total_layers: 154
+      job_id: j0px1o18g
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 547799.0
-      throughput: 1.825487085591613
+    torchscript_onnx_qnn:
+      inference_time: 5974.0
+      throughput: 167.39203213927016
       estimated_peak_memory_range:
-        min: 166916096
-        max: 242608960
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 6336512
+        max: 26276408
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 146
         layers_on_gpu: 0
-        layers_on_cpu: 149
-        total_layers: 149
-      job_id: jn5qemke5
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jep2mym65
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
+      name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:32.154436Z'
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:29.252037Z'
   - torchscript_onnx_tflite:
-      inference_time: 235689.0
-      throughput: 4.242879387667647
+      inference_time: 45673.0
+      throughput: 21.89477371751363
+      estimated_peak_memory_range:
+        min: 774144
+        max: 49521760
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 154
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 154
+      job_id: jz5wqzl65
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 37262.0
+      throughput: 26.83699210992432
       estimated_peak_memory_range:
-        min: 2572288
-        max: 5196608
+        min: 6307840
+        max: 71671376
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 156
+        layers_on_npu: 146
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 156
-      job_id: j2p02or25
+        total_layers: 146
+      job_id: j0pxyrl1g
       job_status: Passed
     reference_device_info:
-      name: RB5 (Proxy)
+      name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:32.154466Z'
+      chipset: Qcs6490
+    timestamp: '2024-05-20T16:35:29.252054Z'
   - torchscript_onnx_tflite:
-      inference_time: 10675.0
-      throughput: 93.6768149882904
+      inference_time: 218485.0
+      throughput: 4.576973247591368
       estimated_peak_memory_range:
-        min: 2576384
-        max: 4529144
+        min: 770048
+        max: 10557616
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 156
+        layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 156
-      job_id: jvgdemq65
+        total_layers: 154
+      job_id: jmg9w2zlp
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
+      name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.154494Z'
+      chipset: Qcs8250
+    timestamp: '2024-05-20T16:35:29.252065Z'
+  - torchscript_onnx_qnn:
+      inference_time: 7096.0
+      throughput: 140.92446448703495
+      estimated_peak_memory_range:
+        min: 6303744
+        max: 6303744
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jopry3ykg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 13843.0
+      throughput: 72.23867658744491
+      estimated_peak_memory_range:
+        min: 34721792
+        max: 34721792
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 151
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 151
+      job_id: j1p87y3q5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 801403.0
+      throughput: 1.2478116503182544
+      estimated_peak_memory_range:
+        min: 204279808
+        max: 204279808
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jogkyxlvp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.252087Z'
```

## qai_hub_models/models/googlenet/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/googlenet/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: GoogLeNet
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1044.0
-      throughput: 957.8544061302682
+      inference_time: 1047.0
+      throughput: 955.1098376313277
       estimated_peak_memory_range:
-        min: 28672
-        max: 2002104
+        min: 16384
+        max: 1526704
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 84
-      job_id: jnp1yvlnp
+      job_id: jqp4wrx2g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1075.0
-      throughput: 930.2325581395348
+      inference_time: 1089.0
+      throughput: 918.2736455463728
       estimated_peak_memory_range:
-        min: 20480
-        max: 26621784
+        min: 618496
+        max: 4593576
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 143
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 143
-      job_id: jz5707wng
+      job_id: jegnev9jg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1293.0
-      throughput: 773.3952049497293
+      inference_time: 1227.0
+      throughput: 814.9959250203749
       estimated_peak_memory_range:
-        min: 12288
-        max: 46074600
+        min: 16384
+        max: 45472688
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 145
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j0pxndj85
+        total_layers: 145
+      job_id: j2p0r0e0p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.213322Z'
+    timestamp: '2024-05-20T16:35:29.373835Z'
   - torchscript_onnx_tflite:
-      inference_time: 650.0
-      throughput: 1538.4615384615386
+      inference_time: 691.0
+      throughput: 1447.178002894356
       estimated_peak_memory_range:
-        min: 16384
-        max: 45786064
+        min: 12288
+        max: 46214624
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 84
-      job_id: jvgdez965
+      job_id: j0px1o78g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 693.0
-      throughput: 1443.001443001443
+      inference_time: 699.0
+      throughput: 1430.615164520744
       estimated_peak_memory_range:
         min: 0
-        max: 53494384
+        max: 56918592
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 143
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 143
-      job_id: jqp4k9o2g
+      job_id: jopry34kg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 852.0
-      throughput: 1173.7089201877934
+      inference_time: 898.0
+      throughput: 1113.5857461024498
       estimated_peak_memory_range:
-        min: 618496
-        max: 24414912
+        min: 602112
+        max: 25082000
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 145
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jo5mqd27p
+        total_layers: 145
+      job_id: j1p87ywq5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.213386Z'
+    timestamp: '2024-05-20T16:35:29.373860Z'
   - torchscript_onnx_tflite:
-      inference_time: 1043.0
-      throughput: 958.7727708533077
+      inference_time: 1047.0
+      throughput: 955.1098376313277
       estimated_peak_memory_range:
         min: 12288
-        max: 1850480
+        max: 17376784
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 84
-      job_id: jlpeenk8p
+      job_id: jo5mzxw7p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1090.0
-      throughput: 917.4311926605504
+      inference_time: 1094.0
+      throughput: 914.0767824497258
       estimated_peak_memory_range:
         min: 622592
-        max: 4955600
+        max: 5356744
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 143
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 143
-      job_id: jnp1ymenp
+      job_id: jqpyd340p
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.213429Z'
+    timestamp: '2024-05-20T16:35:29.373877Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1259.0
+      throughput: 794.2811755361398
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 143
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 143
+      job_id: jep2my765
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1333.0
+      throughput: 750.1875468867216
+      estimated_peak_memory_range:
+        min: 11251712
+        max: 11251712
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 145
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 145
+      job_id: jogkyxrvp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 5736.0
+      throughput: 174.33751743375174
+      estimated_peak_memory_range:
+        min: 11059200
+        max: 11059200
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 84
+        total_layers: 84
+      job_id: jn5q2q9e5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.373901Z'
```

## qai_hub_models/models/googlenet_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,20 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -203,14 +214,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/googlenet_quantized/perf.yaml

```diff
@@ -22,240 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: GoogLeNetQuantized
   performance_metrics:
   - torchscript_onnx_tflite:
       inference_time: 297.0
       throughput: 3367.003367003367
       estimated_peak_memory_range:
         min: 12288
-        max: 1529584
+        max: 1659216
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 84
-      job_id: jopr8nqk5
+      job_id: j1glkme2p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 346.0
-      throughput: 2890.173410404624
+      inference_time: 345.0
+      throughput: 2898.550724637681
       estimated_peak_memory_range:
-        min: 16384
-        max: 139797592
+        min: 90112
+        max: 4621032
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 86
-      job_id: jogk79mvp
+      job_id: jwgov6e15
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 756.0
-      throughput: 1322.7513227513227
+      inference_time: 623.0
+      throughput: 1605.1364365971108
       estimated_peak_memory_range:
         min: 12288
-        max: 22997816
+        max: 31466656
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 94
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1gl61r2g
+        total_layers: 94
+      job_id: jygz7dv4p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.237563Z'
+    timestamp: '2024-05-20T16:35:29.404186Z'
   - torchscript_onnx_tflite:
-      inference_time: 229.0
-      throughput: 4366.812227074236
+      inference_time: 214.0
+      throughput: 4672.897196261682
       estimated_peak_memory_range:
         min: 12288
-        max: 32807600
+        max: 33138256
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 84
-      job_id: jqpyr7w05
+      job_id: jw5614qnp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 242.0
-      throughput: 4132.231404958678
+      inference_time: 250.0
+      throughput: 4000.0
       estimated_peak_memory_range:
-        min: 163840
-        max: 41416608
+        min: 0
+        max: 43090384
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 86
-      job_id: jn5qemoe5
+      job_id: j1pvwkzzg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 547.0
-      throughput: 1828.1535648994516
+      inference_time: 475.0
+      throughput: 2105.2631578947367
       estimated_peak_memory_range:
-        min: 3473408
-        max: 30390976
+        min: 0
+        max: 26393632
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 94
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jw56edlng
+        total_layers: 94
+      job_id: jz5w96m4p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.237614Z'
+    timestamp: '2024-05-20T16:35:29.404215Z'
   - torchscript_onnx_tflite:
-      inference_time: 1013.0
-      throughput: 987.1668311944719
+      inference_time: 297.0
+      throughput: 3367.003367003367
       estimated_peak_memory_range:
-        min: 20480
-        max: 16869552
+        min: 12288
+        max: 1518064
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 84
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 84
+      job_id: j1p3m0qmg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 344.0
+      throughput: 2906.9767441860463
+      estimated_peak_memory_range:
+        min: 16384
+        max: 100695528
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 86
-      job_id: jz5708xqg
+      job_id: jlpevm485
       job_status: Passed
-    torchscript_onnx_qnn:
-      inference_time: 'null'
-      throughput: 'null'
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:29.404232Z'
+  - torchscript_onnx_tflite:
+      inference_time: 950.0
+      throughput: 1052.6315789473683
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 12288
+        max: 17406016
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jegnlwev5
-      job_status: Failed
-    torchscript_onnx_ort:
-      inference_time: 10247.0
-      throughput: 97.58953840148337
+        total_layers: 84
+      job_id: jygzrylk5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1100.0
+      throughput: 909.0909090909091
       estimated_peak_memory_range:
-        min: 2646016
-        max: 50596416
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 163840
+        max: 37495168
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 86
         layers_on_gpu: 0
-        layers_on_cpu: 95
-        total_layers: 95
-      job_id: j1p3vw2mg
+        layers_on_cpu: 0
+        total_layers: 86
+      job_id: jmg9w2owp
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:32.237671Z'
+    timestamp: '2024-05-20T16:35:29.404247Z'
   - torchscript_onnx_tflite:
-      inference_time: 5919.0
-      throughput: 168.94745734076702
+      inference_time: 5755.0
+      throughput: 173.7619461337967
       estimated_peak_memory_range:
         min: 20480
-        max: 6396208
+        max: 7049192
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 86
+        layers_on_npu: 84
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 86
-      job_id: j1p8mj7z5
+        total_layers: 84
+      job_id: jz5wqzy65
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:32.237693Z'
-  - torchscript_onnx_tflite:
-      inference_time: 322.0
-      throughput: 3105.590062111801
+    timestamp: '2024-05-20T16:35:29.404257Z'
+  - torchscript_onnx_qnn:
+      inference_time: 465.0
+      throughput: 2150.537634408602
       estimated_peak_memory_range:
-        min: 12288
-        max: 2046792
+        min: 540672
+        max: 540672
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 86
-      job_id: j0pxnzyj5
+      job_id: j7gjlnk1p
       job_status: Passed
-    torchscript_onnx_qnn:
-      inference_time: 365.0
-      throughput: 2739.72602739726
+    torchscript_onnx_ort:
+      inference_time: 616.0
+      throughput: 1623.3766233766235
       estimated_peak_memory_range:
-        min: 634880
-        max: 5391328
+        min: 19083264
+        max: 19083264
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 88
+        layers_on_npu: 94
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 88
-      job_id: jep20zmxg
+        total_layers: 94
+      job_id: jmg94n9m5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 2182.0
+      throughput: 458.29514207149407
+      estimated_peak_memory_range:
+        min: 1978368
+        max: 1978368
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 94
+        total_layers: 94
+      job_id: jnp18zqng
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.237731Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.404279Z'
```

## qai_hub_models/models/hrnet_pose/app.py

```diff
@@ -196,10 +196,10 @@
         keypoints = keypoints / input_size * scale + center - 0.5 * scale
         keypoints = np.round(keypoints).astype(np.int32)
         if raw_output:
             return keypoints
 
         predicted_images = []
         for i, img in enumerate(NHWC_int_numpy_frames):
-            draw_points(img, keypoints[i], color=(255, 0, 0), size=2)
+            draw_points(img, keypoints[i], color=(255, 0, 0), size=6)
             predicted_images.append(fromarray(img))
         return predicted_images
```

## qai_hub_models/models/hrnet_pose/export.py

```diff
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/hrnet_pose/info.yaml

```diff
@@ -12,24 +12,24 @@
   Estimation
 license: https://github.com/quic/aimet-model-zoo/blob/develop/LICENSE.pdf
 deploy_license: https://qaihub-public-assets.s3.us-west-2.amazonaws.com/qai-hub-models/Qualcomm+AI+Hub+Proprietary+License.pdf
 source_repo:
   https://github.com/quic/aimet-model-zoo/tree/develop/aimet_zoo_torch/hrnet_posenet
 technical_details:
   Model checkpoint: hrnet_posenet_FP32_state_dict
-  Input resolution: 192x256
+  Input resolution: 256x192
   Number of parameters: 28.5M
   Model size: 109 MB
 applicable_scenarios:
   - Injury prevention training
   - Sports performance analysis
   - Posture recognition
 form_factors:
   - Phone
   - Tablet
   - IoT
 related_models: [litehrnet, openpose]
 has_static_banner: yes
-has_animated_banner: no
+has_animated_banner: yes
 license_type: other
 deploy_license_type: AI Model Hub License
 dataset: []
```

## qai_hub_models/models/hrnet_pose/model.py

```diff
@@ -17,15 +17,15 @@
     load_numpy,
 )
 from qai_hub_models.utils.base_model import BaseModel
 from qai_hub_models.utils.image_processing import normalize_image_torchvision
 from qai_hub_models.utils.input_spec import InputSpec
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 1
+MODEL_ASSET_VERSION = 2
 # This model originally comes from https://github.com/leoxiaobin/deep-high-resolution-net.pytorch
 # but we'll use the weights from AIMET
 # Weights and config stored in S3 are sourced from
 # https://github.com/quic/aimet-model-zoo/blob/develop/aimet_zoo_torch/hrnet_posenet/models/model_cards/hrnet_posenet_w8a8.json
 # Weights are found here
 # https://github.com/quic/aimet-model-zoo/releases/download/phase_2_march_artifacts/hrnet_posenet_FP32_state_dict.pth
 DEFAULT_WEIGHTS = "hrnet_posenet_FP32_state_dict.pth"
```

## qai_hub_models/models/hrnet_pose/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: HRNetPose
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2289.0
-      throughput: 436.871996505024
+      inference_time: 2818.0
+      throughput: 354.86160397444996
       estimated_peak_memory_range:
-        min: 16384
-        max: 2655344
+        min: 28672
+        max: 2913312
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 514
+        layers_on_npu: 516
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 514
-      job_id: jwgok4q1p
+        total_layers: 516
+      job_id: jvgdv176g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2297.0
-      throughput: 435.35045711798
+      inference_time: 2886.0
+      throughput: 346.5003465003465
       estimated_peak_memory_range:
-        min: 12288
-        max: 59340792
+        min: 16384
+        max: 20957856
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 745
+        layers_on_npu: 747
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 745
-      job_id: j7gjzw415
+        total_layers: 747
+      job_id: j0px1oe8g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 3007.0
-      throughput: 332.5573661456601
+      inference_time: 3134.0
+      throughput: 319.0810465858328
       estimated_peak_memory_range:
         min: 0
-        max: 148641888
+        max: 128298872
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 749
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jygzo4k45
+        total_layers: 749
+      job_id: jep2my365
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.272940Z'
+    timestamp: '2024-05-20T16:35:29.443668Z'
   - torchscript_onnx_tflite:
-      inference_time: 1753.0
-      throughput: 570.4506560182544
+      inference_time: 2065.0
+      throughput: 484.26150121065376
       estimated_peak_memory_range:
-        min: 225280
-        max: 107290736
+        min: 12288
+        max: 109086992
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 514
+        layers_on_npu: 516
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 514
-      job_id: j1pv09xz5
+        total_layers: 516
+      job_id: jz57drvn5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1719.0
-      throughput: 581.7335660267597
+      inference_time: 2134.0
+      throughput: 468.6035613870665
       estimated_peak_memory_range:
-        min: 606208
-        max: 177224704
+        min: 0
+        max: 189704832
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 745
+        layers_on_npu: 747
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 745
-      job_id: jlpeel38p
+        total_layers: 747
+      job_id: jo5mzxv7p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2250.0
-      throughput: 444.44444444444446
+      inference_time: 2215.0
+      throughput: 451.46726862302484
       estimated_peak_memory_range:
         min: 12288
-        max: 81136704
+        max: 93863680
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 749
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jz5w21n45
+        total_layers: 749
+      job_id: jqpyd3v0p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.273098Z'
+    timestamp: '2024-05-20T16:35:29.443693Z'
   - torchscript_onnx_tflite:
-      inference_time: 2294.0
-      throughput: 435.9197907585004
+      inference_time: 2881.0
+      throughput: 347.1017007983339
       estimated_peak_memory_range:
-        min: 16384
-        max: 3533472
+        min: 24576
+        max: 4152200
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 514
+        layers_on_npu: 516
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 514
-      job_id: jogk7kyyp
+        total_layers: 516
+      job_id: jqp4wrj2g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2291.0
-      throughput: 436.4906154517678
+      inference_time: 2909.0
+      throughput: 343.7607425232039
       estimated_peak_memory_range:
-        min: 610304
-        max: 59474648
+        min: 630784
+        max: 16131888
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 745
+        layers_on_npu: 747
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 745
-      job_id: j1p3vrmxg
+        total_layers: 747
+      job_id: jopry31kg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.273247Z'
+    timestamp: '2024-05-20T16:35:29.443710Z'
+  - torchscript_onnx_qnn:
+      inference_time: 3156.0
+      throughput: 316.85678073510775
+      estimated_peak_memory_range:
+        min: 589824
+        max: 589824
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 747
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 747
+      job_id: jegnevrjg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 2975.0
+      throughput: 336.1344537815126
+      estimated_peak_memory_range:
+        min: 54882304
+        max: 54882304
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 749
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 749
+      job_id: j2p0r0k0p
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 19453.0
+      throughput: 51.40595280933532
+      estimated_peak_memory_range:
+        min: 37265408
+        max: 37265408
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j1p87y8q5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.443733Z'
```

## qai_hub_models/models/huggingface_wavlm_base_plus/export.py

```diff
@@ -115,15 +115,15 @@
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options
+        target_runtime, compile_options, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
```

## qai_hub_models/models/huggingface_wavlm_base_plus/model.py

```diff
@@ -78,15 +78,20 @@
 
     def get_hub_profile_options(
         self, target_runtime: TargetRuntime, other_profile_options: str = ""
     ) -> str:
         profile_options = super().get_hub_profile_options(
             target_runtime, other_profile_options
         )
-        return profile_options + " --compute_unit cpu"
+        if (
+            target_runtime == TargetRuntime.TFLITE
+            and "--compute_unit" not in profile_options
+        ):
+            profile_options = profile_options + " --compute_unit gpu"
+        return profile_options
 
 
 # Modules used to override Huggingface WavLM to be NPU friendly
 class SliceConv1d(torch.nn.Module):
     def __init__(self, orig_module: torch.nn.Conv1d, slice_size: int = 16000):
         """Slice inputs to conv1d to limit the input size to any conv"""
         super().__init__()
```

## qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml

```diff
@@ -18,117 +18,202 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: HuggingFace-WavLM-Base-Plus
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 884463.0
-      throughput: 1.1306295458374178
+      inference_time: 938575.0
+      throughput: 1.0654449564499373
       estimated_peak_memory_range:
-        min: 149233664
-        max: 152668384
+        min: 130052096
+        max: 143676568
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 811
         total_layers: 811
-      job_id: jo5mqdy7p
+      job_id: jmg94n8m5
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jz57dr6n5
+      job_status: Failed
     torchscript_onnx_ort:
-      inference_time: 613080.0
-      throughput: 1.631108501337509
+      inference_time: 'null'
+      throughput: 'null'
       estimated_peak_memory_range:
-        min: 16220160
-        max: 44091568
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
-        layers_on_cpu: 484
-        total_layers: 484
-      job_id: jopr8njk5
-      job_status: Passed
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jo5mzx47p
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.343742Z'
+    timestamp: '2024-05-20T16:35:29.513720Z'
   - torchscript_onnx_tflite:
-      inference_time: 789013.0
-      throughput: 1.2674062404548467
+      inference_time: 852446.0
+      throughput: 1.173094835332678
       estimated_peak_memory_range:
-        min: 148623360
-        max: 174462192
+        min: 148041728
+        max: 183065760
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 811
         total_layers: 811
-      job_id: jegnl78j5
+      job_id: jnp18z3ng
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jqp4wr82g
+      job_status: Failed
     torchscript_onnx_ort:
-      inference_time: 513891.0
-      throughput: 1.9459379518224682
+      inference_time: 'null'
+      throughput: 'null'
       estimated_peak_memory_range:
-        min: 995328
-        max: 204911264
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
-        layers_on_cpu: 484
-        total_layers: 484
-      job_id: jep20vn6g
-      job_status: Passed
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jegnevxjg
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.343896Z'
+    timestamp: '2024-05-20T16:35:29.513746Z'
   - torchscript_onnx_tflite:
-      inference_time: 928773.0
-      throughput: 1.0766893525113241
+      inference_time: 867664.0
+      throughput: 1.1525198694425491
       estimated_peak_memory_range:
-        min: 150151168
-        max: 158231104
+        min: 149274624
+        max: 152991232
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 811
         total_layers: 811
-      job_id: jqp4k2wqg
+      job_id: jvgdv106g
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j0px1om8g
+      job_status: Failed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.343990Z'
+    timestamp: '2024-05-20T16:35:29.513762Z'
+  - torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jopry39kg
+      job_status: Failed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jep2myj65
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.513781Z'
```

## qai_hub_models/models/inception_v3/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/inception_v3/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Inception-v3
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1337.0
-      throughput: 747.9431563201197
+      inference_time: 1342.0
+      throughput: 745.156482861401
       estimated_peak_memory_range:
-        min: 20480
-        max: 2064624
+        min: 12288
+        max: 1685032
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 129
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 129
-      job_id: j2p03600p
+      job_id: jqpyd3n0p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1396.0
-      throughput: 716.3323782234957
+      inference_time: 1414.0
+      throughput: 707.2135785007072
       estimated_peak_memory_range:
         min: 16384
-        max: 150190256
+        max: 149750296
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 219
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 219
-      job_id: jogk78xvp
+      job_id: jogkyxovp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1728.0
-      throughput: 578.7037037037037
+      inference_time: 1719.0
+      throughput: 581.7335660267597
       estimated_peak_memory_range:
-        min: 57344
-        max: 214567960
+        min: 12288
+        max: 214330432
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 221
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1gl6lm2g
+        total_layers: 221
+      job_id: j1p3m0xmg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.362074Z'
+    timestamp: '2024-05-20T16:35:29.537865Z'
   - torchscript_onnx_tflite:
-      inference_time: 1019.0
-      throughput: 981.3542688910696
+      inference_time: 1013.0
+      throughput: 987.1668311944719
       estimated_peak_memory_range:
-        min: 12288
-        max: 51945968
+        min: 16384
+        max: 52159904
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 129
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 129
-      job_id: j1p801yqg
+      job_id: j2p0r0d0p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1044.0
-      throughput: 957.8544061302682
+      inference_time: 1043.0
+      throughput: 958.7727708533077
       estimated_peak_memory_range:
-        min: 618496
-        max: 62186832
+        min: 0
+        max: 66127216
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 219
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 219
-      job_id: jn5qevqe5
+      job_id: jn5q2qze5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1343.0
-      throughput: 744.6016381236038
+      inference_time: 1333.0
+      throughput: 750.1875468867216
       estimated_peak_memory_range:
         min: 618496
-        max: 25688304
+        max: 28967744
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 221
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jw56ew4ng
+        total_layers: 221
+      job_id: jwgov6o15
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.362144Z'
+    timestamp: '2024-05-20T16:35:29.537891Z'
   - torchscript_onnx_tflite:
-      inference_time: 1335.0
-      throughput: 749.0636704119851
+      inference_time: 1352.0
+      throughput: 739.6449704142012
       estimated_peak_memory_range:
-        min: 24576
-        max: 1812440
+        min: 16384
+        max: 2133976
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 129
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 129
-      job_id: jogk7klyp
+      job_id: j1p87y6q5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1397.0
-      throughput: 715.8196134574088
+      inference_time: 1421.0
+      throughput: 703.7297677691766
       estimated_peak_memory_range:
-        min: 36864
-        max: 150659520
+        min: 20480
+        max: 150041024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 219
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 219
-      job_id: j1p3vr4xg
+      job_id: jw5614rnp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.362196Z'
+    timestamp: '2024-05-20T16:35:29.537908Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1636.0
+      throughput: 611.2469437652812
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 219
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 219
+      job_id: j1glkmo2p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1677.0
+      throughput: 596.3029218843172
+      estimated_peak_memory_range:
+        min: 48324608
+        max: 48324608
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 221
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 221
+      job_id: j1pvwkezg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 12033.0
+      throughput: 83.10479514667996
+      estimated_peak_memory_range:
+        min: 26181632
+        max: 26181632
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 123
+        total_layers: 123
+      job_id: j7gjlno1p
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.537932Z'
```

## qai_hub_models/models/inception_v3_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,20 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -203,14 +214,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/inception_v3_quantized/info.yaml

```diff
@@ -3,15 +3,15 @@
 id: inception_v3_quantized
 status: public
 headline: Quantized Imagenet classifier and general purpose backbone.
 domain: Computer Vision
 description: InceptionNetV3 is a machine learning model that can classify images from
   the Imagenet dataset. It can also be used as a backbone in building more complex
   models for specific use cases. This model is post-training quantized to int8 using
-  samples from [Google's open images dataset](https://storage.googleapis.com/openimages/web/index.html).
+  samples from Google's open images dataset.
 use_case: Image Classification
 tags:
   - backbone
   - quantized
 research_paper: http://arxiv.org/abs/1512.00567
 research_paper_title: Rethinking the Inception Architecture for Computer Vision
 license: https://github.com/pytorch/vision/blob/main/LICENSE
```

## qai_hub_models/models/inception_v3_quantized/perf.yaml

```diff
@@ -22,180 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Inception-v3-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 623.0
-      throughput: 1605.1364365971108
+      inference_time: 615.0
+      throughput: 1626.0162601626016
       estimated_peak_memory_range:
-        min: 40960
-        max: 1585824
+        min: 20480
+        max: 1835968
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 144
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 144
-      job_id: jwgok861p
+      job_id: jlpevm885
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 656.0
+      throughput: 1524.3902439024391
+      estimated_peak_memory_range:
+        min: 16384
+        max: 70614144
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 134
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 134
+      job_id: jmg94nkm5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1098.0
-      throughput: 910.7468123861566
+      inference_time: 934.0
+      throughput: 1070.6638115631692
       estimated_peak_memory_range:
-        min: 53248
-        max: 53526464
+        min: 12288
+        max: 63129504
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 137
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j7gjzqn15
+        total_layers: 137
+      job_id: jmg94nkq5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.386127Z'
+    timestamp: '2024-05-20T16:35:29.568165Z'
   - torchscript_onnx_tflite:
-      inference_time: 492.0
-      throughput: 2032.520325203252
+      inference_time: 466.0
+      throughput: 2145.922746781116
       estimated_peak_memory_range:
         min: 12288
-        max: 64321136
+        max: 65030624
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 144
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 144
-      job_id: j1pv07kz5
+      job_id: jygz7d84p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 493.0
+      throughput: 2028.3975659229209
+      estimated_peak_memory_range:
+        min: 163840
+        max: 49682240
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 134
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 134
+      job_id: jnp18z7ng
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 880.0
-      throughput: 1136.3636363636363
+      inference_time: 708.0
+      throughput: 1412.4293785310736
       estimated_peak_memory_range:
-        min: 618496
-        max: 36779824
+        min: 0
+        max: 35132704
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 137
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jlpeeym8p
+        total_layers: 137
+      job_id: jnp18z7kg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.386165Z'
+    timestamp: '2024-05-20T16:35:29.568191Z'
+  - torchscript_onnx_tflite:
+      inference_time: 627.0
+      throughput: 1594.896331738437
+      estimated_peak_memory_range:
+        min: 16384
+        max: 2002888
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 144
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 144
+      job_id: jz5w9684p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 645.0
+      throughput: 1550.3875968992247
+      estimated_peak_memory_range:
+        min: 24576
+        max: 70914568
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 134
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 134
+      job_id: jz5w968zp
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:29.568208Z'
   - torchscript_onnx_tflite:
-      inference_time: 2624.0
-      throughput: 381.0975609756098
+      inference_time: 2476.0
+      throughput: 403.8772213247173
       estimated_peak_memory_range:
         min: 12288
-        max: 20812688
+        max: 21173984
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 146
+        layers_on_npu: 144
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 146
-      job_id: j7gjz8075
+        total_layers: 144
+      job_id: jygzry0o5
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 26460.0
-      throughput: 37.79289493575208
+    torchscript_onnx_qnn:
+      inference_time: 2578.0
+      throughput: 387.8975950349108
       estimated_peak_memory_range:
-        min: 17575936
-        max: 85502320
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 163840
+        max: 52566912
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 134
         layers_on_gpu: 0
-        layers_on_cpu: 138
-        total_layers: 138
-      job_id: jygzond45
+        layers_on_cpu: 0
+        total_layers: 134
+      job_id: jqp4v428p
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:32.386219Z'
+    timestamp: '2024-05-20T16:35:29.568224Z'
   - torchscript_onnx_tflite:
-      inference_time: 7950.0
-      throughput: 125.78616352201257
+      inference_time: 7805.0
+      throughput: 128.12299807815504
       estimated_peak_memory_range:
-        min: 45056
-        max: 4402544
+        min: 16384
+        max: 7895408
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 146
+        layers_on_npu: 144
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 146
-      job_id: jn5qr427p
+        total_layers: 144
+      job_id: jz5wqzr35
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:32.386245Z'
-  - torchscript_onnx_tflite:
-      inference_time: 641.0
-      throughput: 1560.0624024960998
+    timestamp: '2024-05-20T16:35:29.568235Z'
+  - torchscript_onnx_qnn:
+      inference_time: 716.0
+      throughput: 1396.6480446927374
       estimated_peak_memory_range:
-        min: 12288
-        max: 1923000
+        min: 413696
+        max: 413696
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 134
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 134
+      job_id: jvgdv186g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 910.0
+      throughput: 1098.901098901099
+      estimated_peak_memory_range:
+        min: 39702528
+        max: 39702528
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 146
+        layers_on_npu: 137
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 146
-      job_id: jlpeenr7p
+        total_layers: 137
+      job_id: jvgdv18kg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 21412.0
+      throughput: 46.70278348589576
+      estimated_peak_memory_range:
+        min: 20770816
+        max: 20770816
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jz57drkq5
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.386270Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.568257Z'
```

## qai_hub_models/models/lama_dilated/export.py

```diff
@@ -116,20 +116,25 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image,mask"
+        + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image,mask"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +164,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image,mask", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image,mask", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,16 +197,20 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
```

## qai_hub_models/models/lama_dilated/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: LaMa-Dilated
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 87925.0
-      throughput: 11.373329542223486
+      inference_time: 87247.0
+      throughput: 11.46171215056105
       estimated_peak_memory_range:
-        min: 0
-        max: 3269648
+        min: 2240512
+        max: 138049312
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 347
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 347
-      job_id: jz5w24645
+      job_id: jqp4wrmqg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 81938.0
-      throughput: 12.204349630208206
+      inference_time: 81632.0
+      throughput: 12.250098000784007
       estimated_peak_memory_range:
-        min: 1654784
-        max: 33961664
+        min: 4276224
+        max: 42687880
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 332
+        layers_on_npu: 333
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 332
-      job_id: jnp1y6znp
+        total_layers: 333
+      job_id: jegnev7vg
       job_status: Passed
     torchscript_onnx_ort:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jz5w246z5
+      job_id: j2p0r0v2p
       job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.412866Z'
+    timestamp: '2024-05-20T16:35:29.607433Z'
   - torchscript_onnx_tflite:
-      inference_time: 60997.0
-      throughput: 16.39424889748676
+      inference_time: 59804.0
+      throughput: 16.721289545849775
       estimated_peak_memory_range:
-        min: 2707456
-        max: 271146544
+        min: 2932736
+        max: 243608672
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 347
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 347
-      job_id: jmg9jdnm5
+      job_id: j0px1o3jg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 57249.0
-      throughput: 17.4675540184108
+      inference_time: 57736.0
+      throughput: 17.32021615629763
       estimated_peak_memory_range:
-        min: 4161536
-        max: 189298048
+        min: 2392064
+        max: 161784064
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 332
+        layers_on_npu: 333
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 332
-      job_id: jvgde2165
+        total_layers: 333
+      job_id: jopry3nvg
       job_status: Passed
     torchscript_onnx_ort:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jmg9jdnq5
+      job_id: j1p87y4z5
       job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.412968Z'
+    timestamp: '2024-05-20T16:35:29.607460Z'
   - torchscript_onnx_tflite:
-      inference_time: 87453.0
-      throughput: 11.434713503253176
+      inference_time: 85940.0
+      throughput: 11.63602513381429
       estimated_peak_memory_range:
-        min: 3260416
-        max: 139194808
+        min: 3170304
+        max: 139550144
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 347
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 347
-      job_id: jopr871v5
+      job_id: jo5mzxoyp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 82234.0
-      throughput: 12.160420264124328
+      inference_time: 80913.0
+      throughput: 12.358953443822378
       estimated_peak_memory_range:
-        min: 3178496
-        max: 33096560
+        min: 3190784
+        max: 42527696
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 332
+        layers_on_npu: 333
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 332
-      job_id: j1p80kwzg
+        total_layers: 333
+      job_id: jqpyd37rp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.413058Z'
+    timestamp: '2024-05-20T16:35:29.607478Z'
+  - torchscript_onnx_qnn:
+      inference_time: 92003.0
+      throughput: 10.869210786604784
+      estimated_peak_memory_range:
+        min: 4202496
+        max: 4202496
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 333
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 333
+      job_id: jep2myvx5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jogkyx9yp
+      job_status: Failed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 659315.0
+      throughput: 1.5167256925748693
+      estimated_peak_memory_range:
+        min: 278200320
+        max: 278200320
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 220
+        total_layers: 220
+      job_id: jn5q2qm75
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.607503Z'
```

## qai_hub_models/models/litehrnet/app.py

```diff
@@ -99,10 +99,10 @@
         keypoints = np.round(keypoints).astype(np.int32)
 
         if raw_output:
             return keypoints
 
         predicted_images = []
         for i, img in enumerate(NHWC_int_numpy_frames):
-            draw_points(img, keypoints[i], color=(255, 0, 0), size=2)
+            draw_points(img, keypoints[i], color=(255, 0, 0), size=6)
             predicted_images.append(fromarray(img))
         return predicted_images
```

## qai_hub_models/models/litehrnet/export.py

```diff
@@ -116,15 +116,15 @@
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options
+        target_runtime, compile_options, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
```

## qai_hub_models/models/litehrnet/info.yaml

```diff
@@ -23,11 +23,11 @@
   - Posture recognition
 form_factors:
   - Phone
   - Tablet
   - IoT
 related_models: [openpose, hrnet_pose]
 has_static_banner: yes
-has_animated_banner: no
+has_animated_banner: yes
 license_type: apache-2.0
 deploy_license_type: AI Model Hub License
 dataset: []
```

## qai_hub_models/models/litehrnet/perf.yaml

```diff
@@ -18,117 +18,157 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: LiteHRNet
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 15561.0
-      throughput: 64.263222157959
+      inference_time: 11083.0
+      throughput: 90.22827754218171
       estimated_peak_memory_range:
-        min: 6553600
-        max: 13181120
+        min: 6615040
+        max: 31875176
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1226
         layers_on_gpu: 0
         layers_on_cpu: 10
         total_layers: 1236
-      job_id: jvgde21k5
+      job_id: j1glkm1ep
       job_status: Passed
     torchscript_onnx_ort:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jqp4k3rqg
+      job_id: jwgov6445
       job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.436998Z'
+    timestamp: '2024-05-20T16:35:29.637644Z'
   - torchscript_onnx_tflite:
-      inference_time: 10344.0
-      throughput: 96.67440061871616
+      inference_time: 7847.0
+      throughput: 127.43723716069836
       estimated_peak_memory_range:
-        min: 20480
-        max: 73273328
+        min: 16384
+        max: 74259408
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1226
         layers_on_gpu: 0
         layers_on_cpu: 10
         total_layers: 1236
-      job_id: jz5709rqg
+      job_id: jw5614dvp
       job_status: Passed
     torchscript_onnx_ort:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: j0pxnxoj5
+      job_id: j1pvwk97g
       job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.437166Z'
+    timestamp: '2024-05-20T16:35:29.637666Z'
   - torchscript_onnx_tflite:
-      inference_time: 15632.0
-      throughput: 63.97134083930399
+      inference_time: 11125.0
+      throughput: 89.88764044943821
       estimated_peak_memory_range:
-        min: 6529024
-        max: 10764512
+        min: 6553600
+        max: 11774200
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1226
         layers_on_gpu: 0
         layers_on_cpu: 10
         total_layers: 1236
-      job_id: j1gl6qeeg
+      job_id: j1p3m0wxg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.437300Z'
+    timestamp: '2024-05-20T16:35:29.637678Z'
+  - torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j7gjlnw7p
+      job_status: Failed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 17746.0
+      throughput: 56.35072692437733
+      estimated_peak_memory_range:
+        min: 9547776
+        max: 9547776
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 939
+        total_layers: 939
+      job_id: jlpevml75
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.637696Z'
```

## qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml

```diff
@@ -1,10 +1,10 @@
 name: Llama-v2-7B-Chat
 id: llama_v2_7b_chat_quantized
-status: public # Renable when approved by marketing #9577
+status: public
 headline: State-of-the-art large language model useful on a variety of language
   understanding and generation tasks.
 domain: Generative AI
 description: Llama 2 is a family of LLMs. The "Chat" at the end indicates that
   the model is optimized for chatbot-like dialogue. The model is quantized to
   4-bit weights and 16-bit activations making it suitable for on-device
   deployment. For Prompt and output length specified below, the time to first token is
```

## qai_hub_models/models/mediapipe_face/export.py

```diff
@@ -130,15 +130,15 @@
         component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options
+            target_runtime, compile_options, hub_device
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub_device,
             name=f"{model_name}_{component_name}",
@@ -222,19 +222,14 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(
-        model_cls=Model,
-        components=ALL_COMPONENTS,
-        supports_qnn=False,
-        supports_ort=False,
-    )
+    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mediapipe_face/model.py

```diff
@@ -9,15 +9,15 @@
 import torch
 
 from qai_hub_models.models._shared.mediapipe.utils import MediaPipePyTorchAsRoot
 from qai_hub_models.utils.base_model import BaseModel, CollectionModel
 from qai_hub_models.utils.input_spec import InputSpec
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 1
+MODEL_ASSET_VERSION = 2
 
 # Vertex indices can be found in
 # https://github.com/google/mediapipe/blob/0.8.1/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png
 # Found in https://github.com/google/mediapipe/blob/v0.10.3/mediapipe/python/solutions/face_mesh.py
 FACE_LANDMARK_CONNECTIONS = [
     # Lips.
     (61, 146),
```

## qai_hub_models/models/mediapipe_face/perf.yaml

```diff
@@ -18,308 +18,416 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: MediaPipeFaceDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 785.0
-      throughput: 1273.8853503184714
+      inference_time: 815.0
+      throughput: 1226.993865030675
       estimated_peak_memory_range:
-        min: 12288
-        max: 1533536
+        min: 20480
+        max: 1627976
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 112
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 112
-      job_id: jegnlk6v5
+      job_id: jygz7d4zp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 839.0
-      throughput: 1191.8951132300358
+      inference_time: 843.0
+      throughput: 1186.2396204033214
       estimated_peak_memory_range:
-        min: 815104
-        max: 6910200
+        min: 806912
+        max: 6902688
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 148
-      job_id: j2p036z2p
+      job_id: jqp4wr9qg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 996.0
-      throughput: 1004.0160642570281
+      inference_time: 993.0
+      throughput: 1007.0493454179255
       estimated_peak_memory_range:
-        min: 806912
-        max: 6602536
+        min: 802816
+        max: 72047760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 147
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1gl6lveg
+        total_layers: 147
+      job_id: j1p87y1z5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.455108Z'
+    timestamp: '2024-05-20T16:35:29.659622Z'
   - torchscript_onnx_tflite:
-      inference_time: 544.0
-      throughput: 1838.235294117647
+      inference_time: 569.0
+      throughput: 1757.469244288225
       estimated_peak_memory_range:
         min: 12288
-        max: 28679584
+        max: 30017104
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 112
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 112
-      job_id: jep20ekxg
+      job_id: jmg94nxq5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 595.0
-      throughput: 1680.672268907563
+      inference_time: 592.0
+      throughput: 1689.1891891891892
       estimated_peak_memory_range:
-        min: 802816
-        max: 47837376
+        min: 12288
+        max: 47426416
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 148
-      job_id: jogk78eyp
+      job_id: jo5mzxdyp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 706.0
-      throughput: 1416.4305949008499
+      inference_time: 719.0
+      throughput: 1390.8205841446454
       estimated_peak_memory_range:
         min: 12288
-        max: 20347024
+        max: 22023952
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 147
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1p3v6jxg
+        total_layers: 147
+      job_id: jn5q2qv75
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.455176Z'
+    timestamp: '2024-05-20T16:35:29.659647Z'
   - torchscript_onnx_tflite:
-      inference_time: 784.0
-      throughput: 1275.5102040816328
+      inference_time: 778.0
+      throughput: 1285.3470437017995
       estimated_peak_memory_range:
-        min: 24576
-        max: 1602632
+        min: 12288
+        max: 1913768
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 112
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 112
-      job_id: jlpeen47p
+      job_id: jvgdv1zkg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 840.0
-      throughput: 1190.4761904761904
+      inference_time: 845.0
+      throughput: 1183.4319526627219
       estimated_peak_memory_range:
-        min: 815104
-        max: 6172048
+        min: 806912
+        max: 100815984
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 148
-      job_id: j0pxnzej5
+      job_id: jqpyd3mrp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.455221Z'
+    timestamp: '2024-05-20T16:35:29.659664Z'
+  - torchscript_onnx_qnn:
+      inference_time: 928.0
+      throughput: 1077.5862068965516
+      estimated_peak_memory_range:
+        min: 786432
+        max: 786432
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 147
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 147
+      job_id: jopry3wvg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1157.0
+      throughput: 864.304235090752
+      estimated_peak_memory_range:
+        min: 3178496
+        max: 3178496
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 147
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 147
+      job_id: jw5614wvp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 5344.0
+      throughput: 187.125748502994
+      estimated_peak_memory_range:
+        min: 9064448
+        max: 9064448
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jwgov6845
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.659686Z'
 - name: MediaPipeFaceLandmarkDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 315.0
-      throughput: 3174.6031746031745
+      inference_time: 325.0
+      throughput: 3076.923076923077
       estimated_peak_memory_range:
-        min: 24576
-        max: 1781952
+        min: 32768
+        max: 4219616
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 101
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 101
-      job_id: jopr8wvv5
+      job_id: jz5w961zp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 390.0
-      throughput: 2564.102564102564
+      inference_time: 400.0
+      throughput: 2500.0
       estimated_peak_memory_range:
-        min: 458752
-        max: 94680040
+        min: 462848
+        max: 42261400
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 107
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 107
-      job_id: j1p801qzg
+      job_id: j0px1odjg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 494.0
-      throughput: 2024.2914979757086
+      inference_time: 506.0
+      throughput: 1976.2845849802372
       estimated_peak_memory_range:
         min: 12288
-        max: 7623304
+        max: 7765592
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 106
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jw56ewyvg
+        total_layers: 106
+      job_id: jogkyx8yp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.455272Z'
+    timestamp: '2024-05-20T16:35:29.659708Z'
   - torchscript_onnx_tflite:
-      inference_time: 230.0
-      throughput: 4347.826086956522
+      inference_time: 235.0
+      throughput: 4255.31914893617
       estimated_peak_memory_range:
         min: 12288
-        max: 25090016
+        max: 25797520
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 101
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 101
-      job_id: jqpyrm1r5
+      job_id: jnp18zvkg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 285.0
-      throughput: 3508.7719298245615
+      inference_time: 282.0
+      throughput: 3546.099290780142
       estimated_peak_memory_range:
         min: 12288
-        max: 33592960
+        max: 39404800
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 107
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 107
-      job_id: jn5qev675
+      job_id: jegnevkvg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 408.0
-      throughput: 2450.9803921568628
+      inference_time: 395.0
+      throughput: 2531.6455696202534
       estimated_peak_memory_range:
         min: 12288
-        max: 15898592
+        max: 21486416
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 106
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jwgok824p
+        total_layers: 106
+      job_id: j1glkmlep
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.455318Z'
+    timestamp: '2024-05-20T16:35:29.659728Z'
   - torchscript_onnx_tflite:
-      inference_time: 326.0
-      throughput: 3067.4846625766872
+      inference_time: 306.0
+      throughput: 3267.97385620915
       estimated_peak_memory_range:
-        min: 24576
-        max: 1871744
+        min: 28672
+        max: 1867256
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 101
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 101
-      job_id: jygzo0vz5
+      job_id: jz57dr7q5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 396.0
-      throughput: 2525.252525252525
+      inference_time: 378.0
+      throughput: 2645.5026455026455
       estimated_peak_memory_range:
-        min: 458752
-        max: 81438752
+        min: 466944
+        max: 20140984
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 107
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 107
-      job_id: jo5mqlvyp
+      job_id: j2p0r062p
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.455361Z'
+    timestamp: '2024-05-20T16:35:29.659743Z'
+  - torchscript_onnx_qnn:
+      inference_time: 546.0
+      throughput: 1831.5018315018315
+      estimated_peak_memory_range:
+        min: 442368
+        max: 442368
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 106
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 106
+      job_id: jep2myex5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 529.0
+      throughput: 1890.359168241966
+      estimated_peak_memory_range:
+        min: 4382720
+        max: 4382720
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 106
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 106
+      job_id: j1p3m06xg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 2139.0
+      throughput: 467.50818139317437
+      estimated_peak_memory_range:
+        min: 5292032
+        max: 5292032
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 80
+        total_layers: 80
+      job_id: j1pvwk77g
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.659765Z'
```

## qai_hub_models/models/mediapipe_hand/export.py

```diff
@@ -130,15 +130,15 @@
         component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options
+            target_runtime, compile_options, hub_device
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub_device,
             name=f"{model_name}_{component_name}",
@@ -222,16 +222,14 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(
-        model_cls=Model, components=ALL_COMPONENTS, supports_ort=False
-    )
+    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mediapipe_hand/model.py

```diff
@@ -10,15 +10,15 @@
 import torch
 
 from qai_hub_models.models._shared.mediapipe.utils import MediaPipePyTorchAsRoot
 from qai_hub_models.utils.base_model import BaseModel, CollectionModel
 from qai_hub_models.utils.input_spec import InputSpec
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 1
+MODEL_ASSET_VERSION = 2
 
 # https://github.com/metalwhale/hand_tracking/blob/b2a650d61b4ab917a2367a05b85765b81c0564f2/run.py
 #        8   12  16  20
 #        |   |   |   |
 #        7   11  15  19
 #    4   |   |   |   |
 #    |   6   10  14  18
```

## qai_hub_models/models/mediapipe_hand/perf.yaml

```diff
@@ -18,308 +18,416 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: MediaPipeHandDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 953.0
-      throughput: 1049.3179433368311
+      inference_time: 957.0
+      throughput: 1044.932079414838
       estimated_peak_memory_range:
         min: 12288
-        max: 7786576
+        max: 2098904
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 152
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 152
-      job_id: jlpeeyd7p
+      job_id: j7gjlnq7p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1019.0
-      throughput: 981.3542688910696
+      inference_time: 1014.0
+      throughput: 986.1932938856016
       estimated_peak_memory_range:
-        min: 806912
-        max: 8813592
+        min: 12288
+        max: 21477272
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 197
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 197
-      job_id: jnp1y64kp
+      job_id: jvgdv12kg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1219.0
-      throughput: 820.3445447087777
+      inference_time: 1160.0
+      throughput: 862.0689655172414
       estimated_peak_memory_range:
         min: 12288
-        max: 19518840
+        max: 18289360
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 196
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j0pxnxkj5
+        total_layers: 196
+      job_id: jqpyd3xrp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.499757Z'
+    timestamp: '2024-05-20T16:35:29.706483Z'
   - torchscript_onnx_tflite:
-      inference_time: 679.0
-      throughput: 1472.7540500736377
+      inference_time: 680.0
+      throughput: 1470.5882352941176
       estimated_peak_memory_range:
         min: 12288
-        max: 52020064
+        max: 53739952
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 152
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 152
-      job_id: jz5w24ez5
+      job_id: jygz7dnzp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 722.0
-      throughput: 1385.0415512465374
+      inference_time: 725.0
+      throughput: 1379.3103448275863
       estimated_peak_memory_range:
         min: 802816
-        max: 57062560
+        max: 62597664
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 197
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 197
-      job_id: jz5709yqg
+      job_id: jqp4wr3qg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 838.0
-      throughput: 1193.3174224343675
+      inference_time: 868.0
+      throughput: 1152.073732718894
       estimated_peak_memory_range:
-        min: 565248
-        max: 29618560
+        min: 380928
+        max: 38582032
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 196
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jegnlk0v5
+        total_layers: 196
+      job_id: j1p87yxz5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.499831Z'
+    timestamp: '2024-05-20T16:35:29.706509Z'
   - torchscript_onnx_tflite:
-      inference_time: 959.0
-      throughput: 1042.752867570386
+      inference_time: 956.0
+      throughput: 1046.0251046025105
       estimated_peak_memory_range:
         min: 24576
-        max: 3871952
+        max: 4980488
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 152
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 152
-      job_id: j1p3vr8xg
+      job_id: jmg94ndq5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1020.0
-      throughput: 980.3921568627451
+      inference_time: 1011.0
+      throughput: 989.1196834817013
       estimated_peak_memory_range:
-        min: 806912
-        max: 7974248
+        min: 802816
+        max: 6723176
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 197
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 197
-      job_id: jnp1ym3kp
+      job_id: jopry30vg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.499901Z'
+    timestamp: '2024-05-20T16:35:29.706525Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1052.0
+      throughput: 950.5703422053232
+      estimated_peak_memory_range:
+        min: 786432
+        max: 786432
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 196
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 196
+      job_id: jo5mzx8yp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1200.0
+      throughput: 833.3333333333334
+      estimated_peak_memory_range:
+        min: 868352
+        max: 868352
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 196
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 196
+      job_id: jn5q2qy75
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 16080.0
+      throughput: 62.18905472636816
+      estimated_peak_memory_range:
+        min: 802816
+        max: 802816
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 116
+        total_layers: 116
+      job_id: jw56147vp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.706548Z'
 - name: MediaPipeHandLandmarkDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1259.0
-      throughput: 794.2811755361398
+      inference_time: 1214.0
+      throughput: 823.7232289950576
       estimated_peak_memory_range:
-        min: 24576
-        max: 1977616
+        min: 16384
+        max: 2188824
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 159
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 159
-      job_id: jygzon3z5
+      job_id: jlpevmy75
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1293.0
-      throughput: 773.3952049497293
+      inference_time: 1284.0
+      throughput: 778.816199376947
       estimated_peak_memory_range:
-        min: 638976
-        max: 10247184
+        min: 16384
+        max: 51815576
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 210
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 210
-      job_id: jvgde2xk5
+      job_id: jz57dr9q5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 54823.0
-      throughput: 18.240519489995076
+      inference_time: 1506.0
+      throughput: 664.0106241699867
       estimated_peak_memory_range:
-        min: 217088
-        max: 18000624
+        min: 12288
+        max: 42058584
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 209
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jo5mq8nyp
+        total_layers: 209
+      job_id: j2p0r0j2p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.499968Z'
+    timestamp: '2024-05-20T16:35:29.706573Z'
   - torchscript_onnx_tflite:
-      inference_time: 901.0
-      throughput: 1109.8779134295228
+      inference_time: 889.0
+      throughput: 1124.859392575928
       estimated_peak_memory_range:
         min: 12288
-        max: 56691584
+        max: 57135392
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 159
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 159
-      job_id: jmg9jdlq5
+      job_id: jz5w964zp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 963.0
-      throughput: 1038.4215991692627
+      inference_time: 948.0
+      throughput: 1054.8523206751054
       estimated_peak_memory_range:
         min: 802816
-        max: 62409504
+        max: 63945952
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 210
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 210
-      job_id: jqp4k3lqg
+      job_id: j0px1oxjg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 41069.0
-      throughput: 24.34926586963403
+      inference_time: 1099.0
+      throughput: 909.9181073703367
       estimated_peak_memory_range:
-        min: 868352
-        max: 30450496
+        min: 802816
+        max: 33494480
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 209
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jopr8w6v5
+        total_layers: 209
+      job_id: jogkyx4yp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.500029Z'
+    timestamp: '2024-05-20T16:35:29.706594Z'
   - torchscript_onnx_tflite:
-      inference_time: 1206.0
-      throughput: 829.1873963515754
+      inference_time: 1200.0
+      throughput: 833.3333333333334
       estimated_peak_memory_range:
-        min: 40960
-        max: 2078488
+        min: 12288
+        max: 2557040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 159
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 159
-      job_id: jwgok9m4p
+      job_id: jnp18z6kg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1308.0
-      throughput: 764.525993883792
+      inference_time: 1311.0
+      throughput: 762.7765064836003
       estimated_peak_memory_range:
-        min: 811008
-        max: 8238832
+        min: 815104
+        max: 52770200
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 210
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 210
-      job_id: jvgdem0k5
+      job_id: jep2mywx5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.500084Z'
+    timestamp: '2024-05-20T16:35:29.706611Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1461.0
+      throughput: 684.4626967830253
+      estimated_peak_memory_range:
+        min: 786432
+        max: 786432
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 209
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 209
+      job_id: jegnevnvg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1548.0
+      throughput: 645.9948320413437
+      estimated_peak_memory_range:
+        min: 19423232
+        max: 19423232
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 209
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 209
+      job_id: j1glkmxep
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 8524.0
+      throughput: 117.31581417175035
+      estimated_peak_memory_range:
+        min: 20221952
+        max: 20221952
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j1p3m09xg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.706632Z'
```

## qai_hub_models/models/mediapipe_pose/export.py

```diff
@@ -130,15 +130,15 @@
         component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options
+            target_runtime, compile_options, hub_device
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub_device,
             name=f"{model_name}_{component_name}",
@@ -222,16 +222,14 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(
-        model_cls=Model, components=ALL_COMPONENTS, supports_ort=False
-    )
+    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mediapipe_pose/model.py

```diff
@@ -9,15 +9,15 @@
 import torch
 
 from qai_hub_models.models._shared.mediapipe.utils import MediaPipePyTorchAsRoot
 from qai_hub_models.utils.base_model import BaseModel, CollectionModel
 from qai_hub_models.utils.input_spec import InputSpec
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 1
+MODEL_ASSET_VERSION = 2
 
 POSE_LANDMARK_CONNECTIONS = [
     (0, 1),
     (1, 2),
     (2, 3),
     (3, 7),
     (0, 4),
```

## qai_hub_models/models/mediapipe_pose/perf.yaml

```diff
@@ -18,308 +18,416 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: MediaPipePoseDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 835.0
-      throughput: 1197.6047904191616
+      inference_time: 839.0
+      throughput: 1191.8951132300358
       estimated_peak_memory_range:
-        min: 16384
-        max: 1889240
+        min: 24576
+        max: 2326784
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 107
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 107
-      job_id: j2p03642p
+      job_id: jwgov6r45
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 884.0
-      throughput: 1131.2217194570135
+      inference_time: 873.0
+      throughput: 1145.475372279496
       estimated_peak_memory_range:
-        min: 69632
-        max: 15459024
+        min: 12288
+        max: 16427488
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 140
-      job_id: j1gl6l4eg
+      job_id: jmg94nmq5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1006.0
-      throughput: 994.0357852882704
+      inference_time: 1003.0
+      throughput: 997.0089730807578
       estimated_peak_memory_range:
-        min: 16384
-        max: 9676016
+        min: 36864
+        max: 10321904
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 139
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1pv07q75
+        total_layers: 139
+      job_id: jopry3lvg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.544709Z'
+    timestamp: '2024-05-20T16:35:29.766048Z'
   - torchscript_onnx_tflite:
-      inference_time: 612.0
-      throughput: 1633.986928104575
+      inference_time: 606.0
+      throughput: 1650.1650165016501
       estimated_peak_memory_range:
         min: 16384
-        max: 40580928
+        max: 41021648
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 107
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 107
-      job_id: jogk78vyp
+      job_id: j7gjln77p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 636.0
-      throughput: 1572.3270440251572
+      inference_time: 630.0
+      throughput: 1587.3015873015872
       estimated_peak_memory_range:
-        min: 208896
-        max: 44032080
+        min: 0
+        max: 45101520
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 140
-      job_id: j1p3v6nxg
+      job_id: jvgdv13kg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 732.0
-      throughput: 1366.120218579235
+      inference_time: 769.0
+      throughput: 1300.3901170351105
       estimated_peak_memory_range:
-        min: 208896
-        max: 21601008
+        min: 212992
+        max: 30386624
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 139
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jlpeeyo7p
+        total_layers: 139
+      job_id: jqpyd3orp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.544766Z'
+    timestamp: '2024-05-20T16:35:29.766075Z'
   - torchscript_onnx_tflite:
-      inference_time: 845.0
-      throughput: 1183.4319526627219
+      inference_time: 829.0
+      throughput: 1206.2726176115802
       estimated_peak_memory_range:
-        min: 32768
-        max: 1538160
+        min: 77824
+        max: 1838752
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 107
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 107
-      job_id: j2p03xdep
+      job_id: jygz7dmzp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 886.0
-      throughput: 1128.6681715575621
+      inference_time: 875.0
+      throughput: 1142.857142857143
       estimated_peak_memory_range:
-        min: 12288
-        max: 104292296
+        min: 229376
+        max: 5314120
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 140
-      job_id: j1pv0nem5
+      job_id: jo5mzxmyp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.544811Z'
+    timestamp: '2024-05-20T16:35:29.766091Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1047.0
+      throughput: 955.1098376313277
+      estimated_peak_memory_range:
+        min: 540672
+        max: 540672
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 139
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 139
+      job_id: jqp4wr1qg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1080.0
+      throughput: 925.925925925926
+      estimated_peak_memory_range:
+        min: 1073152
+        max: 1073152
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 139
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 139
+      job_id: j1p87yez5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 15947.0
+      throughput: 62.70771932024832
+      estimated_peak_memory_range:
+        min: 26939392
+        max: 26939392
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 81
+        total_layers: 81
+      job_id: jn5q2ql75
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.766117Z'
 - name: MediaPipePoseLandmarkDetector
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1206.0
-      throughput: 829.1873963515754
+      inference_time: 1204.0
+      throughput: 830.5647840531561
       estimated_peak_memory_range:
-        min: 16384
-        max: 2448848
+        min: 24576
+        max: 2528368
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 230
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 230
-      job_id: j1p8012zg
+      job_id: j1pvwkd7g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1297.0
-      throughput: 771.0100231303007
+      inference_time: 1311.0
+      throughput: 762.7765064836003
       estimated_peak_memory_range:
-        min: 12288
-        max: 15533680
+        min: 16384
+        max: 13548072
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 306
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 306
-      job_id: jw56ew2vg
+      job_id: jnp18zjkg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 106535.0
-      throughput: 9.386586567794621
+      inference_time: 1658.0
+      throughput: 603.1363088057901
       estimated_peak_memory_range:
-        min: 102400
-        max: 26214168
+        min: 53248
+        max: 26730224
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 304
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j7gjzqd75
+        total_layers: 304
+      job_id: jep2myrx5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.544899Z'
+    timestamp: '2024-05-20T16:35:29.766142Z'
   - torchscript_onnx_tflite:
-      inference_time: 880.0
-      throughput: 1136.3636363636363
+      inference_time: 864.0
+      throughput: 1157.4074074074074
       estimated_peak_memory_range:
-        min: 16384
-        max: 87924496
+        min: 20480
+        max: 88312288
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 230
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 230
-      job_id: jn5qev075
+      job_id: jlpevmz75
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 964.0
-      throughput: 1037.344398340249
+      inference_time: 948.0
+      throughput: 1054.8523206751054
       estimated_peak_memory_range:
         min: 802816
-        max: 83648384
+        max: 89559840
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 306
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 306
-      job_id: jwgok8z4p
+      job_id: jz57dr4q5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 82694.0
-      throughput: 12.092775775751566
+      inference_time: 1253.0
+      throughput: 798.0845969672786
       estimated_peak_memory_range:
-        min: 819200
-        max: 35448288
+        min: 454656
+        max: 38605792
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 304
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jygzon2z5
+        total_layers: 304
+      job_id: j2p0r0m2p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.544985Z'
+    timestamp: '2024-05-20T16:35:29.766163Z'
   - torchscript_onnx_tflite:
-      inference_time: 1247.0
-      throughput: 801.924619085806
+      inference_time: 1244.0
+      throughput: 803.8585209003215
       estimated_peak_memory_range:
-        min: 12288
-        max: 2817072
+        min: 86016
+        max: 3237392
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 230
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 230
-      job_id: j1p80k68g
+      job_id: jz5w967zp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1291.0
-      throughput: 774.5933384972889
+      inference_time: 1309.0
+      throughput: 763.9419404125287
       estimated_peak_memory_range:
-        min: 24576
-        max: 13908424
+        min: 12288
+        max: 14098200
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 306
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 306
-      job_id: j7gjz8o85
+      job_id: jegnevzvg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.545055Z'
+    timestamp: '2024-05-20T16:35:29.766179Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1501.0
+      throughput: 666.2225183211193
+      estimated_peak_memory_range:
+        min: 786432
+        max: 786432
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 305
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 305
+      job_id: j0px1o4jg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1633.0
+      throughput: 612.369871402327
+      estimated_peak_memory_range:
+        min: 7917568
+        max: 7917568
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 304
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 304
+      job_id: jogkyx2yp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 6059.0
+      throughput: 165.0437365901964
+      estimated_peak_memory_range:
+        min: 20336640
+        max: 20336640
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j1glkmyep
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.766201Z'
```

## qai_hub_models/models/mediapipe_selfie/export.py

```diff
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mediapipe_selfie/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: MediaPipe-Selfie-Segmentation
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 792.0
-      throughput: 1262.6262626262626
+      inference_time: 807.0
+      throughput: 1239.1573729863692
       estimated_peak_memory_range:
         min: 12288
-        max: 4536656
+        max: 1954960
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 118
-      job_id: jnp1y62kp
+      job_id: jw56148vp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 773.0
-      throughput: 1293.6610608020699
+      inference_time: 787.0
+      throughput: 1270.6480304955528
       estimated_peak_memory_range:
-        min: 32768
-        max: 18516080
+        min: 28672
+        max: 13500824
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 138
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 138
-      job_id: jz57092qg
+      job_id: j1pvwkl7g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 164651.0
-      throughput: 6.073452332509368
+      inference_time: 1327.0
+      throughput: 753.5795026375282
       estimated_peak_memory_range:
-        min: 1437696
-        max: 5932024
+        min: 802816
+        max: 5487496
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j0pxnx9j5
+        total_layers: 140
+      job_id: jz5w96lzp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.589824Z'
+    timestamp: '2024-05-20T16:35:29.823981Z'
   - torchscript_onnx_tflite:
-      inference_time: 536.0
-      throughput: 1865.6716417910447
+      inference_time: 542.0
+      throughput: 1845.018450184502
       estimated_peak_memory_range:
-        min: 12288
-        max: 23055696
+        min: 16384
+        max: 23610032
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 118
-      job_id: jvgde2nk5
+      job_id: j1p3m0zxg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 525.0
-      throughput: 1904.7619047619048
+      inference_time: 510.0
+      throughput: 1960.7843137254902
       estimated_peak_memory_range:
         min: 176128
-        max: 41755712
+        max: 41845584
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 138
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 138
-      job_id: jqp4k3nqg
+      job_id: j7gjlnr7p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 121169.0
-      throughput: 8.252935981975588
+      inference_time: 945.0
+      throughput: 1058.2010582010582
       estimated_peak_memory_range:
         min: 12288
-        max: 18735968
+        max: 20917104
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 140
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jo5mq8eyp
+        total_layers: 140
+      job_id: jmg94nzq5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.589884Z'
+    timestamp: '2024-05-20T16:35:29.824007Z'
   - torchscript_onnx_tflite:
-      inference_time: 785.0
-      throughput: 1273.8853503184714
+      inference_time: 809.0
+      throughput: 1236.0939431396787
       estimated_peak_memory_range:
-        min: 12288
-        max: 2039720
+        min: 20480
+        max: 1607472
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 118
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 118
-      job_id: j0pxnzd95
+      job_id: jwgov6l45
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 772.0
-      throughput: 1295.3367875647668
+      inference_time: 787.0
+      throughput: 1270.6480304955528
       estimated_peak_memory_range:
-        min: 819200
-        max: 8273816
+        min: 806912
+        max: 41288280
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 138
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 138
-      job_id: jep20zvmg
+      job_id: jygz7dlzp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.589938Z'
+    timestamp: '2024-05-20T16:35:29.824029Z'
+  - torchscript_onnx_qnn:
+      inference_time: 945.0
+      throughput: 1058.2010582010582
+      estimated_peak_memory_range:
+        min: 786432
+        max: 786432
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 138
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 138
+      job_id: jlpevm775
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1395.0
+      throughput: 716.8458781362007
+      estimated_peak_memory_range:
+        min: 2465792
+        max: 2465792
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 140
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 140
+      job_id: jnp18znkg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 4582.0
+      throughput: 218.2453077258839
+      estimated_peak_memory_range:
+        min: 16928768
+        max: 16928768
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 110
+        total_layers: 110
+      job_id: jvgdv1dkg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.824052Z'
```

## qai_hub_models/models/mnasnet05/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mnasnet05/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: MNASNet05
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 800.0
-      throughput: 1250.0
+      inference_time: 771.0
+      throughput: 1297.0168612191958
       estimated_peak_memory_range:
-        min: 16384
-        max: 1867832
+        min: 49152
+        max: 2163152
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 71
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 71
-      job_id: jopr8w8v5
+      job_id: jqpyd384p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 848.0
-      throughput: 1179.245283018868
+      inference_time: 824.0
+      throughput: 1213.5922330097087
       estimated_peak_memory_range:
-        min: 630784
-        max: 4926760
+        min: 16384
+        max: 45567712
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 103
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 103
-      job_id: jqpyrmrr5
+      job_id: jogkyx6op
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 990.0
-      throughput: 1010.10101010101
+      inference_time: 768.0
+      throughput: 1302.0833333333333
       estimated_peak_memory_range:
-        min: 12288
-        max: 21275160
+        min: 16384
+        max: 18880896
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 104
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1p8010zg
+        total_layers: 104
+      job_id: j1p3m0ozg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.613841Z'
+    timestamp: '2024-05-20T16:35:29.865693Z'
   - torchscript_onnx_tflite:
-      inference_time: 530.0
-      throughput: 1886.7924528301887
+      inference_time: 522.0
+      throughput: 1915.7088122605364
       estimated_peak_memory_range:
-        min: 12288
-        max: 45612800
+        min: 16384
+        max: 46214320
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 71
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 71
-      job_id: jep20e0xg
+      job_id: j2p0r0oep
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 565.0
-      throughput: 1769.9115044247787
+      inference_time: 562.0
+      throughput: 1779.3594306049822
       estimated_peak_memory_range:
         min: 0
-        max: 41195552
+        max: 38662336
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 103
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 103
-      job_id: j2p03632p
+      job_id: jn5q2q4m5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 641.0
-      throughput: 1560.0624024960998
+      inference_time: 531.0
+      throughput: 1883.2391713747645
       estimated_peak_memory_range:
-        min: 24576
-        max: 21468016
+        min: 634880
+        max: 26749664
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 104
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jogk787yp
+        total_layers: 104
+      job_id: jwgov6dd5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.613900Z'
+    timestamp: '2024-05-20T16:35:29.865722Z'
   - torchscript_onnx_tflite:
-      inference_time: 799.0
-      throughput: 1251.5644555694619
+      inference_time: 774.0
+      throughput: 1291.9896640826873
       estimated_peak_memory_range:
-        min: 20480
-        max: 1900528
+        min: 28672
+        max: 1977952
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 71
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 71
-      job_id: j1p3vrwzg
+      job_id: j1p87yj85
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 852.0
-      throughput: 1173.7089201877934
+      inference_time: 834.0
+      throughput: 1199.0407673860911
       estimated_peak_memory_range:
-        min: 0
-        max: 47875160
+        min: 16384
+        max: 24694288
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 103
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 103
-      job_id: jlpeenl0p
+      job_id: jw5614o7p
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.613941Z'
+    timestamp: '2024-05-20T16:35:29.865740Z'
+  - torchscript_onnx_qnn:
+      inference_time: 952.0
+      throughput: 1050.420168067227
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 103
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 103
+      job_id: j1glkmwlp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 816.0
+      throughput: 1225.4901960784314
+      estimated_peak_memory_range:
+        min: 15839232
+        max: 15839232
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 104
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 104
+      job_id: j1pvwk2mg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 2632.0
+      throughput: 379.9392097264438
+      estimated_peak_memory_range:
+        min: 11706368
+        max: 11706368
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j7gjln38p
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.865762Z'
```

## qai_hub_models/models/mobilenet_v2/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mobilenet_v2/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: MobileNet-v2
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 974.0
-      throughput: 1026.694045174538
+      inference_time: 935.0
+      throughput: 1069.51871657754
       estimated_peak_memory_range:
         min: 20480
-        max: 1954912
+        max: 1805232
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 72
-      job_id: j1gl6l6eg
+      job_id: jlpevm605
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1281.0
-      throughput: 780.64012490242
+      inference_time: 1268.0
+      throughput: 788.6435331230284
       estimated_peak_memory_range:
         min: 622592
-        max: 7823048
+        max: 52139528
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 105
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 105
-      job_id: j1p3v6vxg
+      job_id: jmg94nov5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1110.0
-      throughput: 900.9009009009009
+      inference_time: 926.0
+      throughput: 1079.913606911447
       estimated_peak_memory_range:
         min: 12288
-        max: 31867536
+        max: 26577912
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 105
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1pv07075
+        total_layers: 105
+      job_id: jqp4wr4lg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.638108Z'
+    timestamp: '2024-05-20T16:35:29.896619Z'
   - torchscript_onnx_tflite:
-      inference_time: 651.0
-      throughput: 1536.0983102918588
+      inference_time: 622.0
+      throughput: 1607.717041800643
       estimated_peak_memory_range:
-        min: 16384
-        max: 56986240
+        min: 12288
+        max: 56265456
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 72
-      job_id: jw56ewevg
+      job_id: jygz7dz6p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 836.0
-      throughput: 1196.1722488038276
+      inference_time: 828.0
+      throughput: 1207.729468599034
       estimated_peak_memory_range:
         min: 618496
-        max: 42487872
+        max: 39673920
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 105
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 105
-      job_id: jwgok8k4p
+      job_id: jnp18zolg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 750.0
-      throughput: 1333.3333333333333
+      inference_time: 638.0
+      throughput: 1567.398119122257
       estimated_peak_memory_range:
-        min: 12288
-        max: 22319216
+        min: 471040
+        max: 25361728
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 105
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j7gjzqz75
+        total_layers: 105
+      job_id: j0px1or9g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.638177Z'
+    timestamp: '2024-05-20T16:35:29.896647Z'
   - torchscript_onnx_tflite:
-      inference_time: 957.0
-      throughput: 1044.932079414838
+      inference_time: 941.0
+      throughput: 1062.6992561105208
       estimated_peak_memory_range:
-        min: 57344
-        max: 1611720
+        min: 28672
+        max: 1470792
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 72
-      job_id: jvgdemzl5
+      job_id: jz5w96yjp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1265.0
-      throughput: 790.5138339920949
+      inference_time: 1269.0
+      throughput: 788.0220646178093
       estimated_peak_memory_range:
-        min: 618496
-        max: 128931224
+        min: 16384
+        max: 143728912
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 105
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 105
-      job_id: jo5mql8qp
+      job_id: jz57drnr5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.638216Z'
+    timestamp: '2024-05-20T16:35:29.896664Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1516.0
+      throughput: 659.6306068601583
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 105
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 105
+      job_id: jvgdv16lg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 994.0
+      throughput: 1006.0362173038229
+      estimated_peak_memory_range:
+        min: 17502208
+        max: 17502208
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 105
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 105
+      job_id: jo5mzxkqp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 8063.0
+      throughput: 124.0233163834801
+      estimated_peak_memory_range:
+        min: 798720
+        max: 798720
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 57
+        total_layers: 57
+      job_id: jegnevqmg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.896687Z'
```

## qai_hub_models/models/mobilenet_v2_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,20 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -203,14 +214,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mobilenet_v2_quantized/perf.yaml

```diff
@@ -22,240 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: MobileNet-v2-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 302.0
-      throughput: 3311.2582781456954
+      inference_time: 295.0
+      throughput: 3389.830508474576
       estimated_peak_memory_range:
-        min: 16384
-        max: 1568424
+        min: 40960
+        max: 6698304
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 72
-      job_id: jygzonoz5
+      job_id: jopry3deg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 666.0
-      throughput: 1501.5015015015015
+      inference_time: 654.0
+      throughput: 1529.051987767584
       estimated_peak_memory_range:
-        min: 12288
-        max: 75287400
+        min: 172032
+        max: 5185880
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 71
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 71
-      job_id: jmg9jdjq5
+      job_id: j2p0r09ep
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 897.0
-      throughput: 1114.8272017837235
+      inference_time: 634.0
+      throughput: 1577.2870662460568
       estimated_peak_memory_range:
-        min: 12288
-        max: 146664848
+        min: 200704
+        max: 21639208
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 77
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jvgde2ek5
+        total_layers: 77
+      job_id: j1glkm8lp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.662420Z'
+    timestamp: '2024-05-20T16:35:29.927306Z'
   - torchscript_onnx_tflite:
-      inference_time: 233.0
-      throughput: 4291.845493562232
+      inference_time: 238.0
+      throughput: 4201.680672268908
       estimated_peak_memory_range:
         min: 12288
-        max: 37162256
+        max: 37430768
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 72
-      job_id: jz5w242z5
+      job_id: jep2mydm5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 480.0
-      throughput: 2083.3333333333335
+      inference_time: 474.0
+      throughput: 2109.7046413502107
       estimated_peak_memory_range:
-        min: 159744
-        max: 36918192
+        min: 163840
+        max: 38345472
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 71
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 71
-      job_id: jnp1y6ykp
+      job_id: j1p87yr85
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 644.0
-      throughput: 1552.7950310559006
+      inference_time: 463.0
+      throughput: 2159.827213822894
       estimated_peak_memory_range:
         min: 0
-        max: 18572416
+        max: 22362560
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 77
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jz57090qg
+        total_layers: 77
+      job_id: jw5614m7p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.662476Z'
+    timestamp: '2024-05-20T16:35:29.927332Z'
   - torchscript_onnx_tflite:
-      inference_time: 949.0
-      throughput: 1053.740779768177
+      inference_time: 296.0
+      throughput: 3378.3783783783783
       estimated_peak_memory_range:
         min: 12288
-        max: 23229040
+        max: 1719624
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 74
+        layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 74
-      job_id: jogk7k8op
+        total_layers: 72
+      job_id: jqpyd324p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 659.0
+      throughput: 1517.4506828528072
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 20480
+        max: 75350840
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 71
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jwgok98dp
-      job_status: Failed
-    torchscript_onnx_ort:
-      inference_time: 6507.0
-      throughput: 153.68065160596282
+        total_layers: 71
+      job_id: jn5q2q1m5
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:29.927350Z'
+  - torchscript_onnx_tflite:
+      inference_time: 853.0
+      throughput: 1172.3329425556858
       estimated_peak_memory_range:
-        min: 335872
-        max: 43247408
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 12288
+        max: 23360768
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 72
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 72
+      job_id: j0pxyrqlg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1400.0
+      throughput: 714.2857142857143
+      estimated_peak_memory_range:
+        min: 0
+        max: 34410432
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 71
         layers_on_gpu: 0
-        layers_on_cpu: 84
-        total_layers: 84
-      job_id: jqp4k3kqg
+        layers_on_cpu: 0
+        total_layers: 71
+      job_id: j2p0l9wnp
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:32.662525Z'
+    timestamp: '2024-05-20T16:35:29.927366Z'
   - torchscript_onnx_tflite:
-      inference_time: 7442.0
-      throughput: 134.37248051599033
+      inference_time: 7603.0
+      throughput: 131.5270288044193
       estimated_peak_memory_range:
-        min: 12288
-        max: 11587968
+        min: 20480
+        max: 11376824
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 72
+        layers_on_npu: 70
         layers_on_gpu: 2
         layers_on_cpu: 0
-        total_layers: 74
-      job_id: j1gl2wkep
+        total_layers: 72
+      job_id: jo5m3k79g
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:32.662544Z'
-  - torchscript_onnx_tflite:
-      inference_time: 325.0
-      throughput: 3076.923076923077
+    timestamp: '2024-05-20T16:35:29.927377Z'
+  - torchscript_onnx_qnn:
+      inference_time: 762.0
+      throughput: 1312.3359580052493
       estimated_peak_memory_range:
-        min: 20480
-        max: 1768808
+        min: 573440
+        max: 573440
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 74
+        layers_on_npu: 71
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 74
-      job_id: j1p80k18g
+        total_layers: 71
+      job_id: jogkyx0op
       job_status: Passed
-    torchscript_onnx_qnn:
-      inference_time: 695.0
-      throughput: 1438.8489208633093
+    torchscript_onnx_ort:
+      inference_time: 677.0
+      throughput: 1477.1048744460857
       estimated_peak_memory_range:
-        min: 20480
-        max: 131789128
+        min: 19963904
+        max: 19963904
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 73
+        layers_on_npu: 77
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 73
-      job_id: jw56e0w7g
+        total_layers: 77
+      job_id: j1p3m07zg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 43191.0
+      throughput: 23.15297168391563
+      estimated_peak_memory_range:
+        min: 20062208
+        max: 20062208
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jwgov6wd5
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.662578Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.927401Z'
```

## qai_hub_models/models/mobilenet_v3_large/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mobilenet_v3_large/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: MobileNet-v3-Large
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1022.0
-      throughput: 978.4735812133073
+      inference_time: 1002.0
+      throughput: 998.003992015968
       estimated_peak_memory_range:
-        min: 16384
-        max: 1643944
+        min: 12288
+        max: 1963520
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 136
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 136
-      job_id: j0pxnxnj5
+      job_id: j1pvwkmmg
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 3790.0
-      throughput: 263.85224274406335
+    torchscript_onnx_qnn:
+      inference_time: 1037.0
+      throughput: 964.3201542912246
       estimated_peak_memory_range:
         min: 0
-        max: 28283024
+        max: 68891008
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 9
+        layers_on_npu: 144
         layers_on_gpu: 0
-        layers_on_cpu: 8
-        total_layers: 17
-      job_id: jegnlkmv5
+        layers_on_cpu: 0
+        total_layers: 144
+      job_id: jygz7dy6p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1038.0
+      throughput: 963.3911368015414
+      estimated_peak_memory_range:
+        min: 12288
+        max: 87795632
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 162
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 162
+      job_id: jvgdv14lg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.697406Z'
+    timestamp: '2024-05-20T16:35:29.966635Z'
   - torchscript_onnx_tflite:
-      inference_time: 691.0
-      throughput: 1447.178002894356
+      inference_time: 702.0
+      throughput: 1424.5014245014245
       estimated_peak_memory_range:
-        min: 16384
-        max: 61060464
+        min: 12288
+        max: 61294288
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 136
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 136
-      job_id: jo5mq8qyp
+      job_id: j7gjlny8p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 712.0
+      throughput: 1404.4943820224719
+      estimated_peak_memory_range:
+        min: 0
+        max: 51701120
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 144
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 144
+      job_id: jz5w96zjp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2766.0
-      throughput: 361.53289949385396
+      inference_time: 719.0
+      throughput: 1390.8205841446454
       estimated_peak_memory_range:
-        min: 12288
-        max: 25734304
+        min: 618496
+        max: 32246576
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 9
+        layers_on_npu: 162
         layers_on_gpu: 0
-        layers_on_cpu: 8
-        total_layers: 17
-      job_id: jopr8w2v5
+        layers_on_cpu: 0
+        total_layers: 162
+      job_id: jz57dr8r5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.697460Z'
+    timestamp: '2024-05-20T16:35:29.966661Z'
   - torchscript_onnx_tflite:
-      inference_time: 1022.0
-      throughput: 978.4735812133073
+      inference_time: 1001.0
+      throughput: 999.000999000999
       estimated_peak_memory_range:
-        min: 24576
-        max: 1929640
+        min: 20480
+        max: 1880160
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 136
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 136
-      job_id: jz5w2r4j5
+      job_id: jlpevmx05
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1041.0
+      throughput: 960.6147934678194
+      estimated_peak_memory_range:
+        min: 20480
+        max: 47502336
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 144
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 144
+      job_id: jnp18z1lg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.697490Z'
+    timestamp: '2024-05-20T16:35:29.966678Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1207.0
+      throughput: 828.5004142502071
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 144
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 144
+      job_id: jmg94n2v5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1108.0
+      throughput: 902.5270758122743
+      estimated_peak_memory_range:
+        min: 54001664
+        max: 54001664
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 162
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 162
+      job_id: jqp4wr2lg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 9897.0
+      throughput: 101.0407194099222
+      estimated_peak_memory_range:
+        min: 1593344
+        max: 1593344
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 126
+        total_layers: 126
+      job_id: j0px1oz9g
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.966700Z'
```

## qai_hub_models/models/mobilenet_v3_large_quantized/export.py

```diff
@@ -26,14 +26,15 @@
     print_inference_metrics,
     print_on_target_demo_cmd,
     print_profile_metrics_from_job,
 )
 from qai_hub_models.utils.qai_hub_helpers import (
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
+    transpose_channel_first_to_last,
 )
 from qai_hub_models.utils.qnn_helpers import get_qnn_inputs
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
     chipset: Optional[str] = None,
@@ -118,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -161,14 +169,22 @@
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
+        # Convert inputs from channel first to channel last
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
+        )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
         )
@@ -196,14 +212,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml

```diff
@@ -22,180 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: MobileNet-v3-Large-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 585.0
-      throughput: 1709.4017094017095
+      inference_time: 357.0
+      throughput: 2801.1204481792715
+      estimated_peak_memory_range:
+        min: 16384
+        max: 2663832
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 135
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 135
+      job_id: jo5mzxlqp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 623.0
+      throughput: 1605.1364365971108
       estimated_peak_memory_range:
         min: 12288
-        max: 1681920
+        max: 7124224
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 136
+        layers_on_npu: 126
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 136
-      job_id: jqpyrmjr5
+        total_layers: 126
+      job_id: jep2myzm5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 6430.0
-      throughput: 155.52099533437013
+      inference_time: 5302.0
+      throughput: 188.6080724254998
       estimated_peak_memory_range:
-        min: 15818752
-        max: 29085400
+        min: 15572992
+        max: 31527200
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 9
+        layers_on_npu: 150
         layers_on_gpu: 0
-        layers_on_cpu: 8
-        total_layers: 17
-      job_id: j1p801mzg
+        layers_on_cpu: 24
+        total_layers: 174
+      job_id: jogkyxkop
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.715082Z'
+    timestamp: '2024-05-20T16:35:29.996961Z'
   - torchscript_onnx_tflite:
-      inference_time: 413.0
-      throughput: 2421.3075060532688
+      inference_time: 277.0
+      throughput: 3610.1083032490974
       estimated_peak_memory_range:
         min: 12288
-        max: 46829184
+        max: 47595728
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 136
+        layers_on_npu: 135
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 136
-      job_id: j2p03622p
+        total_layers: 135
+      job_id: jegnevwmg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 452.0
+      throughput: 2212.3893805309735
+      estimated_peak_memory_range:
+        min: 0
+        max: 45251296
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: jqpyd3y4p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 4730.0
-      throughput: 211.41649048625794
+      inference_time: 4131.0
+      throughput: 242.0721374969741
       estimated_peak_memory_range:
-        min: 21893120
-        max: 53274160
+        min: 21827584
+        max: 58653840
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 9
+        layers_on_npu: 150
         layers_on_gpu: 0
-        layers_on_cpu: 8
-        total_layers: 17
-      job_id: jogk78qyp
+        layers_on_cpu: 24
+        total_layers: 174
+      job_id: jn5q2qdm5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.715131Z'
+    timestamp: '2024-05-20T16:35:29.996991Z'
   - torchscript_onnx_tflite:
-      inference_time: 1547.0
-      throughput: 646.4124111182934
+      inference_time: 351.0
+      throughput: 2849.002849002849
       estimated_peak_memory_range:
         min: 12288
-        max: 28081232
+        max: 1686776
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 138
+        layers_on_npu: 135
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 138
-      job_id: jegnlwnm5
+        total_layers: 135
+      job_id: jopry37eg
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 10400.0
-      throughput: 96.15384615384616
+    torchscript_onnx_qnn:
+      inference_time: 624.0
+      throughput: 1602.5641025641025
       estimated_peak_memory_range:
-        min: 11681792
-        max: 108762160
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 24576
+        max: 15252232
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 126
         layers_on_gpu: 0
-        layers_on_cpu: 218
-        total_layers: 218
-      job_id: jn5qevr75
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: j1p87yk85
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
+      name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:32.715190Z'
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:29.997008Z'
   - torchscript_onnx_tflite:
-      inference_time: 5306.0
-      throughput: 188.46588767433096
+      inference_time: 1189.0
+      throughput: 841.0428931875525
       estimated_peak_memory_range:
-        min: 40960
-        max: 2748408
+        min: 12288
+        max: 28245440
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 138
+        layers_on_npu: 135
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 138
-      job_id: jw56zo1vg
+        total_layers: 135
+      job_id: jqp4v4z1p
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jqpy629l5
+      job_status: Failed
     reference_device_info:
-      name: RB5 (Proxy)
+      name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:32.715215Z'
+      chipset: Qcs6490
+    timestamp: '2024-05-20T16:35:29.997028Z'
   - torchscript_onnx_tflite:
-      inference_time: 667.0
-      throughput: 1499.2503748125937
+      inference_time: 6580.0
+      throughput: 151.9756838905775
       estimated_peak_memory_range:
-        min: 40960
-        max: 1853728
+        min: 45056
+        max: 10222544
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 138
+        layers_on_npu: 135
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 138
-      job_id: jo5mqlmqp
+        total_layers: 135
+      job_id: j0pxyrwlg
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
+      name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.715252Z'
+      chipset: Qcs8250
+    timestamp: '2024-05-20T16:35:29.997039Z'
+  - torchscript_onnx_qnn:
+      inference_time: 705.0
+      throughput: 1418.4397163120568
+      estimated_peak_memory_range:
+        min: 520192
+        max: 520192
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: j2p0r0xep
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 4772.0
+      throughput: 209.55574182732607
+      estimated_peak_memory_range:
+        min: 25464832
+        max: 25464832
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 150
+        layers_on_gpu: 0
+        layers_on_cpu: 24
+        total_layers: 174
+      job_id: j1glkmqlp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 28805.0
+      throughput: 34.71619510501649
+      estimated_peak_memory_range:
+        min: 20099072
+        max: 20099072
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jw561407p
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:29.997063Z'
```

## qai_hub_models/models/mobilenet_v3_small/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/mobilenet_v3_small/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: MobileNet-v3-Small
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 840.0
-      throughput: 1190.4761904761904
+      inference_time: 834.0
+      throughput: 1199.0407673860911
       estimated_peak_memory_range:
-        min: 12288
-        max: 1842512
+        min: 16384
+        max: 1577560
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 124
-      job_id: j1gl6l2eg
+      job_id: j1p3m0rzg
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 3404.0
-      throughput: 293.7720329024677
+    torchscript_onnx_qnn:
+      inference_time: 866.0
+      throughput: 1154.7344110854503
       estimated_peak_memory_range:
         min: 16384
-        max: 13250040
+        max: 24077256
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: j7gjln88p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 813.0
+      throughput: 1230.0123001230013
+      estimated_peak_memory_range:
+        min: 12288
+        max: 34364368
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 10
+        layers_on_npu: 146
         layers_on_gpu: 0
-        layers_on_cpu: 9
-        total_layers: 19
-      job_id: j1p3v61xg
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jmg94nqv5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.742071Z'
+    timestamp: '2024-05-20T16:35:30.036067Z'
   - torchscript_onnx_tflite:
-      inference_time: 547.0
-      throughput: 1828.1535648994516
+      inference_time: 545.0
+      throughput: 1834.8623853211009
       estimated_peak_memory_range:
-        min: 12288
-        max: 40731056
+        min: 20480
+        max: 41085008
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 124
-      job_id: jw56ewzvg
+      job_id: jwgov69d5
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 3006.0
-      throughput: 332.667997338656
+    torchscript_onnx_qnn:
+      inference_time: 582.0
+      throughput: 1718.213058419244
       estimated_peak_memory_range:
         min: 12288
-        max: 27095152
+        max: 46524832
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 10
+        layers_on_npu: 126
         layers_on_gpu: 0
-        layers_on_cpu: 9
-        total_layers: 19
-      job_id: jwgok8n4p
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: jlpevmn05
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 560.0
+      throughput: 1785.7142857142858
+      estimated_peak_memory_range:
+        min: 618496
+        max: 27970128
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jnp18zmlg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.742122Z'
+    timestamp: '2024-05-20T16:35:30.036094Z'
   - torchscript_onnx_tflite:
-      inference_time: 844.0
-      throughput: 1184.8341232227488
+      inference_time: 826.0
+      throughput: 1210.6537530266344
       estimated_peak_memory_range:
-        min: 12288
-        max: 1902856
+        min: 24576
+        max: 1999704
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 124
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 124
-      job_id: j1gl6qxlg
+      job_id: j1pvwknmg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 866.0
+      throughput: 1154.7344110854503
+      estimated_peak_memory_range:
+        min: 0
+        max: 25356816
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: jz5w96rjp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.742147Z'
+    timestamp: '2024-05-20T16:35:30.036111Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1032.0
+      throughput: 968.9922480620155
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: jygz7d06p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 908.0
+      throughput: 1101.3215859030836
+      estimated_peak_memory_range:
+        min: 3018752
+        max: 3018752
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jvgdv1mlg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 4962.0
+      throughput: 201.53164046755342
+      estimated_peak_memory_range:
+        min: 1437696
+        max: 1437696
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 114
+        total_layers: 114
+      job_id: jz57dr1r5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.036134Z'
```

## qai_hub_models/models/openai_clip/export.py

```diff
@@ -130,15 +130,15 @@
         component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options
+            target_runtime, compile_options, hub_device
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub_device,
             name=f"{model_name}_{component_name}",
@@ -222,19 +222,14 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(
-        model_cls=Model,
-        components=ALL_COMPONENTS,
-        supports_qnn=False,
-        supports_ort=False,
-    )
+    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/openai_clip/perf.yaml

```diff
@@ -18,218 +18,416 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: CLIPTextEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 15395.0
-      throughput: 64.95615459564793
+      inference_time: 13312.0
+      throughput: 75.1201923076923
       estimated_peak_memory_range:
-        min: 32768
-        max: 2875584
+        min: 20480
+        max: 2971744
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 574
         layers_on_gpu: 0
         layers_on_cpu: 2
         total_layers: 576
-      job_id: j7gjzq275
+      job_id: jqp4wr6lg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 7826.0
+      throughput: 127.77919754663941
+      estimated_peak_memory_range:
+        min: 45056
+        max: 25299672
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 377
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 377
+      job_id: jqpyd3k4p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 33201.0
-      throughput: 30.119574711605072
+      inference_time: 31411.0
+      throughput: 31.83598102575531
       estimated_peak_memory_range:
-        min: 40960
-        max: 328459688
+        min: 16384
+        max: 325180960
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 389
         layers_on_gpu: 0
-        layers_on_cpu: 1
-        total_layers: 2
-      job_id: jmg9jdyq5
+        layers_on_cpu: 0
+        total_layers: 389
+      job_id: jwgov67d5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.759891Z'
+    timestamp: '2024-05-20T16:35:30.066622Z'
   - torchscript_onnx_tflite:
-      inference_time: 11237.0
-      throughput: 88.99172376968941
+      inference_time: 9410.0
+      throughput: 106.26992561105207
       estimated_peak_memory_range:
         min: 16384
-        max: 219358080
+        max: 211565584
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 574
         layers_on_gpu: 0
         layers_on_cpu: 2
         total_layers: 576
-      job_id: jygzonjz5
+      job_id: jo5mzx1qp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 5494.0
+      throughput: 182.01674554058974
+      estimated_peak_memory_range:
+        min: 0
+        max: 141191120
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 377
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 377
+      job_id: j1p87yd85
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 23967.0
-      throughput: 41.7240372178412
+      inference_time: 22506.0
+      throughput: 44.43259575224385
       estimated_peak_memory_range:
         min: 36864
-        max: 216279616
+        max: 184881664
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 389
         layers_on_gpu: 0
-        layers_on_cpu: 1
-        total_layers: 2
-      job_id: jvgde2qk5
+        layers_on_cpu: 0
+        total_layers: 389
+      job_id: j7gjln68p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.759974Z'
+    timestamp: '2024-05-20T16:35:30.066650Z'
   - torchscript_onnx_tflite:
-      inference_time: 15367.0
-      throughput: 65.07451031430989
+      inference_time: 13176.0
+      throughput: 75.89556769884639
       estimated_peak_memory_range:
-        min: 49152
-        max: 3357800
+        min: 16384
+        max: 3268096
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 574
         layers_on_gpu: 0
         layers_on_cpu: 2
         total_layers: 576
-      job_id: j7gjz8785
+      job_id: jopry3meg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 7787.0
+      throughput: 128.4191601386927
+      estimated_peak_memory_range:
+        min: 32768
+        max: 17390072
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 377
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 377
+      job_id: jw561497p
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.760069Z'
+    timestamp: '2024-05-20T16:35:30.066668Z'
+  - torchscript_onnx_qnn:
+      inference_time: 8463.0
+      throughput: 118.16140848398913
+      estimated_peak_memory_range:
+        min: 229376
+        max: 229376
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 377
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 377
+      job_id: jn5q2qxm5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 32986.0
+      throughput: 30.315891590371674
+      estimated_peak_memory_range:
+        min: 137265152
+        max: 137265152
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 389
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 389
+      job_id: jygz7dq6p
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 9446.0
+      throughput: 105.86491636671607
+      estimated_peak_memory_range:
+        min: 684032
+        max: 684032
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 290
+        total_layers: 290
+      job_id: jmg94n7v5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.066690Z'
 - name: CLIPImageEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 126657.0
-      throughput: 7.895339381163299
+      inference_time: 126619.0
+      throughput: 7.8977088746554625
       estimated_peak_memory_range:
-        min: 163840
-        max: 3470824
+        min: 126976
+        max: 4408960
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 576
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 576
-      job_id: jlpeeyw7p
+      job_id: j0px1o89g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 50334.0
+      throughput: 19.86728652600628
+      estimated_peak_memory_range:
+        min: 16384
+        max: 67772216
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 371
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 371
+      job_id: j2p0r08ep
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 173185.0
+      throughput: 5.774172128071138
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 40960
+        max: 529782032
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 382
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jnp1y6wkp
-      job_status: Failed
+        total_layers: 382
+      job_id: j1pvwkymg
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.760150Z'
+    timestamp: '2024-05-20T16:35:30.066714Z'
   - torchscript_onnx_tflite:
-      inference_time: 96976.0
-      throughput: 10.31182973106748
+      inference_time: 95991.0
+      throughput: 10.417643320727985
       estimated_peak_memory_range:
-        min: 229376
-        max: 865695568
+        min: 204800
+        max: 748165536
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 576
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 576
-      job_id: jz5w243z5
+      job_id: jegnevdmg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 37870.0
+      throughput: 26.406126221283337
+      estimated_peak_memory_range:
+        min: 655360
+        max: 195252672
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 371
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 371
+      job_id: jogkyxwop
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 128177.0
-      throughput: 7.801711695546003
+      inference_time: 131060.0
+      throughput: 7.630093087135663
       estimated_peak_memory_range:
-        min: 774144
-        max: 1720363664
+        min: 618496
+        max: 1274243488
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 382
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jz5w243j5
+        total_layers: 382
+      job_id: jlpevm005
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.760227Z'
+    timestamp: '2024-05-20T16:35:30.066735Z'
   - torchscript_onnx_tflite:
-      inference_time: 127012.0
-      throughput: 7.873271816836205
+      inference_time: 126196.0
+      throughput: 7.924181432058068
       estimated_peak_memory_range:
-        min: 184320
-        max: 4508448
+        min: 155648
+        max: 4526472
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 576
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 576
-      job_id: jlpeenz0p
+      job_id: jep2myqm5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 50570.0
+      throughput: 19.774569903104606
+      estimated_peak_memory_range:
+        min: 57344
+        max: 57651824
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 371
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 371
+      job_id: j1p3m0lzg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.760295Z'
+    timestamp: '2024-05-20T16:35:30.066751Z'
+  - torchscript_onnx_qnn:
+      inference_time: 48896.0
+      throughput: 20.451570680628272
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 369
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 369
+      job_id: j1glkm9lp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 168856.0
+      throughput: 5.922205903254844
+      estimated_peak_memory_range:
+        min: 492744704
+        max: 492744704
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 382
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 382
+      job_id: jz5w960jp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jnp18zklg
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.066780Z'
```

## qai_hub_models/models/openpose/export.py

```diff
@@ -116,20 +116,25 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        + " --force_channel_last_output output_0,output_1"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0,output_1",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +164,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +195,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0,output_1", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0,output_1", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/openpose/info.yaml

```diff
@@ -27,11 +27,11 @@
   - Phone
   - Tablet
   - IoT
 related_models:
   - litehrnet
   - mediapipe_pose
 has_static_banner: yes
-has_animated_banner: no
+has_animated_banner: yes
 license_type: other
 deploy_license_type: AI Model Hub License
 dataset: []
```

## qai_hub_models/models/openpose/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: OpenPose
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 11751.0
-      throughput: 85.09914049868097
+      inference_time: 11697.0
+      throughput: 85.4920064973925
       estimated_peak_memory_range:
-        min: 225280
-        max: 2603680
+        min: 204800
+        max: 2413880
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 103
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 103
-      job_id: j0pxnxy95
+      job_id: jvgdv1ylg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 11827.0
-      throughput: 84.5522955948254
+      inference_time: 11783.0
+      throughput: 84.86803021301876
       estimated_peak_memory_range:
-        min: 651264
-        max: 242798248
+        min: 638976
+        max: 240653744
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 186
-      job_id: jegnlk3m5
+      job_id: jnp18z92g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 12055.0
-      throughput: 82.9531314807134
+      inference_time: 11925.0
+      throughput: 83.85744234800839
       estimated_peak_memory_range:
-        min: 589824
-        max: 430729112
+        min: 622592
+        max: 408558976
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 189
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jep20elmg
+        total_layers: 189
+      job_id: j0px1oq1g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.792235Z'
+    timestamp: '2024-05-20T16:35:30.123582Z'
   - torchscript_onnx_tflite:
-      inference_time: 8779.0
-      throughput: 113.90818999886092
+      inference_time: 8714.0
+      throughput: 114.75786091347257
       estimated_peak_memory_range:
-        min: 196608
-        max: 34017488
+        min: 212992
+        max: 35487584
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 103
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 103
-      job_id: jo5mq83qp
+      job_id: jz5w96k6p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 8774.0
-      throughput: 113.97310234784591
+      inference_time: 8761.0
+      throughput: 114.1422212076247
       estimated_peak_memory_range:
-        min: 638976
-        max: 51579776
+        min: 618496
+        max: 53231792
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 186
-      job_id: jopr8wee5
+      job_id: jvgdv1keg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 9248.0
-      throughput: 108.13148788927336
+      inference_time: 9189.0
+      throughput: 108.82576994232234
       estimated_peak_memory_range:
-        min: 622592
-        max: 22342656
+        min: 2715648
+        max: 30463376
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 189
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jqpyrm645
+        total_layers: 189
+      job_id: jo5mzx7wp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.792298Z'
+    timestamp: '2024-05-20T16:35:30.123609Z'
   - torchscript_onnx_tflite:
-      inference_time: 11875.0
-      throughput: 84.21052631578948
+      inference_time: 11765.0
+      throughput: 84.99787505312368
       estimated_peak_memory_range:
-        min: 139264
-        max: 2225560
+        min: 233472
+        max: 2374096
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 103
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 103
-      job_id: j0pxnzl15
+      job_id: jmg94nrl5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 11826.0
-      throughput: 84.5594452900389
+      inference_time: 11798.0
+      throughput: 84.76012883539583
       estimated_peak_memory_range:
-        min: 663552
-        max: 242581864
+        min: 622592
+        max: 241415392
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 186
-      job_id: jep20zr4g
+      job_id: jqp4wr7vg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.792350Z'
+    timestamp: '2024-05-20T16:35:30.123627Z'
+  - torchscript_onnx_qnn:
+      inference_time: 14112.0
+      throughput: 70.86167800453515
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 186
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 186
+      job_id: jz57drml5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 12340.0
+      throughput: 81.03727714748784
+      estimated_peak_memory_range:
+        min: 90116096
+        max: 90116096
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 189
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 189
+      job_id: jegnev4rg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 167707.0
+      throughput: 5.962780325210039
+      estimated_peak_memory_range:
+        min: 87339008
+        max: 87339008
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 103
+        total_layers: 103
+      job_id: jopry3r9g
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.123650Z'
```

## qai_hub_models/models/quicksrnetlarge/export.py

```diff
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetlarge/model.py

```diff
@@ -53,15 +53,15 @@
         model.eval()
 
         return cls(model)
 
     def get_evaluator(self) -> BaseEvaluator:
         return SuperResolutionOutputEvaluator()
 
-    def forward(self, image: torch.Tensor) -> torch.Tensor:
+    def forward(self, image):
         """
         Run QuickSRNet-Large on `image`, and produce an upscaled image
 
         Parameters:
             image: Pixel values pre-processed for model consumption.
                    Range: float[0, 1]
                    3-channel Color Space: RGB
```

## qai_hub_models/models/quicksrnetlarge/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: QuickSRNetLarge
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2492.0
-      throughput: 401.2841091492777
+      inference_time: 2434.0
+      throughput: 410.84634346754314
       estimated_peak_memory_range:
-        min: 16384
-        max: 8350520
+        min: 24576
+        max: 1530712
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 31
-      job_id: j1p801z8g
+      job_id: j7gjln9xp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2101.0
-      throughput: 475.9638267491671
+      inference_time: 2102.0
+      throughput: 475.7373929590866
       estimated_peak_memory_range:
-        min: 225280
-        max: 5584760
+        min: 16384
+        max: 6719848
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: jn5qev3m5
+      job_id: jz5w96j6p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2738.0
-      throughput: 365.23009495982467
+      inference_time: 2677.0
+      throughput: 373.55248412401943
       estimated_peak_memory_range:
-        min: 12288
-        max: 5692928
+        min: 28672
+        max: 47131704
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 33
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jw56ewn7g
+        total_layers: 33
+      job_id: jz57drql5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.816274Z'
+    timestamp: '2024-05-20T16:35:30.164349Z'
   - torchscript_onnx_tflite:
-      inference_time: 1917.0
-      throughput: 521.6484089723526
+      inference_time: 1778.0
+      throughput: 562.429696287964
       estimated_peak_memory_range:
         min: 16384
-        max: 28332832
+        max: 28468960
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 31
-      job_id: jogk783op
+      job_id: jlpevmq15
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1500.0
-      throughput: 666.6666666666666
+      inference_time: 1506.0
+      throughput: 664.0106241699867
       estimated_peak_memory_range:
-        min: 208896
-        max: 17648384
+        min: 204800
+        max: 21459584
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: j1gl6l3lg
+      job_id: jmg94n6l5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1897.0
-      throughput: 527.1481286241434
+      inference_time: 1850.0
+      throughput: 540.5405405405405
       estimated_peak_memory_range:
         min: 212992
-        max: 19230192
+        max: 18821168
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 33
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1p3v6ezg
+        total_layers: 33
+      job_id: jqp4wrzvg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.816325Z'
+    timestamp: '2024-05-20T16:35:30.164374Z'
   - torchscript_onnx_tflite:
-      inference_time: 2485.0
-      throughput: 402.4144869215292
+      inference_time: 2448.0
+      throughput: 408.4967320261438
       estimated_peak_memory_range:
-        min: 32768
-        max: 1755936
+        min: 16384
+        max: 7574720
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 31
-      job_id: j1p80kexg
+      job_id: jygz7d6kp
       job_status: Passed
     torchscript_onnx_qnn:
       inference_time: 2097.0
       throughput: 476.87172150691464
       estimated_peak_memory_range:
-        min: 225280
-        max: 13035320
+        min: 212992
+        max: 78311448
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: jw56e080g
+      job_id: jvgdv1jeg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.816351Z'
+    timestamp: '2024-05-20T16:35:30.164392Z'
+  - torchscript_onnx_qnn:
+      inference_time: 2961.0
+      throughput: 337.7237419790611
+      estimated_peak_memory_range:
+        min: 217088
+        max: 217088
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 31
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 31
+      job_id: jnp18zr2g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 2660.0
+      throughput: 375.9398496240602
+      estimated_peak_memory_range:
+        min: 13025280
+        max: 13025280
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 33
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 33
+      job_id: j0px1ow1g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 14976.0
+      throughput: 66.77350427350427
+      estimated_peak_memory_range:
+        min: 31150080
+        max: 31150080
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 15
+        total_layers: 15
+      job_id: jo5mzxjwp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.164414Z'
```

## qai_hub_models/models/quicksrnetlarge_quantized/export.py

```diff
@@ -120,20 +120,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -167,16 +171,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -196,27 +202,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetlarge_quantized/model.py

```diff
@@ -4,105 +4,78 @@
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 # isort: off
 # This verifies aimet is installed, and this must be included first.
 from qai_hub_models.utils.quantization_aimet import (
     AIMETQuantizableMixin,
+    constrain_quantized_inputs_to_image_range,
 )
 
 # isort: on
 
 import torch
 from aimet_torch.cross_layer_equalization import equalize_model
+from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
-from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.quicksrnetlarge.model import QuickSRNetLarge
-from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config_legacy_v2
+from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 2
+MODEL_ASSET_VERSION = 3
 
-# Weights and config stored in S3 are sourced from
-# https://github.com/quic/aimet-model-zoo/blob/develop/aimet_zoo_torch/quicksrnet/model/model_cards/quicksrnet_large_4x_w8a8.json:
-# https://github.com/quic/aimet-model-zoo/releases/download/phase_2_january_artifacts/quicksrnet_large_4x_checkpoint_int8.pth
-# and
-# https://raw.githubusercontent.com/quic/aimet/release-aimet-1.23/TrainingExtensions/common/src/python/aimet_common/quantsim_config/default_config_per_channel.js
-# Encodings were generated with AIMET QuantSim library
-QUANTIZED_WEIGHTS = "quicksrnet_large_4x_checkpoint_int8.pth"
-AIMET_ENCODINGS = "aimet_quantization_encodings.json"
+DEFAULT_ENCODINGS = "quicksrnetlarge_quantized_encodings.json"
 SCALING_FACTOR = 4
 
 
 class QuickSRNetLargeQuantizable(AIMETQuantizableMixin, QuickSRNetLarge):
     """QuickSRNetLarge with post train quantization support.
 
     Supports only 8 bit weights and activations, and only loads pre-quantized checkpoints.
     Support for quantizing using your own weights & data will come at a later date."""
 
     def __init__(
         self,
         quicksrnet_model: QuantizationSimModel,
     ) -> None:
         QuickSRNetLarge.__init__(self, quicksrnet_model.model)
-        AIMETQuantizableMixin.__init__(
-            self, quicksrnet_model, needs_onnx_direct_aimet_export=True
-        )
+        AIMETQuantizableMixin.__init__(self, quicksrnet_model)
 
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "QuickSRNetLargeQuantizable":
         """
         Parameters:
           aimet_encodings:
             if "DEFAULT": Loads the model with aimet encodings calibrated on BSD300.
             elif None: Doesn't load any encodings. Used when computing encodings.
             else: Interprets as a filepath and loads the encodings stored there.
         """
         # Load Model
-        quicksrnet = QuickSRNetLarge.from_pretrained()
-        input_shape = quicksrnet.get_input_spec()["image"][0]
-        equalize_model(quicksrnet, input_shape)
-
-        # Download weights and quantization parameters
-        weights = CachedWebModelAsset.from_asset_store(
-            MODEL_ID, MODEL_ASSET_VERSION, QUANTIZED_WEIGHTS
-        ).fetch()
-        aimet_config = get_default_aimet_config_legacy_v2()
-
-        # Load the model weights and quantization parameters
-        # In this particular instance, the state_dict keys from the model are all named "model.<expected name>"
-        # where <expected name> is the name of each key in the weights file - without the word model.
-        # We rename all the keys to add the word model
-        state_dict = torch.load(weights, map_location=torch.device("cpu"))["state_dict"]
-        new_state_dict = {"model." + key: value for key, value in state_dict.items()}
-        quicksrnet.load_state_dict(new_state_dict)
+        fp16_model = QuickSRNetLarge.from_pretrained()
+        input_shape = cls.get_input_spec()["image"][0]
+        model = prepare_model(fp16_model)
+        equalize_model(model, input_shape)
+
         sim = QuantizationSimModel(
-            quicksrnet,
+            model,
             quant_scheme="tf_enhanced",
             default_param_bw=8,
             default_output_bw=8,
-            config_file=aimet_config,
+            config_file=get_default_aimet_config(),
             dummy_input=torch.rand(input_shape),
         )
+        constrain_quantized_inputs_to_image_range(sim)
         if aimet_encodings:
             if aimet_encodings == "DEFAULT":
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
-                    MODEL_ID, MODEL_ASSET_VERSION, AIMET_ENCODINGS
+                    MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
 
         return cls(sim)
-
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        if target_runtime == TargetRuntime.QNN:
-            return SourceModelFormat.ONNX
-        else:
-            return SourceModelFormat.TORCHSCRIPT
```

## qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml

```diff
@@ -22,135 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: QuickSRNetLarge-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1512.0
-      throughput: 661.3756613756614
+      inference_time: 1340.0
+      throughput: 746.2686567164179
       estimated_peak_memory_range:
-        min: 20480
-        max: 1404424
+        min: 16384
+        max: 1701800
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 31
-      job_id: j1pv07vm5
+      job_id: jegnevjrg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1156.0
+      throughput: 865.0519031141869
+      estimated_peak_memory_range:
+        min: 16384
+        max: 8330600
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 19
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 19
+      job_id: jqpyd397p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1455.0
+      throughput: 687.2852233676975
+      estimated_peak_memory_range:
+        min: 212992
+        max: 8065904
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 24
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 24
+      job_id: jn5q2qj45
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.840385Z'
+    timestamp: '2024-05-20T16:35:30.195379Z'
   - torchscript_onnx_tflite:
-      inference_time: 1167.0
-      throughput: 856.898029134533
+      inference_time: 996.0
+      throughput: 1004.0160642570281
       estimated_peak_memory_range:
-        min: 12288
-        max: 25644128
+        min: 16384
+        max: 24755152
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 31
-      job_id: j7gjzqe85
+      job_id: jopry3z9g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 811.0
+      throughput: 1233.0456226880394
+      estimated_peak_memory_range:
+        min: 12288
+        max: 18436512
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 19
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 19
+      job_id: j2p0r0n6p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1054.0
+      throughput: 948.7666034155598
+      estimated_peak_memory_range:
+        min: 0
+        max: 16738208
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 24
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 24
+      job_id: j1glkmj8p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.840407Z'
+    timestamp: '2024-05-20T16:35:30.195406Z'
   - torchscript_onnx_tflite:
-      inference_time: 6024.0
-      throughput: 166.00265604249668
+      inference_time: 1313.0
+      throughput: 761.6146230007616
       estimated_peak_memory_range:
-        min: 40960
-        max: 19668928
+        min: 360448
+        max: 2507680
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 30
+        layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 33
-      job_id: jz5w2ry65
+        total_layers: 31
+      job_id: jep2my245
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1162.0
+      throughput: 860.5851979345955
+      estimated_peak_memory_range:
+        min: 20480
+        max: 11496296
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 19
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 19
+      job_id: jogkyxj2p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
+      name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:32.840436Z'
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:30.195422Z'
   - torchscript_onnx_tflite:
-      inference_time: 41995.0
-      throughput: 23.81235861412073
+      inference_time: 4195.0
+      throughput: 238.37902264600714
       estimated_peak_memory_range:
-        min: 1863680
-        max: 4699224
+        min: 45056
+        max: 18644448
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 30
+        layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 33
-      job_id: j1p31omxg
+        total_layers: 31
+      job_id: jz5wqzvm5
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j0pxyrjlg
+      job_status: Failed
     reference_device_info:
-      name: RB5 (Proxy)
+      name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:32.840455Z'
+      chipset: Qcs6490
+    timestamp: '2024-05-20T16:35:30.195441Z'
   - torchscript_onnx_tflite:
-      inference_time: 1874.0
-      throughput: 533.6179295624333
+      inference_time: 37890.0
+      throughput: 26.392187912377935
       estimated_peak_memory_range:
-        min: 24576
-        max: 6948872
+        min: 3629056
+        max: 6133384
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 30
+        layers_on_npu: 28
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 33
-      job_id: jygzo0lk5
+        total_layers: 31
+      job_id: jmg9w218p
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
+      name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.840469Z'
+      chipset: Qcs8250
+    timestamp: '2024-05-20T16:35:30.195453Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1082.0
+      throughput: 924.2144177449168
+      estimated_peak_memory_range:
+        min: 53248
+        max: 53248
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 19
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 19
+      job_id: j1p87ylx5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1417.0
+      throughput: 705.7163020465773
+      estimated_peak_memory_range:
+        min: 8822784
+        max: 8822784
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 24
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 24
+      job_id: jw5614k0p
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 212078.0
+      throughput: 4.715246277313064
+      estimated_peak_memory_range:
+        min: 29732864
+        max: 29732864
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j1p3m0ylg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.195475Z'
```

## qai_hub_models/models/quicksrnetlarge_quantized/test.py

```diff
@@ -1,28 +1,31 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 import os
-import tempfile
 import zipfile
 
 import numpy as np
 import pytest
 import torch
 
 from qai_hub_models.models._shared.super_resolution.app import SuperResolutionApp
 from qai_hub_models.models.quicksrnetlarge_quantized.demo import IMAGE_ADDRESS
 from qai_hub_models.models.quicksrnetlarge_quantized.demo import main as demo_main
 from qai_hub_models.models.quicksrnetlarge_quantized.model import (
     MODEL_ASSET_VERSION,
     MODEL_ID,
     QuickSRNetLargeQuantizable,
 )
-from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, load_image
+from qai_hub_models.utils.asset_loaders import (
+    CachedWebModelAsset,
+    load_image,
+    qaihm_temp_dir,
+)
 from qai_hub_models.utils.testing import assert_most_close, skip_clone_repo_check
 
 OUTPUT_IMAGE_ADDRESS = CachedWebModelAsset.from_asset_store(
     MODEL_ID, MODEL_ASSET_VERSION, "quicksrnetlarge_quantized_output.png"
 )
 
 
@@ -65,15 +68,15 @@
 
 
 @pytest.mark.skip("https://github.com/tetraai/tetracode/issues/9606")
 @skip_clone_repo_check
 def test_aimet_export():
     model = QuickSRNetLargeQuantizable.from_pretrained()
     name = model.__class__.__name__
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         output_zip = model.convert_to_onnx_and_aimet_encodings(
             tmpdir,
         )
         assert os.path.exists(output_zip)
         with zipfile.ZipFile(output_zip, "r") as zip:
             assert zip.namelist() == [
                 f"{name}.aimet/",
```

## qai_hub_models/models/quicksrnetmedium/export.py

```diff
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetmedium/model.py

```diff
@@ -31,14 +31,15 @@
     """Exportable QuickSRNet-Medium upscaler, end-to-end."""
 
     def __init__(
         self,
         quicksrnet_model: torch.nn.Module,
     ) -> None:
         super().__init__()
+        self.relu = torch.nn.ReLU()
         self.model = quicksrnet_model
 
     @classmethod
     def from_pretrained(cls) -> QuickSRNetMedium:
         model = _load_quicksrnet_source_model(
             SCALING_FACTOR,
             NUM_CHANNELS,
@@ -53,29 +54,29 @@
         model.eval()
 
         return cls(model)
 
     def get_evaluator(self) -> BaseEvaluator:
         return SuperResolutionOutputEvaluator()
 
-    def forward(self, image: torch.Tensor) -> torch.Tensor:
+    def forward(self, image):
         """
         Run QuickSRNet-Medium on `image`, and produce an upscaled image
 
         Parameters:
             image: Pixel values pre-processed for model consumption.
                    Range: float[0, 1]
                    3-channel Color Space: RGB
 
         Returns:
             image: Pixel values
                    Range: float[0, 1]
                    3-channel Color Space: RGB
         """
-
+        # image = self.relu(image)
         return self.model(image)
 
     @staticmethod
     def get_input_spec(
         batch_size: int = 1,
         num_channels: int = 3,
         height: int = 128,
```

## qai_hub_models/models/quicksrnetmedium/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: QuickSRNetMedium
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1385.0
-      throughput: 722.0216606498195
+      inference_time: 1388.0
+      throughput: 720.4610951008646
       estimated_peak_memory_range:
-        min: 16384
-        max: 1507064
+        min: 32768
+        max: 1844064
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 17
-      job_id: jlpeeyk0p
+      job_id: jwgov6jx5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 998.0
-      throughput: 1002.0040080160321
+      inference_time: 1011.0
+      throughput: 989.1196834817013
       estimated_peak_memory_range:
-        min: 221184
-        max: 7358048
+        min: 28672
+        max: 8507224
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 17
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 17
-      job_id: jz5w24qj5
+      job_id: jlpevmj15
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1500.0
-      throughput: 666.6666666666666
+      inference_time: 1498.0
+      throughput: 667.5567423230974
       estimated_peak_memory_range:
-        min: 212992
-        max: 8597144
+        min: 12288
+        max: 8500872
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 19
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jnp1y6elp
+        total_layers: 19
+      job_id: jnp18z02g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.860888Z'
+    timestamp: '2024-05-20T16:35:30.235442Z'
   - torchscript_onnx_tflite:
-      inference_time: 871.0
-      throughput: 1148.105625717566
+      inference_time: 923.0
+      throughput: 1083.4236186348862
       estimated_peak_memory_range:
         min: 16384
-        max: 19182544
+        max: 19845568
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 17
-      job_id: jygzonr65
+      job_id: j1pvwkjjg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 641.0
-      throughput: 1560.0624024960998
+      inference_time: 648.0
+      throughput: 1543.20987654321
       estimated_peak_memory_range:
         min: 208896
-        max: 14603312
+        max: 15787072
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 17
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 17
-      job_id: jmg9jdwv5
+      job_id: jygz7d1kp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1118.0
-      throughput: 894.4543828264758
+      inference_time: 1030.0
+      throughput: 970.8737864077669
       estimated_peak_memory_range:
-        min: 217088
-        max: 15048656
+        min: 0
+        max: 14123616
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 19
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jvgde2ol5
+        total_layers: 19
+      job_id: jvgdv1weg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.860931Z'
+    timestamp: '2024-05-20T16:35:30.235469Z'
   - torchscript_onnx_tflite:
-      inference_time: 1365.0
-      throughput: 732.6007326007326
+      inference_time: 1370.0
+      throughput: 729.92700729927
       estimated_peak_memory_range:
         min: 24576
-        max: 16231088
+        max: 1369376
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 17
-      job_id: jz5708olg
+      job_id: j7gjlnjxp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1005.0
-      throughput: 995.0248756218906
+      inference_time: 1008.0
+      throughput: 992.063492063492
       estimated_peak_memory_range:
         min: 221184
-        max: 6072368
+        max: 12353904
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 17
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 17
-      job_id: jegnlw1r5
+      job_id: jmg94nvl5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.860962Z'
+    timestamp: '2024-05-20T16:35:30.235492Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1129.0
+      throughput: 885.7395925597874
+      estimated_peak_memory_range:
+        min: 217088
+        max: 217088
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 17
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 17
+      job_id: jz5w96o6p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1559.0
+      throughput: 641.4368184733804
+      estimated_peak_memory_range:
+        min: 8896512
+        max: 8896512
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 19
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 19
+      job_id: jz57drzl5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 3368.0
+      throughput: 296.91211401425176
+      estimated_peak_memory_range:
+        min: 33103872
+        max: 33103872
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jqp4wrqvg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.235514Z'
```

## qai_hub_models/models/quicksrnetmedium_quantized/export.py

```diff
@@ -120,20 +120,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -167,16 +171,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -196,27 +202,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetmedium_quantized/model.py

```diff
@@ -4,104 +4,77 @@
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 # isort: off
 # This verifies aimet is installed, and this must be included first.
 from qai_hub_models.utils.quantization_aimet import (
     AIMETQuantizableMixin,
+    constrain_quantized_inputs_to_image_range,
 )
 
 # isort: on
 
 import torch
 from aimet_torch.cross_layer_equalization import equalize_model
+from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
-from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.quicksrnetmedium.model import QuickSRNetMedium
-from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config_legacy_v2
+from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 2
+MODEL_ASSET_VERSION = 4
 
-# Weights and config stored in S3 are sourced from
-# https://github.com/quic/aimet-model-zoo/blob/develop/aimet_zoo_torch/quicksrnet/model/model_cards/quicksrnet_medium_4x_w8a8.json:
-# https://github.com/quic/aimet-model-zoo/releases/download/phase_2_january_artifacts/quicksrnet_medium_4x_checkpoint_int8.pth
-# and
-# https://raw.githubusercontent.com/quic/aimet/release-aimet-1.23/TrainingExtensions/common/src/python/aimet_common/quantsim_config/default_config_per_channel.js
-# Encodings were generated with AIMET QuantSim library
-QUANTIZED_WEIGHTS = "quicksrnet_medium_4x_checkpoint_int8.pth"
-AIMET_ENCODINGS = "aimet_quantization_encodings.json"
+DEFAULT_ENCODINGS = "quicksrnetmedium_quantized_encodings.json"
 SCALING_FACTOR = 4
 
 
 class QuickSRNetMediumQuantizable(AIMETQuantizableMixin, QuickSRNetMedium):
     """QuickSRNetMedium with post train quantization support.
     Supports only 8 bit weights and activations, and only loads pre-quantized checkpoints.
     Support for quantizing using your own weights & data will come at a later date."""
 
     def __init__(
         self,
         quicksrnet_model: QuantizationSimModel,
     ) -> None:
         QuickSRNetMedium.__init__(self, quicksrnet_model.model)
-        AIMETQuantizableMixin.__init__(
-            self, quicksrnet_model, needs_onnx_direct_aimet_export=True
-        )
+        AIMETQuantizableMixin.__init__(self, quicksrnet_model)
 
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> "QuickSRNetMediumQuantizable":
         """
         Parameters:
           aimet_encodings:
             if "DEFAULT": Loads the model with aimet encodings calibrated on BSD300.
             elif None: Doesn't load any encodings. Used when computing encodings.
             else: Interprets as a filepath and loads the encodings stored there.
         """
         # Load Model
-        quicksrnet = QuickSRNetMedium.from_pretrained()
-        input_shape = quicksrnet.get_input_spec()["image"][0]
-        equalize_model(quicksrnet, input_shape)
-
-        # Download weights and quantization parameters
-        weights = CachedWebModelAsset.from_asset_store(
-            MODEL_ID, MODEL_ASSET_VERSION, QUANTIZED_WEIGHTS
-        ).fetch()
-        aimet_config = get_default_aimet_config_legacy_v2()
-
-        # Load the model weights and quantization parameters
-        # In this particular instance, the state_dict keys from the model are all named "model.<expected name>"
-        # where <expected name> is the name of each key in the weights file - without the word model.
-        # We rename all the keys to add the word model
-        state_dict = torch.load(weights, map_location=torch.device("cpu"))["state_dict"]
-        new_state_dict = {"model." + key: value for key, value in state_dict.items()}
-        quicksrnet.load_state_dict(new_state_dict)
+        fp16_model = QuickSRNetMedium.from_pretrained()
+        input_shape = cls.get_input_spec()["image"][0]
+        model = prepare_model(fp16_model)
+        equalize_model(model, input_shape)
         sim = QuantizationSimModel(
-            quicksrnet,
+            model,
             quant_scheme="tf_enhanced",
             default_param_bw=8,
             default_output_bw=8,
-            config_file=aimet_config,
+            config_file=get_default_aimet_config(),
             dummy_input=torch.rand(input_shape),
         )
+        constrain_quantized_inputs_to_image_range(sim)
+
         if aimet_encodings:
             if aimet_encodings == "DEFAULT":
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
-                    MODEL_ID, MODEL_ASSET_VERSION, AIMET_ENCODINGS
+                    MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
 
         return cls(sim)
-
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        if target_runtime == TargetRuntime.QNN:
-            return SourceModelFormat.ONNX
-        else:
-            return SourceModelFormat.TORCHSCRIPT
```

## qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml

```diff
@@ -22,135 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: QuickSRNetMedium-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1046.0
-      throughput: 956.0229445506692
+      inference_time: 992.0
+      throughput: 1008.0645161290323
       estimated_peak_memory_range:
-        min: 1339392
-        max: 2781424
+        min: 12288
+        max: 1410992
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 17
-      job_id: jqp4k3wlg
+      job_id: j0px1ov1g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 801.0
+      throughput: 1248.4394506866417
+      estimated_peak_memory_range:
+        min: 65536
+        max: 68916056
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 11
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 11
+      job_id: jopry3k9g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1215.0
+      throughput: 823.0452674897119
+      estimated_peak_memory_range:
+        min: 12288
+        max: 9491496
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 16
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 16
+      job_id: j1p87yox5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.884809Z'
+    timestamp: '2024-05-20T16:35:30.266116Z'
   - torchscript_onnx_tflite:
-      inference_time: 871.0
-      throughput: 1148.105625717566
+      inference_time: 865.0
+      throughput: 1156.0693641618498
       estimated_peak_memory_range:
         min: 16384
-        max: 19479952
+        max: 19816736
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 17
-      job_id: j0pxnx195
+      job_id: jo5mzxrwp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 550.0
+      throughput: 1818.1818181818182
+      estimated_peak_memory_range:
+        min: 65536
+        max: 15505168
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 11
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 11
+      job_id: jep2my845
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 882.0
+      throughput: 1133.7868480725624
+      estimated_peak_memory_range:
+        min: 0
+        max: 14140464
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 16
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 16
+      job_id: jogkyxz2p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.884829Z'
+    timestamp: '2024-05-20T16:35:30.266144Z'
   - torchscript_onnx_tflite:
-      inference_time: 3381.0
-      throughput: 295.77048210588583
+      inference_time: 1016.0
+      throughput: 984.2519685039371
       estimated_peak_memory_range:
-        min: 12288
-        max: 15175488
+        min: 69632
+        max: 1384896
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 16
+        layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 19
-      job_id: j1p80kjxg
+        total_layers: 17
+      job_id: jegnev2rg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 803.0
+      throughput: 1245.3300124533
+      estimated_peak_memory_range:
+        min: 65536
+        max: 70718264
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 11
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 11
+      job_id: j2p0r0y6p
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
+      name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:32.884857Z'
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:30.266161Z'
   - torchscript_onnx_tflite:
-      inference_time: 15536.0
-      throughput: 64.36663233779609
+      inference_time: 1823.0
+      throughput: 548.5463521667581
       estimated_peak_memory_range:
-        min: 1720320
-        max: 4755304
+        min: 20480
+        max: 13941344
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 16
+        layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 19
-      job_id: jwgondv4p
+        total_layers: 17
+      job_id: jygzrykx5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1392.0
+      throughput: 718.3908045977012
+      estimated_peak_memory_range:
+        min: 65536
+        max: 15064032
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 11
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 11
+      job_id: jmg9w2emp
       job_status: Passed
     reference_device_info:
-      name: RB5 (Proxy)
+      name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:32.884871Z'
+      chipset: Qcs6490
+    timestamp: '2024-05-20T16:35:30.266177Z'
   - torchscript_onnx_tflite:
-      inference_time: 1396.0
-      throughput: 716.3323782234957
+      inference_time: 9357.0
+      throughput: 106.87186063909373
       estimated_peak_memory_range:
-        min: 32768
-        max: 1677424
+        min: 3276800
+        max: 6753144
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 16
+        layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 19
-      job_id: j1gl6qw8g
+        total_layers: 17
+      job_id: jz5wqznm5
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
+      name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.884887Z'
+      chipset: Qcs8250
+    timestamp: '2024-05-20T16:35:30.266189Z'
+  - torchscript_onnx_qnn:
+      inference_time: 794.0
+      throughput: 1259.4458438287154
+      estimated_peak_memory_range:
+        min: 53248
+        max: 53248
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 11
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 11
+      job_id: jqpyd3e7p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1142.0
+      throughput: 875.6567425569177
+      estimated_peak_memory_range:
+        min: 8826880
+        max: 8826880
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 16
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 16
+      job_id: jn5q2q845
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 3479.0
+      throughput: 287.4389192296637
+      estimated_peak_memory_range:
+        min: 15757312
+        max: 15757312
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 17
+        total_layers: 17
+      job_id: j1glkmn8p
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.266211Z'
```

## qai_hub_models/models/quicksrnetmedium_quantized/test.py

```diff
@@ -1,28 +1,31 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 import os
-import tempfile
 import zipfile
 
 import numpy as np
 import pytest
 import torch
 
 from qai_hub_models.models._shared.super_resolution.app import SuperResolutionApp
 from qai_hub_models.models.quicksrnetmedium_quantized.demo import IMAGE_ADDRESS
 from qai_hub_models.models.quicksrnetmedium_quantized.demo import main as demo_main
 from qai_hub_models.models.quicksrnetmedium_quantized.model import (
     MODEL_ASSET_VERSION,
     MODEL_ID,
     QuickSRNetMediumQuantizable,
 )
-from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, load_image
+from qai_hub_models.utils.asset_loaders import (
+    CachedWebModelAsset,
+    load_image,
+    qaihm_temp_dir,
+)
 from qai_hub_models.utils.testing import assert_most_close, skip_clone_repo_check
 
 OUTPUT_IMAGE_ADDRESS = CachedWebModelAsset.from_asset_store(
     MODEL_ID, MODEL_ASSET_VERSION, "quicksrnetmedium_quantized_output.png"
 )
 
 
@@ -65,15 +68,15 @@
 
 
 @pytest.mark.skip("https://github.com/tetraai/tetracode/issues/9606")
 @skip_clone_repo_check
 def test_aimet_export():
     model = QuickSRNetMediumQuantizable.from_pretrained()
     name = model.__class__.__name__
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         output_zip = model.convert_to_onnx_and_aimet_encodings(
             tmpdir,
         )
         assert os.path.exists(output_zip)
         with zipfile.ZipFile(output_zip, "r") as zip:
             assert zip.namelist() == [
                 f"{name}.aimet/",
```

## qai_hub_models/models/quicksrnetsmall/export.py

```diff
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetsmall/model.py

```diff
@@ -53,15 +53,15 @@
         model.eval()
 
         return cls(model)
 
     def get_evaluator(self) -> BaseEvaluator:
         return SuperResolutionOutputEvaluator()
 
-    def forward(self, image: torch.Tensor) -> torch.Tensor:
+    def forward(self, image):
         """
         Run QuickSRNet-Small on `image`, and produce an upscaled image
 
         Parameters:
             image: Pixel values pre-processed for model consumption.
                    Range: float[0, 1]
                    3-channel Color Space: RGB
```

## qai_hub_models/models/quicksrnetsmall/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: QuickSRNetSmall
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1316.0
-      throughput: 759.8784194528876
+      inference_time: 1315.0
+      throughput: 760.4562737642585
       estimated_peak_memory_range:
-        min: 24576
-        max: 8392968
+        min: 16384
+        max: 8193912
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 11
-      job_id: jo5mq8zqp
+      job_id: jw561460p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1010.0
-      throughput: 990.0990099009902
+      inference_time: 999.0
+      throughput: 1001.001001001001
       estimated_peak_memory_range:
-        min: 217088
-        max: 51877032
+        min: 229376
+        max: 63786312
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 11
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 11
-      job_id: jopr8wye5
+      job_id: j1pvwk3jg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1411.0
-      throughput: 708.7172218284904
+      inference_time: 1418.0
+      throughput: 705.2186177715091
       estimated_peak_memory_range:
-        min: 217088
-        max: 8686544
+        min: 90112
+        max: 2421520
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 13
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jqpyrmd45
+        total_layers: 13
+      job_id: jz5w96v6p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.905242Z'
+    timestamp: '2024-05-20T16:35:30.306313Z'
   - torchscript_onnx_tflite:
-      inference_time: 914.0
-      throughput: 1094.0919037199126
+      inference_time: 884.0
+      throughput: 1131.2217194570135
       estimated_peak_memory_range:
-        min: 16384
-        max: 18347856
+        min: 20480
+        max: 18573536
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 11
-      job_id: jegnlkem5
+      job_id: j1p3m0klg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 617.0
-      throughput: 1620.7455429497568
+      inference_time: 621.0
+      throughput: 1610.3059581320451
       estimated_peak_memory_range:
-        min: 208896
-        max: 14414800
+        min: 0
+        max: 14770544
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 11
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 11
-      job_id: jep20emmg
+      job_id: j7gjlnxxp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1011.0
-      throughput: 989.1196834817013
+      inference_time: 931.0
+      throughput: 1074.1138560687432
       estimated_peak_memory_range:
-        min: 0
-        max: 12267184
+        min: 12288
+        max: 12222752
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 13
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j2p036rep
+        total_layers: 13
+      job_id: jmg94n1l5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.905287Z'
+    timestamp: '2024-05-20T16:35:30.306339Z'
   - torchscript_onnx_tflite:
-      inference_time: 1327.0
-      throughput: 753.5795026375282
+      inference_time: 1314.0
+      throughput: 761.03500761035
       estimated_peak_memory_range:
-        min: 28672
-        max: 8134240
+        min: 20480
+        max: 7936728
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 11
-      job_id: j1p3vrolg
+      job_id: jwgov6yx5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1021.0
-      throughput: 979.4319294809011
+      inference_time: 996.0
+      throughput: 1004.0160642570281
       estimated_peak_memory_range:
-        min: 249856
-        max: 7951808
+        min: 229376
+        max: 3511288
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 11
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 11
-      job_id: jlpeen61p
+      job_id: jygz7dekp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.905312Z'
+    timestamp: '2024-05-20T16:35:30.306357Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1089.0
+      throughput: 918.2736455463728
+      estimated_peak_memory_range:
+        min: 241664
+        max: 241664
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 11
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 11
+      job_id: jlpevm915
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1478.0
+      throughput: 676.5899864682003
+      estimated_peak_memory_range:
+        min: 8847360
+        max: 8847360
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 13
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 13
+      job_id: jnp18zl2g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 2483.0
+      throughput: 402.7386226339106
+      estimated_peak_memory_range:
+        min: 33112064
+        max: 33112064
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jvgdv19eg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.306381Z'
```

## qai_hub_models/models/quicksrnetsmall_quantized/export.py

```diff
@@ -120,20 +120,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -167,16 +171,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -196,27 +202,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/quicksrnetsmall_quantized/model.py

```diff
@@ -4,103 +4,76 @@
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 # isort: off
 # This verifies aimet is installed, and this must be included first.
 from qai_hub_models.utils.quantization_aimet import (
     AIMETQuantizableMixin,
+    constrain_quantized_inputs_to_image_range,
 )
 
 # isort: on
 
 import torch
 from aimet_torch.cross_layer_equalization import equalize_model
+from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
-from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.quicksrnetsmall.model import QuickSRNetSmall
-from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config_legacy_v2
+from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 2
+MODEL_ASSET_VERSION = 4
 
-# Weights and config stored in S3 are sourced from
-# https://github.com/quic/aimet-model-zoo/blob/develop/aimet_zoo_torch/quicksrnet/model/model_cards/quicksrnet_small_4x_w8a8.json:
-# https://github.com/quic/aimet-model-zoo/releases/download/phase_2_january_artifacts/quicksrnet_small_4x_checkpoint_int8.pth
-# and
-# https://raw.githubusercontent.com/quic/aimet/release-aimet-1.23/TrainingExtensions/common/src/python/aimet_common/quantsim_config/default_config_per_channel.js
-# Encodings were generated with AIMET QuantSim library
-QUANTIZED_WEIGHTS = "quicksrnet_small_4x_checkpoint_int8.pth"
-AIMET_ENCODINGS = "aimet_quantization_encodings.json"
+DEFAULT_ENCODINGS = "quicksrnetsmall_quantized_encodings.json"
 SCALING_FACTOR = 4
 
 
 class QuickSRNetSmallQuantizable(AIMETQuantizableMixin, QuickSRNetSmall):
     """QuickSRNetSmall with post train quantization support.
     Supports only 8 bit weights and activations, and only loads pre-quantized checkpoints.
     Support for quantizing using your own weights & data will come at a later date."""
 
     def __init__(
         self,
         quicksrnet_model: QuantizationSimModel,
     ) -> None:
         QuickSRNetSmall.__init__(self, quicksrnet_model.model)
-        AIMETQuantizableMixin.__init__(
-            self, quicksrnet_model, needs_onnx_direct_aimet_export=True
-        )
+        AIMETQuantizableMixin.__init__(self, quicksrnet_model)
 
     @classmethod
     def from_pretrained(
         cls, aimet_encodings: str | None = "DEFAULT"
     ) -> "QuickSRNetSmallQuantizable":
         """
         Parameters:
           aimet_encodings:
             if "DEFAULT": Loads the model with aimet encodings calibrated on BSD300.
             elif None: Doesn't load any encodings. Used when computing encodings.
             else: Interprets as a filepath and loads the encodings stored there.
         """
         # Load Model
-        quicksrnet = QuickSRNetSmall.from_pretrained()
-        input_shape = quicksrnet.get_input_spec()["image"][0]
-        equalize_model(quicksrnet, input_shape)
-
-        # Download weights and quantization parameters
-        weights = CachedWebModelAsset.from_asset_store(
-            MODEL_ID, MODEL_ASSET_VERSION, QUANTIZED_WEIGHTS
-        ).fetch()
-        aimet_config = get_default_aimet_config_legacy_v2()
-
-        # Load the model weights and quantization parameters
-        # In this particular instance, the state_dict keys from the model are all named "model.<expected name>"
-        # where <expected name> is the name of each key in the weights file - without the word model.
-        # We rename all the keys to add the word model
-        state_dict = torch.load(weights, map_location=torch.device("cpu"))["state_dict"]
-        new_state_dict = {"model." + key: value for key, value in state_dict.items()}
-        quicksrnet.load_state_dict(new_state_dict)
+        fp16_model = QuickSRNetSmall.from_pretrained()
+        input_shape = cls.get_input_spec()["image"][0]
+        model = prepare_model(fp16_model)
+        equalize_model(model, input_shape)
         sim = QuantizationSimModel(
-            quicksrnet,
+            fp16_model,
             quant_scheme="tf_enhanced",
             default_param_bw=8,
             default_output_bw=8,
-            config_file=aimet_config,
+            config_file=get_default_aimet_config(),
             dummy_input=torch.rand(input_shape),
         )
+        constrain_quantized_inputs_to_image_range(sim)
+
         if aimet_encodings:
             if aimet_encodings == "DEFAULT":
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
-                    MODEL_ID, MODEL_ASSET_VERSION, AIMET_ENCODINGS
+                    MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
 
         return cls(sim)
-
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        if target_runtime == TargetRuntime.QNN:
-            return SourceModelFormat.ONNX
-        else:
-            return SourceModelFormat.TORCHSCRIPT
```

## qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml

```diff
@@ -22,135 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: QuickSRNetSmall-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 987.0
-      throughput: 1013.1712259371834
+      inference_time: 957.0
+      throughput: 1044.932079414838
       estimated_peak_memory_range:
-        min: 20480
-        max: 1821960
+        min: 1048576
+        max: 3323920
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 11
-      job_id: jogk78yop
+      job_id: jz57drwl5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 662.0
+      throughput: 1510.5740181268882
+      estimated_peak_memory_range:
+        min: 20480
+        max: 2419512
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 8
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 8
+      job_id: jo5mzx2wp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1143.0
+      throughput: 874.8906386701663
+      estimated_peak_memory_range:
+        min: 212992
+        max: 2520600
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 14
+      job_id: jqpyd3w7p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.929166Z'
+    timestamp: '2024-05-20T16:35:30.337379Z'
   - torchscript_onnx_tflite:
-      inference_time: 1612.0
-      throughput: 620.3473945409429
+      inference_time: 788.0
+      throughput: 1269.0355329949239
       estimated_peak_memory_range:
-        min: 16384
-        max: 18121488
+        min: 0
+        max: 18194848
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 11
-      job_id: jn5qev2m5
+      job_id: jqp4wrovg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 450.0
+      throughput: 2222.222222222222
+      estimated_peak_memory_range:
+        min: 61440
+        max: 12988496
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 8
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 8
+      job_id: jegnevyrg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 818.0
+      throughput: 1222.4938875305625
+      estimated_peak_memory_range:
+        min: 212992
+        max: 14543472
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 14
+      job_id: j2p0r0q6p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.929186Z'
+    timestamp: '2024-05-20T16:35:30.337406Z'
   - torchscript_onnx_tflite:
-      inference_time: 3227.0
-      throughput: 309.88534242330337
+      inference_time: 979.0
+      throughput: 1021.4504596527069
       estimated_peak_memory_range:
-        min: 49152
-        max: 15102016
+        min: 28672
+        max: 2811096
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 10
+        layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 13
-      job_id: jqp4k24vg
+        total_layers: 11
+      job_id: j0px1oj1g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 661.0
+      throughput: 1512.8593040847202
+      estimated_peak_memory_range:
+        min: 20480
+        max: 11468640
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 8
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 8
+      job_id: jep2my645
       job_status: Passed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
+      name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:32.929214Z'
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:30.337423Z'
   - torchscript_onnx_tflite:
-      inference_time: 12108.0
-      throughput: 82.59002312520647
+      inference_time: 1682.0
+      throughput: 594.5303210463734
       estimated_peak_memory_range:
-        min: 5685248
-        max: 13091440
+        min: 12288
+        max: 13230640
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 10
+        layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 13
-      job_id: j1pvr2w75
+        total_layers: 11
+      job_id: j1p3er0m5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1294.0
+      throughput: 772.7975270479135
+      estimated_peak_memory_range:
+        min: 65536
+        max: 12983280
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 8
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 8
+      job_id: jz5wqr645
       job_status: Passed
     reference_device_info:
-      name: RB5 (Proxy)
+      name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:32.929227Z'
+      chipset: Qcs6490
+    timestamp: '2024-05-20T16:35:30.337440Z'
   - torchscript_onnx_tflite:
-      inference_time: 1388.0
-      throughput: 720.4610951008646
+      inference_time: 5698.0
+      throughput: 175.5001755001755
       estimated_peak_memory_range:
-        min: 24576
-        max: 1828056
+        min: 3362816
+        max: 13394304
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 10
+        layers_on_npu: 8
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 13
-      job_id: j0pxnzr15
+        total_layers: 11
+      job_id: jwgo3961g
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
+      name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.929244Z'
+      chipset: Qcs8250
+    timestamp: '2024-05-20T16:35:30.337451Z'
+  - torchscript_onnx_qnn:
+      inference_time: 762.0
+      throughput: 1312.3359580052493
+      estimated_peak_memory_range:
+        min: 49152
+        max: 49152
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 8
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 8
+      job_id: jopry3q9g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1088.0
+      throughput: 919.1176470588235
+      estimated_peak_memory_range:
+        min: 9007104
+        max: 9007104
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 14
+      job_id: j1p87y9x5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 8332.0
+      throughput: 120.01920307249159
+      estimated_peak_memory_range:
+        min: 33210368
+        max: 33210368
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jogkyxn2p
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.337478Z'
```

## qai_hub_models/models/quicksrnetsmall_quantized/test.py

```diff
@@ -1,28 +1,31 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 import os
-import tempfile
 import zipfile
 
 import numpy as np
 import pytest
 import torch
 
 from qai_hub_models.models._shared.super_resolution.app import SuperResolutionApp
 from qai_hub_models.models.quicksrnetsmall_quantized.demo import IMAGE_ADDRESS
 from qai_hub_models.models.quicksrnetsmall_quantized.demo import main as demo_main
 from qai_hub_models.models.quicksrnetsmall_quantized.model import (
     MODEL_ASSET_VERSION,
     MODEL_ID,
     QuickSRNetSmallQuantizable,
 )
-from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, load_image
+from qai_hub_models.utils.asset_loaders import (
+    CachedWebModelAsset,
+    load_image,
+    qaihm_temp_dir,
+)
 from qai_hub_models.utils.testing import assert_most_close, skip_clone_repo_check
 
 OUTPUT_IMAGE_ADDRESS = CachedWebModelAsset.from_asset_store(
     MODEL_ID, MODEL_ASSET_VERSION, "quicksrnetsmall_quantized_output.png"
 )
 
 
@@ -65,15 +68,15 @@
 
 
 @pytest.mark.skip("https://github.com/tetraai/tetracode/issues/9606")
 @skip_clone_repo_check
 def test_aimet_export():
     model = QuickSRNetSmallQuantizable.from_pretrained()
     name = model.__class__.__name__
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         output_zip = model.convert_to_onnx_and_aimet_encodings(
             tmpdir,
         )
         assert os.path.exists(output_zip)
         with zipfile.ZipFile(output_zip, "r") as zip:
             assert zip.namelist() == [
                 f"{name}.aimet/",
```

## qai_hub_models/models/real_esrgan_general_x4v3/export.py

```diff
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Real-ESRGAN-General-x4v3
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 7205.0
-      throughput: 138.79250520471894
+      inference_time: 7261.0
+      throughput: 137.72207684891887
       estimated_peak_memory_range:
-        min: 15941632
-        max: 27205736
+        min: 17612800
+        max: 21719648
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 69
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 72
-      job_id: j1gl6lklg
+      job_id: jn5q2qk45
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 7008.0
-      throughput: 142.69406392694063
+      inference_time: 6254.0
+      throughput: 159.89766549408378
       estimated_peak_memory_range:
-        min: 45056
-        max: 45937496
+        min: 245760
+        max: 5108560
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 72
-      job_id: j1p3v6mzg
+      job_id: j1p3m03lg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 7130.0
-      throughput: 140.25245441795232
+      inference_time: 6861.0
+      throughput: 145.75134819997086
       estimated_peak_memory_range:
-        min: 8429568
-        max: 23590888
+        min: 6336512
+        max: 17772656
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 74
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1pv07wm5
+        total_layers: 74
+      job_id: jlpevm115
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.949581Z'
+    timestamp: '2024-05-20T16:35:30.377608Z'
   - torchscript_onnx_tflite:
-      inference_time: 5369.0
-      throughput: 186.25442354255912
+      inference_time: 5603.0
+      throughput: 178.4758165268606
       estimated_peak_memory_range:
-        min: 20480
-        max: 55365360
+        min: 16384
+        max: 55868880
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 69
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 72
-      job_id: jw56ew17g
+      job_id: j1glkmz8p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 4934.0
-      throughput: 202.67531414673692
+      inference_time: 4592.0
+      throughput: 217.77003484320556
       estimated_peak_memory_range:
-        min: 12288
-        max: 31445424
+        min: 208896
+        max: 33800560
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 72
-      job_id: jwgok8vdp
+      job_id: jwgov60x5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 5279.0
-      throughput: 189.42981625307823
+      inference_time: 5149.0
+      throughput: 194.21246844047388
       estimated_peak_memory_range:
-        min: 8392704
-        max: 47488976
+        min: 2310144
+        max: 36369760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 74
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j7gjzql85
+        total_layers: 74
+      job_id: jygz7d9kp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.949640Z'
+    timestamp: '2024-05-20T16:35:30.377636Z'
   - torchscript_onnx_tflite:
-      inference_time: 7123.0
-      throughput: 140.39028499227854
+      inference_time: 7335.0
+      throughput: 136.332651670075
       estimated_peak_memory_range:
-        min: 15777792
-        max: 23652120
+        min: 9465856
+        max: 18689240
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 69
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 72
-      job_id: jopr87d95
+      job_id: jw5614j0p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 7016.0
-      throughput: 142.53135689851769
+      inference_time: 6280.0
+      throughput: 159.23566878980893
       estimated_peak_memory_range:
-        min: 32768
-        max: 10477536
+        min: 53248
+        max: 43875408
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 72
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 72
-      job_id: j1p80krxg
+      job_id: j7gjlnmxp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.949674Z'
+    timestamp: '2024-05-20T16:35:30.377655Z'
+  - torchscript_onnx_qnn:
+      inference_time: 8724.0
+      throughput: 114.62631820265933
+      estimated_peak_memory_range:
+        min: 229376
+        max: 229376
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 72
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 72
+      job_id: j1pvwkojg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 7228.0
+      throughput: 138.35085777531822
+      estimated_peak_memory_range:
+        min: 8613888
+        max: 8613888
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 74
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 74
+      job_id: jz5w96n6p
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 58952.0
+      throughput: 16.96295291084272
+      estimated_peak_memory_range:
+        min: 26607616
+        max: 26607616
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 70
+        total_layers: 70
+      job_id: jmg94nel5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.377681Z'
```

## qai_hub_models/models/real_esrgan_x4plus/export.py

```diff
@@ -116,15 +116,15 @@
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options
+        target_runtime, compile_options, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -188,14 +188,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/real_esrgan_x4plus/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Real-ESRGAN-x4plus
   performance_metrics:
-  - torchscript_onnx_qnn:
-      inference_time: 65726.0
-      throughput: 15.214679122417309
+  - torchscript_onnx_tflite:
+      inference_time: 68854.0
+      throughput: 14.523484474395097
       estimated_peak_memory_range:
-        min: 102400
-        max: 107703704
+        min: 28672
+        max: 3752144
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1028
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1028
+      job_id: jnp18zx2g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 68240.0
+      throughput: 14.654161781946073
+      estimated_peak_memory_range:
+        min: 94208
+        max: 108186752
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1031
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1031
-      job_id: jygzon765
+      job_id: jmg94new5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 69431.0
-      throughput: 14.402788379830335
+      inference_time: 67823.0
+      throughput: 14.744260796484967
       estimated_peak_memory_range:
-        min: 6467584
-        max: 119585224
+        min: 6422528
+        max: 150577760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 1030
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jmg9jd4v5
+        total_layers: 1030
+      job_id: jqp4wr08g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.973754Z'
-  - torchscript_onnx_qnn:
-      inference_time: 50526.0
-      throughput: 19.79179036535645
+    timestamp: '2024-05-20T16:35:30.408294Z'
+  - torchscript_onnx_tflite:
+      inference_time: 54608.0
+      throughput: 18.312335188983297
+      estimated_peak_memory_range:
+        min: 3264512
+        max: 587498384
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1028
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1028
+      job_id: jvgdv1leg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 50248.0
+      throughput: 19.901289603566312
       estimated_peak_memory_range:
-        min: 53248
-        max: 259398784
+        min: 86016
+        max: 262075680
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1031
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1031
-      job_id: jz5w249j5
+      job_id: jnp18zx8g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 50628.0
-      throughput: 19.751915935845776
+      inference_time: 51447.0
+      throughput: 19.43747934767819
       estimated_peak_memory_range:
-        min: 7217152
-        max: 193898256
+        min: 6303744
+        max: 192645232
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 1030
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jvgde2vl5
+        total_layers: 1030
+      job_id: j0px1o23g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.973885Z'
-  - torchscript_onnx_qnn:
-      inference_time: 67718.0
-      throughput: 14.767122478513837
+    timestamp: '2024-05-20T16:35:30.408323Z'
+  - torchscript_onnx_tflite:
+      inference_time: 74054.0
+      throughput: 13.503659491722257
+      estimated_peak_memory_range:
+        min: 3284992
+        max: 5941440
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1028
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1028
+      job_id: jz5w96n3p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 64798.0
+      throughput: 15.432575079477761
       estimated_peak_memory_range:
-        min: 163840
-        max: 107805352
+        min: 102400
+        max: 107714376
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1031
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1031
-      job_id: j1p3vr7lg
+      job_id: jz57dr3v5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.974009Z'
+    timestamp: '2024-05-20T16:35:30.408341Z'
+  - torchscript_onnx_qnn:
+      inference_time: 73958.0
+      throughput: 13.521187701127667
+      estimated_peak_memory_range:
+        min: 217088
+        max: 217088
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1030
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1030
+      job_id: jvgdv1lrg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 65800.0
+      throughput: 15.19756838905775
+      estimated_peak_memory_range:
+        min: 1351680
+        max: 1351680
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1030
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1030
+      job_id: jo5mzxydp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 598980.0
+      throughput: 1.669504824868944
+      estimated_peak_memory_range:
+        min: 550260736
+        max: 550260736
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jegnev8kg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.408364Z'
```

## qai_hub_models/models/regnet/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/regnet/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: RegNet
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2314.0
-      throughput: 432.152117545376
+      inference_time: 2321.0
+      throughput: 430.8487720809996
       estimated_peak_memory_range:
-        min: 16384
-        max: 2190392
+        min: 28672
+        max: 2093984
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 114
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 114
-      job_id: jqp4k3xlg
+      job_id: jopry3j0g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2128.0
-      throughput: 469.9248120300752
+      inference_time: 2130.0
+      throughput: 469.4835680751174
       estimated_peak_memory_range:
-        min: 20480
-        max: 15932376
+        min: 16384
+        max: 16919216
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 188
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 188
-      job_id: jo5mq8wqp
+      job_id: j2p0r079p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2423.0
-      throughput: 412.71151465125877
+      inference_time: 2312.0
+      throughput: 432.52595155709344
       estimated_peak_memory_range:
-        min: 12288
-        max: 87079712
+        min: 49152
+        max: 79165336
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 190
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jopr8w4e5
+        total_layers: 190
+      job_id: j1glkmrjp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:32.991772Z'
+    timestamp: '2024-05-20T16:35:30.439253Z'
   - torchscript_onnx_tflite:
-      inference_time: 1616.0
-      throughput: 618.8118811881188
+      inference_time: 1625.0
+      throughput: 615.3846153846154
       estimated_peak_memory_range:
-        min: 12288
-        max: 134209840
+        min: 78073856
+        max: 211737456
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 114
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 114
-      job_id: j0pxnx795
+      job_id: jep2mynr5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1506.0
-      throughput: 664.0106241699867
+      inference_time: 1481.0
+      throughput: 675.219446320054
       estimated_peak_memory_range:
-        min: 618496
-        max: 77239488
+        min: 0
+        max: 72188080
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 188
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 188
-      job_id: jegnlk9m5
+      job_id: j1p87yvk5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1699.0
-      throughput: 588.5815185403178
+      inference_time: 1586.0
+      throughput: 630.517023959647
       estimated_peak_memory_range:
-        min: 618496
-        max: 36167024
+        min: 0
+        max: 38564464
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 190
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jep20e7mg
+        total_layers: 190
+      job_id: jw5614l6p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:32.991836Z'
+    timestamp: '2024-05-20T16:35:30.439280Z'
   - torchscript_onnx_tflite:
-      inference_time: 2329.0
-      throughput: 429.36882782310005
+      inference_time: 2331.0
+      throughput: 429.000429000429
       estimated_peak_memory_range:
-        min: 24576
-        max: 2315288
+        min: 32768
+        max: 2367144
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 114
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 114
-      job_id: jvgdemme5
+      job_id: jqpyd308p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2130.0
-      throughput: 469.4835680751174
+      inference_time: 2139.0
+      throughput: 467.50818139317437
       estimated_peak_memory_range:
-        min: 12288
-        max: 56502216
+        min: 16384
+        max: 66236928
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 188
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 188
-      job_id: jo5mqllwp
+      job_id: jn5q2qon5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:32.991902Z'
+    timestamp: '2024-05-20T16:35:30.439298Z'
+  - torchscript_onnx_qnn:
+      inference_time: 2466.0
+      throughput: 405.51500405515003
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 188
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 188
+      job_id: jogkyxmwp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 2190.0
+      throughput: 456.62100456621005
+      estimated_peak_memory_range:
+        min: 34840576
+        max: 34840576
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 190
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 190
+      job_id: j1p3m023g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 14744.0
+      throughput: 67.82419967444385
+      estimated_peak_memory_range:
+        min: 70148096
+        max: 70148096
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 85
+        total_layers: 85
+      job_id: jwgov6qq5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.439321Z'
```

## qai_hub_models/models/resnet101/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnet101/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ResNet101
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 3390.0
-      throughput: 294.9852507374631
+      inference_time: 3366.0
+      throughput: 297.08853238265004
       estimated_peak_memory_range:
-        min: 28672
-        max: 1775440
+        min: 36864
+        max: 2178824
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 147
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 147
-      job_id: j7gjzq085
+      job_id: jo5mznxdp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 3448.0
-      throughput: 290.0232018561485
+      inference_time: 3453.0
+      throughput: 289.6032435563278
       estimated_peak_memory_range:
-        min: 638976
-        max: 216598456
+        min: 618496
+        max: 173565024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 245
-      job_id: jygzonx65
+      job_id: jep2mkyr5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 3747.0
-      throughput: 266.88017080330934
+      inference_time: 3601.0
+      throughput: 277.700638711469
       estimated_peak_memory_range:
-        min: 618496
-        max: 366172984
+        min: 12288
+        max: 300122744
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 247
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jmg9jd3v5
+        total_layers: 247
+      job_id: jn5q26qn5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.051295Z'
+    timestamp: '2024-05-20T16:35:30.509475Z'
   - torchscript_onnx_tflite:
-      inference_time: 2446.0
-      throughput: 408.8307440719542
+      inference_time: 2430.0
+      throughput: 411.52263374485597
       estimated_peak_memory_range:
-        min: 212992
-        max: 104476752
+        min: 16384
+        max: 107021088
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 147
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 147
-      job_id: jlpeeyr0p
+      job_id: jegne6vkg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2469.0
-      throughput: 405.0222762251924
+      inference_time: 2501.0
+      throughput: 399.8400639744102
       estimated_peak_memory_range:
-        min: 434176
-        max: 81113840
+        min: 618496
+        max: 81769760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 245
-      job_id: jz5w24dj5
+      job_id: jqpyd138p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2676.0
-      throughput: 373.69207772795215
+      inference_time: 2626.0
+      throughput: 380.8073115003808
       estimated_peak_memory_range:
         min: 618496
-        max: 44227744
+        max: 47698672
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 247
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jnp1y6dlp
+        total_layers: 247
+      job_id: j1glkvmjp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.051366Z'
+    timestamp: '2024-05-20T16:35:30.509503Z'
   - torchscript_onnx_tflite:
-      inference_time: 3443.0
-      throughput: 290.4443799012489
+      inference_time: 3408.0
+      throughput: 293.42723004694835
       estimated_peak_memory_range:
         min: 24576
-        max: 2329152
+        max: 2314664
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 147
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 147
-      job_id: jep20zqrg
+      job_id: jopryv30g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 3473.0
-      throughput: 287.93550244745177
+      inference_time: 3469.0
+      throughput: 288.2675122513693
       estimated_peak_memory_range:
         min: 622592
-        max: 217592784
+        max: 173821024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 245
-      job_id: jn5qedxn5
+      job_id: j1p87qyk5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.051462Z'
+    timestamp: '2024-05-20T16:35:30.509521Z'
+  - torchscript_onnx_qnn:
+      inference_time: 3993.0
+      throughput: 250.4382669671926
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 245
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 245
+      job_id: j2p0rz09p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 3504.0
+      throughput: 285.38812785388126
+      estimated_peak_memory_range:
+        min: 56750080
+        max: 56750080
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 247
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 247
+      job_id: jw561y46p
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 28994.0
+      throughput: 34.48989446092295
+      estimated_peak_memory_range:
+        min: 51179520
+        max: 51179520
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 110
+        total_layers: 110
+      job_id: j1p3mj03g
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.509545Z'
```

## qai_hub_models/models/resnet101_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,20 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -203,14 +214,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnet101_quantized/perf.yaml

```diff
@@ -22,240 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ResNet101Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1171.0
-      throughput: 853.9709649871904
+      inference_time: 1181.0
+      throughput: 846.740050804403
       estimated_peak_memory_range:
-        min: 28672
-        max: 1746016
+        min: 40960
+        max: 2202864
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 148
-      job_id: jz5709vrg
+      job_id: jlpevddo5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1394.0
-      throughput: 717.3601147776184
+      inference_time: 1381.0
+      throughput: 724.112961622013
       estimated_peak_memory_range:
-        min: 12288
-        max: 186309248
+        min: 172032
+        max: 8857136
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 146
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 146
-      job_id: jopr8w1e5
+      job_id: jmg94llw5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1804.0
-      throughput: 554.3237250554324
+      inference_time: 1574.0
+      throughput: 635.3240152477764
       estimated_peak_memory_range:
-        min: 12288
-        max: 70503128
+        min: 28672
+        max: 151107432
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jogk78rop
+        total_layers: 154
+      job_id: jqp4wll8g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.075407Z'
+    timestamp: '2024-05-20T16:35:30.539412Z'
   - torchscript_onnx_tflite:
-      inference_time: 922.0
-      throughput: 1084.5986984815618
+      inference_time: 889.0
+      throughput: 1124.859392575928
       estimated_peak_memory_range:
-        min: 16384
-        max: 92718400
+        min: 12288
+        max: 92553280
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 148
-      job_id: jo5mq8vqp
+      job_id: jygz733op
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1061.0
-      throughput: 942.5070688030161
+      inference_time: 1045.0
+      throughput: 956.9377990430622
       estimated_peak_memory_range:
-        min: 167936
-        max: 59048544
+        min: 116203520
+        max: 179474976
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 146
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 146
-      job_id: jep20e3mg
+      job_id: jnp18448g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1380.0
-      throughput: 724.6376811594203
+      inference_time: 1217.0
+      throughput: 821.6926869350863
       estimated_peak_memory_range:
-        min: 618496
-        max: 46374032
+        min: 0
+        max: 43890864
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jn5qev9m5
+        total_layers: 154
+      job_id: j0px1kk3g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.075467Z'
+    timestamp: '2024-05-20T16:35:30.539439Z'
   - torchscript_onnx_tflite:
-      inference_time: 4806.0
-      throughput: 208.07324178110696
+      inference_time: 1190.0
+      throughput: 840.3361344537815
       estimated_peak_memory_range:
-        min: 24576
-        max: 27299616
+        min: 45056
+        max: 1732448
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 150
+        layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 150
-      job_id: jvgdemjr5
+        total_layers: 148
+      job_id: jz5w9ee3p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 1380.0
+      throughput: 724.6376811594203
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 20480
+        max: 100094264
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 146
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: j1gl6qnjg
-      job_status: Failed
-    torchscript_onnx_ort:
-      inference_time: 53190.0
-      throughput: 18.80052641473961
+        total_layers: 146
+      job_id: jz57dyyv5
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:30.539460Z'
+  - torchscript_onnx_tflite:
+      inference_time: 4782.0
+      throughput: 209.11752404851526
       estimated_peak_memory_range:
-        min: 12480512
-        max: 88971072
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 12288
+        max: 29359024
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 148
         layers_on_gpu: 0
-        layers_on_cpu: 156
-        total_layers: 156
-      job_id: j1gl6lelg
+        layers_on_cpu: 0
+        total_layers: 148
+      job_id: j2p0lx30p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 5013.0
+      throughput: 199.48134849391582
+      estimated_peak_memory_range:
+        min: 163840
+        max: 61142352
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: j1p3ervm5
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:33.075531Z'
+    timestamp: '2024-05-20T16:35:30.539477Z'
   - torchscript_onnx_tflite:
-      inference_time: 17430.0
-      throughput: 57.37234652897303
+      inference_time: 17166.0
+      throughput: 58.25468950250495
       estimated_peak_memory_range:
-        min: 16384
-        max: 2096256
+        min: 36864
+        max: 4633128
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 150
+        layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 150
-      job_id: jlpew6v7p
+        total_layers: 148
+      job_id: j1p8zk0qp
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:33.075558Z'
-  - torchscript_onnx_tflite:
-      inference_time: 1196.0
-      throughput: 836.1204013377926
+    timestamp: '2024-05-20T16:35:30.539488Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1408.0
+      throughput: 710.2272727272727
       estimated_peak_memory_range:
-        min: 24576
-        max: 2116496
+        min: 356352
+        max: 356352
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 150
+        layers_on_npu: 146
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 150
-      job_id: jygzo01o5
+        total_layers: 146
+      job_id: jvgdvxxrg
       job_status: Passed
-    torchscript_onnx_qnn:
-      inference_time: 1433.0
-      throughput: 697.8367062107467
+    torchscript_onnx_ort:
+      inference_time: 1445.0
+      throughput: 692.0415224913495
       estimated_peak_memory_range:
-        min: 61440
-        max: 10820048
+        min: 43081728
+        max: 43081728
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 148
+        layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 148
-      job_id: jlpeen9op
+        total_layers: 154
+      job_id: jo5mznndp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 6842.0
+      throughput: 146.15609470914936
+      estimated_peak_memory_range:
+        min: 1634304
+        max: 1634304
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 156
+        total_layers: 156
+      job_id: jegne66kg
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.075622Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.539511Z'
```

## qai_hub_models/models/resnet18/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnet18/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ResNet18
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1398.0
-      throughput: 715.307582260372
+      inference_time: 1410.0
+      throughput: 709.2198581560284
       estimated_peak_memory_range:
-        min: 24576
-        max: 2046480
+        min: 12288
+        max: 1495520
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 38
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 38
-      job_id: jw56ewq7g
+      job_id: jopryvv0g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1489.0
-      throughput: 671.591672263264
+      inference_time: 1471.0
+      throughput: 679.8096532970768
       estimated_peak_memory_range:
-        min: 12288
-        max: 83625152
+        min: 16384
+        max: 94295528
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 53
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 53
-      job_id: jwgok8edp
+      job_id: j2p0rzz9p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1543.0
-      throughput: 648.0881399870383
+      inference_time: 1335.0
+      throughput: 749.0636704119851
       estimated_peak_memory_range:
-        min: 16384
-        max: 82413040
+        min: 61440
+        max: 90905104
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 55
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j7gjzqk85
+        total_layers: 55
+      job_id: j1glkvvjp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.110933Z'
+    timestamp: '2024-05-20T16:35:30.578687Z'
   - torchscript_onnx_tflite:
-      inference_time: 987.0
-      throughput: 1013.1712259371834
+      inference_time: 981.0
+      throughput: 1019.367991845056
       estimated_peak_memory_range:
         min: 12288
-        max: 24202432
+        max: 24130336
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 38
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 38
-      job_id: j1p3v6qzg
+      job_id: jep2mkkr5
       job_status: Passed
     torchscript_onnx_qnn:
       inference_time: 1015.0
       throughput: 985.2216748768473
       estimated_peak_memory_range:
-        min: 0
-        max: 31898144
+        min: 618496
+        max: 30836368
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 53
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 53
-      job_id: j1pv07zm5
+      job_id: j1p87qqk5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1128.0
-      throughput: 886.5248226950355
+      inference_time: 947.0
+      throughput: 1055.9662090813094
       estimated_peak_memory_range:
-        min: 618496
-        max: 19073216
+        min: 0
+        max: 20884768
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 55
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jlpeey40p
+        total_layers: 55
+      job_id: jw561yy6p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.110984Z'
+    timestamp: '2024-05-20T16:35:30.578715Z'
   - torchscript_onnx_tflite:
-      inference_time: 1376.0
-      throughput: 726.7441860465116
+      inference_time: 1408.0
+      throughput: 710.2272727272727
       estimated_peak_memory_range:
-        min: 20480
-        max: 1963688
+        min: 24576
+        max: 1608360
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 38
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 38
-      job_id: j2p03xqnp
+      job_id: jqpyd118p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1485.0
-      throughput: 673.4006734006734
+      inference_time: 1473.0
+      throughput: 678.8866259334691
       estimated_peak_memory_range:
-        min: 16384
-        max: 83668248
+        min: 20480
+        max: 83818904
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 53
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 53
-      job_id: j1gl6qrmg
+      job_id: jn5q266n5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.111013Z'
+    timestamp: '2024-05-20T16:35:30.578738Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1572.0
+      throughput: 636.1323155216285
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 53
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 53
+      job_id: jogkyeewp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1329.0
+      throughput: 752.4454477050414
+      estimated_peak_memory_range:
+        min: 32423936
+        max: 32423936
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 55
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 55
+      job_id: j1p3mjj3g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 6023.0
+      throughput: 166.03021749958492
+      estimated_peak_memory_range:
+        min: 22114304
+        max: 22114304
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 26
+        total_layers: 26
+      job_id: jwgov22q5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.578760Z'
```

## qai_hub_models/models/resnet18_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,20 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -203,14 +214,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnet18_quantized/perf.yaml

```diff
@@ -22,240 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ResNet18Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 427.0
-      throughput: 2341.92037470726
+      inference_time: 421.0
+      throughput: 2375.296912114014
       estimated_peak_memory_range:
-        min: 24576
-        max: 14744816
+        min: 16384
+        max: 14552648
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 39
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 39
-      job_id: jz5w24mj5
+      job_id: j1pvw6qkg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 633.0
-      throughput: 1579.778830963665
+      inference_time: 636.0
+      throughput: 1572.3270440251572
       estimated_peak_memory_range:
         min: 16384
-        max: 61110464
+        max: 29686208
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 37
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 37
-      job_id: jnp1y6qlp
+      job_id: jygz732op
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 977.0
-      throughput: 1023.5414534288639
+      inference_time: 752.0
+      throughput: 1329.787234042553
       estimated_peak_memory_range:
-        min: 45056
-        max: 142126416
+        min: 24576
+        max: 30406712
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 45
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jz5w24x65
+        total_layers: 45
+      job_id: jvgdvxnrg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.135184Z'
+    timestamp: '2024-05-20T16:35:30.609007Z'
   - torchscript_onnx_tflite:
-      inference_time: 351.0
-      throughput: 2849.002849002849
+      inference_time: 343.0
+      throughput: 2915.451895043732
       estimated_peak_memory_range:
         min: 12288
-        max: 24268608
+        max: 23898080
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 39
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 39
-      job_id: jmg9jd9v5
+      job_id: j7gjlvdvp
       job_status: Passed
     torchscript_onnx_qnn:
       inference_time: 480.0
       throughput: 2083.3333333333335
       estimated_peak_memory_range:
-        min: 0
-        max: 26088768
+        min: 163840
+        max: 27124384
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 37
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 37
-      job_id: jvgde27l5
+      job_id: jz5w9ew3p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 750.0
-      throughput: 1333.3333333333333
+      inference_time: 631.0
+      throughput: 1584.7860538827258
       estimated_peak_memory_range:
         min: 0
-        max: 19250192
+        max: 21432704
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 45
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jmg9jd8l5
+        total_layers: 45
+      job_id: jz57dy2v5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.135231Z'
+    timestamp: '2024-05-20T16:35:30.609034Z'
   - torchscript_onnx_tflite:
-      inference_time: 1555.0
-      throughput: 643.0868167202573
+      inference_time: 419.0
+      throughput: 2386.634844868735
       estimated_peak_memory_range:
-        min: 16384
-        max: 14843920
+        min: 12288
+        max: 1584296
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 41
+        layers_on_npu: 39
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 41
-      job_id: jygzo0kx5
+        total_layers: 39
+      job_id: jlpevdoo5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 630.0
+      throughput: 1587.3015873015872
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 20480
+        max: 29451032
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 37
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jqpyry0l5
-      job_status: Failed
-    torchscript_onnx_ort:
-      inference_time: 11826.0
-      throughput: 84.5594452900389
+        total_layers: 37
+      job_id: jnp18428g
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:30.609055Z'
+  - torchscript_onnx_tflite:
+      inference_time: 1452.0
+      throughput: 688.7052341597796
       estimated_peak_memory_range:
-        min: 1556480
-        max: 29105488
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 12288
+        max: 14834800
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 39
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 39
+      job_id: jo5m3lqyg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1817.0
+      throughput: 550.357732526142
+      estimated_peak_memory_range:
+        min: 12288
+        max: 24293456
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 37
         layers_on_gpu: 0
-        layers_on_cpu: 47
-        total_layers: 47
-      job_id: jvgde20e5
+        layers_on_cpu: 0
+        total_layers: 37
+      job_id: j1p8zkmzp
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:33.135269Z'
+    timestamp: '2024-05-20T16:35:30.609072Z'
   - torchscript_onnx_tflite:
-      inference_time: 7308.0
-      throughput: 136.83634373289544
+      inference_time: 7043.0
+      throughput: 141.9849495953429
       estimated_peak_memory_range:
         min: 12288
-        max: 6786960
+        max: 6989920
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 41
+        layers_on_npu: 39
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 41
-      job_id: jygzjz7zp
+        total_layers: 39
+      job_id: jegn3wmv5
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:33.135284Z'
-  - torchscript_onnx_tflite:
-      inference_time: 463.0
-      throughput: 2159.827213822894
+    timestamp: '2024-05-20T16:35:30.609083Z'
+  - torchscript_onnx_qnn:
+      inference_time: 768.0
+      throughput: 1302.0833333333333
       estimated_peak_memory_range:
-        min: 20480
-        max: 15182520
+        min: 569344
+        max: 569344
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 41
+        layers_on_npu: 37
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 41
-      job_id: jlpeen3vp
+        total_layers: 37
+      job_id: jmg94l0w5
       job_status: Passed
-    torchscript_onnx_qnn:
-      inference_time: 680.0
-      throughput: 1470.5882352941176
+    torchscript_onnx_ort:
+      inference_time: 714.0
+      throughput: 1400.5602240896358
       estimated_peak_memory_range:
-        min: 24576
-        max: 60765408
+        min: 11710464
+        max: 11710464
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 39
+        layers_on_npu: 45
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 39
-      job_id: jo5mqly9p
+        total_layers: 45
+      job_id: jqp4wln8g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 143079.0
+      throughput: 6.989145856484879
+      estimated_peak_memory_range:
+        min: 7467008
+        max: 7467008
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j0px1k93g
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.135313Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.609107Z'
```

## qai_hub_models/models/resnet50/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnet50/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2302.0
-      throughput: 434.4048653344918
+      inference_time: 2272.0
+      throughput: 440.14084507042253
       estimated_peak_memory_range:
-        min: 20480
-        max: 2370264
+        min: 12288
+        max: 1939880
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 79
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 79
-      job_id: jqp4k38vg
+      job_id: jo5mznedp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2340.0
-      throughput: 427.35042735042737
+      inference_time: 2382.0
+      throughput: 419.81528127623847
       estimated_peak_memory_range:
-        min: 20480
-        max: 185567384
+        min: 622592
+        max: 186262680
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 126
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 126
-      job_id: jegnlkxr5
+      job_id: jep2mkxr5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2587.0
-      throughput: 386.5481252415926
+      inference_time: 2370.0
+      throughput: 421.9409282700422
       estimated_peak_memory_range:
         min: 12288
-        max: 217558712
+        max: 205580248
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 128
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jep20ej4g
+        total_layers: 128
+      job_id: jogkyevwp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.170270Z'
+    timestamp: '2024-05-20T16:35:30.648313Z'
   - torchscript_onnx_tflite:
-      inference_time: 1648.0
-      throughput: 606.7961165048544
+      inference_time: 1645.0
+      throughput: 607.90273556231
       estimated_peak_memory_range:
         min: 16384
-        max: 69510112
+        max: 70261792
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 79
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 79
-      job_id: jo5mq84wp
+      job_id: jegne60kg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1630.0
-      throughput: 613.4969325153374
+      inference_time: 1682.0
+      throughput: 594.5303210463734
       estimated_peak_memory_range:
         min: 618496
-        max: 51350896
+        max: 50091680
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 126
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 126
-      job_id: jopr8w995
+      job_id: jqpyd1z8p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1868.0
-      throughput: 535.3319057815846
+      inference_time: 1734.0
+      throughput: 576.7012687427913
       estimated_peak_memory_range:
-        min: 0
-        max: 35536992
+        min: 142139392
+        max: 174512736
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 128
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jqpyrmn75
+        total_layers: 128
+      job_id: jn5q260n5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.170321Z'
+    timestamp: '2024-05-20T16:35:30.648341Z'
   - torchscript_onnx_tflite:
-      inference_time: 2299.0
-      throughput: 434.97172683775557
+      inference_time: 2272.0
+      throughput: 440.14084507042253
       estimated_peak_memory_range:
-        min: 24576
-        max: 2160472
+        min: 28672
+        max: 2414432
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 79
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 79
-      job_id: jvgdey1z5
+      job_id: jopryv60g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2343.0
-      throughput: 426.8032437046522
+      inference_time: 2386.0
+      throughput: 419.11148365465215
       estimated_peak_memory_range:
-        min: 626688
-        max: 186221872
+        min: 618496
+        max: 186113032
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 126
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 126
-      job_id: jopr8m375
+      job_id: j1p87q2k5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.170363Z'
+    timestamp: '2024-05-20T16:35:30.648359Z'
+  - torchscript_onnx_qnn:
+      inference_time: 2691.0
+      throughput: 371.6090672612412
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: j2p0rz49p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 2284.0
+      throughput: 437.82837127845886
+      estimated_peak_memory_range:
+        min: 76500992
+        max: 76500992
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 128
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 128
+      job_id: j1glkv4jp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 15563.0
+      throughput: 64.2549636959455
+      estimated_peak_memory_range:
+        min: 40939520
+        max: 40939520
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 59
+        total_layers: 59
+      job_id: jw561y26p
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.648386Z'
```

## qai_hub_models/models/resnext101/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnext101/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ResNeXt101
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 6665.0
-      throughput: 150.03750937734435
+      inference_time: 6708.0
+      throughput: 149.0757304710793
       estimated_peak_memory_range:
-        min: 53248
-        max: 3235600
+        min: 24576
+        max: 2889376
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 147
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 147
-      job_id: jogk78o2p
+      job_id: jo5mzn69p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 6665.0
-      throughput: 150.03750937734435
+      inference_time: 6648.0
+      throughput: 150.42117930204572
       estimated_peak_memory_range:
-        min: 94208
-        max: 34973960
+        min: 16384
+        max: 35804344
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 245
-      job_id: j1gl6lo8g
+      job_id: jep2mk9q5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 7040.0
-      throughput: 142.04545454545453
+      inference_time: 6983.0
+      throughput: 143.20492624946297
       estimated_peak_memory_range:
-        min: 0
-        max: 454692632
+        min: 32768
+        max: 451743424
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 247
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1p3v6xlg
+        total_layers: 247
+      job_id: jogkyeqnp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.282825Z'
+    timestamp: '2024-05-20T16:35:30.789744Z'
   - torchscript_onnx_tflite:
-      inference_time: 4816.0
-      throughput: 207.64119601328903
+      inference_time: 4868.0
+      throughput: 205.42317173377157
       estimated_peak_memory_range:
         min: 20480
-        max: 366481792
+        max: 365272832
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 147
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 147
-      job_id: jn5qevz45
+      job_id: jegne6mqg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 4797.0
-      throughput: 208.46362309776944
+      inference_time: 4799.0
+      throughput: 208.37674515524068
       estimated_peak_memory_range:
-        min: 618496
-        max: 131176640
+        min: 0
+        max: 123278800
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 245
-      job_id: jw56ewr0g
+      job_id: jqpyd1jlp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 5231.0
-      throughput: 191.16803670426305
+      inference_time: 5124.0
+      throughput: 195.160031225605
       estimated_peak_memory_range:
-        min: 618496
-        max: 100656704
+        min: 626688
+        max: 90094496
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 247
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jwgok8oxp
+        total_layers: 247
+      job_id: jn5q26ro5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.282896Z'
+    timestamp: '2024-05-20T16:35:30.789769Z'
   - torchscript_onnx_tflite:
-      inference_time: 6712.0
-      throughput: 148.98688915375448
+      inference_time: 6665.0
+      throughput: 150.03750937734435
       estimated_peak_memory_range:
-        min: 36864
-        max: 3053288
+        min: 57344
+        max: 2680608
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 147
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 147
-      job_id: j7gjz6215
+      job_id: jopryv27g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 6586.0
-      throughput: 151.83723048891588
+      inference_time: 6622.0
+      throughput: 151.01177891875565
       estimated_peak_memory_range:
-        min: 16384
-        max: 36067624
+        min: 0
+        max: 37100696
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 245
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 245
-      job_id: jmg9j7ym5
+      job_id: j1p87qmo5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.282959Z'
+    timestamp: '2024-05-20T16:35:30.789786Z'
+  - torchscript_onnx_qnn:
+      inference_time: 9078.0
+      throughput: 110.15642211940956
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 245
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 245
+      job_id: j2p0rz2np
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 6736.0
+      throughput: 148.45605700712588
+      estimated_peak_memory_range:
+        min: 108900352
+        max: 108900352
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 247
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 247
+      job_id: j1glkv3mp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 63884.0
+      throughput: 15.653371736271993
+      estimated_peak_memory_range:
+        min: 101425152
+        max: 101425152
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 110
+        total_layers: 110
+      job_id: jw561ynyp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.789808Z'
```

## qai_hub_models/models/resnext101_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,20 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -203,14 +214,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnext101_quantized/perf.yaml

```diff
@@ -22,180 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ResNeXt101Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2913.0
-      throughput: 343.2887058015791
+      inference_time: 3033.0
+      throughput: 329.7065611605671
       estimated_peak_memory_range:
-        min: 24576
-        max: 1706912
+        min: 16384
+        max: 2184152
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 148
-      job_id: j7gjzqox5
+      job_id: jlpevdkv5
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 3921.0
-      throughput: 255.03698036215252
+    torchscript_onnx_qnn:
+      inference_time: 3107.0
+      throughput: 321.853878339234
       estimated_peak_memory_range:
         min: 12288
-        max: 136560960
+        max: 32784840
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jmg94lw85
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 3421.0
+      throughput: 292.3121894182987
+      estimated_peak_memory_range:
+        min: 0
+        max: 137016264
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jygzon8k5
+        total_layers: 154
+      job_id: jqp4wlv1g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.306938Z'
+    timestamp: '2024-05-20T16:35:30.820290Z'
   - torchscript_onnx_tflite:
-      inference_time: 2167.0
-      throughput: 461.4674665436087
+      inference_time: 2053.0
+      throughput: 487.0920603994155
       estimated_peak_memory_range:
         min: 12288
-        max: 262604528
+        max: 258014032
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 148
-      job_id: jlpeey81p
+      job_id: jygz73rxp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 2280.0
+      throughput: 438.5964912280702
+      estimated_peak_memory_range:
+        min: 12288
+        max: 118044256
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jnp184e7g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2990.0
-      throughput: 334.44816053511704
+      inference_time: 2540.0
+      throughput: 393.7007874015748
       estimated_peak_memory_range:
         min: 618496
-        max: 95251808
+        max: 92001632
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 154
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jz5w24165
+        total_layers: 154
+      job_id: j0px1kylg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.306993Z'
+    timestamp: '2024-05-20T16:35:30.820316Z'
   - torchscript_onnx_tflite:
-      inference_time: 10468.0
-      throughput: 95.52923194497517
+      inference_time: 2932.0
+      throughput: 341.06412005457025
       estimated_peak_memory_range:
-        min: 32768
-        max: 199144352
+        min: 24576
+        max: 2554384
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 150
+        layers_on_npu: 148
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 150
-      job_id: jlpee0v8p
+        total_layers: 148
+      job_id: jz5w9eqmp
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 88885.0
-      throughput: 11.250492209034146
+    torchscript_onnx_qnn:
+      inference_time: 3081.0
+      throughput: 324.5699448231094
       estimated_peak_memory_range:
-        min: 8159232
-        max: 88001424
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 16384
+        max: 35435296
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 146
         layers_on_gpu: 0
-        layers_on_cpu: 156
-        total_layers: 156
-      job_id: jmg9jdxl5
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jz57dyx95
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:30.820333Z'
+  - torchscript_onnx_tflite:
+      inference_time: 10331.0
+      throughput: 96.79605072113058
+      estimated_peak_memory_range:
+        min: 45056
+        max: 199157040
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 148
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 148
+      job_id: jygzr07z5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 11010.0
+      throughput: 90.82652134423252
+      estimated_peak_memory_range:
+        min: 167936
+        max: 124990144
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 146
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 146
+      job_id: jqp4v2wqp
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:33.307043Z'
+    timestamp: '2024-05-20T16:35:30.820350Z'
   - torchscript_onnx_tflite:
-      inference_time: 134216.0
-      throughput: 7.450676521428146
+      inference_time: 133798.0
+      throughput: 7.473953272844138
       estimated_peak_memory_range:
-        min: 24576
-        max: 357047544
+        min: 184320
+        max: 355878408
       primary_compute_unit: GPU
       precision: int8
       layer_info:
-        layers_on_npu: 14
+        layers_on_npu: 12
         layers_on_gpu: 125
         layers_on_cpu: 11
-        total_layers: 150
-      job_id: jmg9yo4q5
+        total_layers: 148
+      job_id: jz5wqr9z5
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:33.307071Z'
-  - torchscript_onnx_tflite:
-      inference_time: 2909.0
-      throughput: 343.7607425232039
+    timestamp: '2024-05-20T16:35:30.820361Z'
+  - torchscript_onnx_qnn:
+      inference_time: 3328.0
+      throughput: 300.4807692307692
       estimated_peak_memory_range:
-        min: 16384
-        max: 2753672
+        min: 249856
+        max: 249856
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 150
+        layers_on_npu: 146
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 150
-      job_id: j1p3vlqxg
+        total_layers: 146
+      job_id: jvgdvxozg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 3366.0
+      throughput: 297.08853238265004
+      estimated_peak_memory_range:
+        min: 137375744
+        max: 137375744
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 154
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 154
+      job_id: jo5mzn39p
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 228816.0
+      throughput: 4.370323753583666
+      estimated_peak_memory_range:
+        min: 1384448
+        max: 1384448
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 156
+        total_layers: 156
+      job_id: jegne63qg
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.307097Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.820384Z'
```

## qai_hub_models/models/resnext50/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnext50/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ResNeXt50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2502.0
-      throughput: 399.68025579536373
+      inference_time: 2512.0
+      throughput: 398.0891719745223
       estimated_peak_memory_range:
-        min: 16384
-        max: 2039136
+        min: 12288
+        max: 2465560
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 79
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 79
-      job_id: jnp1y6v2p
+      job_id: jopryve7g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2619.0
-      throughput: 381.82512409316536
+      inference_time: 2556.0
+      throughput: 391.23630672926447
       estimated_peak_memory_range:
-        min: 12288
-        max: 67332096
+        min: 16384
+        max: 87753520
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 126
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 126
-      job_id: jz57097lg
+      job_id: j2p0rzlnp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2938.0
-      throughput: 340.3675970047652
+      inference_time: 2844.0
+      throughput: 351.6174402250352
       estimated_peak_memory_range:
-        min: 90112
-        max: 153500352
+        min: 229376
+        max: 171515144
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 128
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j0pxnxd15
+        total_layers: 128
+      job_id: j1glkvkmp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.333746Z'
+    timestamp: '2024-05-20T16:35:30.859710Z'
   - torchscript_onnx_tflite:
-      inference_time: 1788.0
-      throughput: 559.2841163310962
+      inference_time: 1790.0
+      throughput: 558.659217877095
       estimated_peak_memory_range:
         min: 16384
-        max: 164107600
+        max: 160881424
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 79
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 79
-      job_id: jvgde2ze5
+      job_id: jep2mklq5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1857.0
-      throughput: 538.5029617662897
+      inference_time: 1858.0
+      throughput: 538.2131324004306
       estimated_peak_memory_range:
-        min: 0
-        max: 60102256
+        min: 618496
+        max: 60637072
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 126
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 126
-      job_id: jqp4k39vg
+      job_id: j1p87qzo5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2158.0
-      throughput: 463.3920296570899
+      inference_time: 2037.0
+      throughput: 490.9180166912126
       estimated_peak_memory_range:
         min: 618496
-        max: 42526736
+        max: 41012496
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 128
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jo5mq8dwp
+        total_layers: 128
+      job_id: jw561y1yp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.333800Z'
+    timestamp: '2024-05-20T16:35:30.859737Z'
   - torchscript_onnx_tflite:
-      inference_time: 2497.0
-      throughput: 400.4805766920304
+      inference_time: 2499.0
+      throughput: 400.16006402561027
       estimated_peak_memory_range:
-        min: 53248
-        max: 2221936
+        min: 24576
+        max: 2189296
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 79
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 79
-      job_id: jnp1yk3kp
+      job_id: jqpyd16lp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2594.0
-      throughput: 385.50501156515037
+      inference_time: 2548.0
+      throughput: 392.4646781789639
       estimated_peak_memory_range:
-        min: 618496
-        max: 68165536
+        min: 622592
+        max: 88624416
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 126
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 126
-      job_id: j0pxn8mj5
+      job_id: jn5q263o5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.333840Z'
+    timestamp: '2024-05-20T16:35:30.859755Z'
+  - torchscript_onnx_qnn:
+      inference_time: 2925.0
+      throughput: 341.88034188034186
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: jogkye3np
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 2645.0
+      throughput: 378.0718336483932
+      estimated_peak_memory_range:
+        min: 75046912
+        max: 75046912
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 128
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 128
+      job_id: j1p3mjmng
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 23055.0
+      throughput: 43.37453914552158
+      estimated_peak_memory_range:
+        min: 31170560
+        max: 31170560
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 59
+        total_layers: 59
+      job_id: jwgov2vk5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.859779Z'
```

## qai_hub_models/models/resnext50_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,20 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -203,14 +214,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/resnext50_quantized/perf.yaml

```diff
@@ -22,180 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: ResNeXt50Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 949.0
-      throughput: 1053.740779768177
+      inference_time: 945.0
+      throughput: 1058.2010582010582
       estimated_peak_memory_range:
-        min: 40960
-        max: 32336880
+        min: 16384
+        max: 1978440
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 80
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 80
-      job_id: jopr8wn95
+      job_id: j1pvw6wrg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1182.0
+      throughput: 846.0236886632825
+      estimated_peak_memory_range:
+        min: 12288
+        max: 98529472
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 78
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 78
+      job_id: jygz737xp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1749.0
-      throughput: 571.7552887364208
+      inference_time: 1456.0
+      throughput: 686.8131868131868
       estimated_peak_memory_range:
         min: 12288
-        max: 65405552
+        max: 110506920
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jqpyrm775
+        total_layers: 86
+      job_id: jvgdvxvzg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.357768Z'
+    timestamp: '2024-05-20T16:35:30.890153Z'
   - torchscript_onnx_tflite:
-      inference_time: 724.0
-      throughput: 1381.2154696132598
+      inference_time: 710.0
+      throughput: 1408.4507042253522
       estimated_peak_memory_range:
         min: 12288
-        max: 99522896
+        max: 99630928
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 80
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 80
-      job_id: jep20ev4g
+      job_id: j7gjlvlep
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 882.0
+      throughput: 1133.7868480725624
+      estimated_peak_memory_range:
+        min: 167936
+        max: 57911616
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 78
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 78
+      job_id: jz5w9e9mp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1274.0
-      throughput: 784.9293563579278
+      inference_time: 1110.0
+      throughput: 900.9009009009009
       estimated_peak_memory_range:
         min: 0
-        max: 42945536
+        max: 41473776
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j2p036v6p
+        total_layers: 86
+      job_id: jz57dyd95
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.357805Z'
+    timestamp: '2024-05-20T16:35:30.890180Z'
   - torchscript_onnx_tflite:
-      inference_time: 3105.0
-      throughput: 322.061191626409
+      inference_time: 940.0
+      throughput: 1063.8297872340424
       estimated_peak_memory_range:
         min: 12288
-        max: 54933392
+        max: 1980696
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 82
+        layers_on_npu: 80
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 82
-      job_id: jygzoq8z5
+        total_layers: 80
+      job_id: jlpevdvv5
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 31790.0
-      throughput: 31.456432840515884
+    torchscript_onnx_qnn:
+      inference_time: 1178.0
+      throughput: 848.8964346349745
       estimated_peak_memory_range:
-        min: 8765440
-        max: 56053712
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 167936
+        max: 10889000
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 78
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 78
+      job_id: jnp18487g
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:30.890198Z'
+  - torchscript_onnx_tflite:
+      inference_time: 3222.0
+      throughput: 310.36623215394167
+      estimated_peak_memory_range:
+        min: 16384
+        max: 54803712
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 80
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 80
+      job_id: jlpeknr7p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 3456.0
+      throughput: 289.35185185185185
+      estimated_peak_memory_range:
+        min: 163840
+        max: 51993520
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 78
         layers_on_gpu: 0
-        layers_on_cpu: 88
-        total_layers: 88
-      job_id: j1p8014xg
+        layers_on_cpu: 0
+        total_layers: 78
+      job_id: jz5wqrdj5
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:33.357842Z'
+    timestamp: '2024-05-20T16:35:30.890215Z'
   - torchscript_onnx_tflite:
-      inference_time: 64556.0
-      throughput: 15.49042691616581
+      inference_time: 65861.0
+      throughput: 15.183492506946449
       estimated_peak_memory_range:
-        min: 0
-        max: 94711912
+        min: 8355840
+        max: 26461872
       primary_compute_unit: GPU
       precision: int8
       layer_info:
-        layers_on_npu: 14
+        layers_on_npu: 12
         layers_on_gpu: 57
         layers_on_cpu: 11
-        total_layers: 82
-      job_id: jnp1wo8kg
+        total_layers: 80
+      job_id: jygzr0xz5
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:33.357866Z'
-  - torchscript_onnx_tflite:
-      inference_time: 987.0
-      throughput: 1013.1712259371834
+    timestamp: '2024-05-20T16:35:30.890226Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1353.0
+      throughput: 739.0983000739099
+      estimated_peak_memory_range:
+        min: 438272
+        max: 438272
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 78
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 78
+      job_id: jmg94l485
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1402.0
+      throughput: 713.2667617689016
       estimated_peak_memory_range:
-        min: 24576
-        max: 1688688
+        min: 52183040
+        max: 52183040
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 82
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 82
-      job_id: jogk7woyp
+        total_layers: 86
+      job_id: jqp4wlw1g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 202223.0
+      throughput: 4.945035925686
+      estimated_peak_memory_range:
+        min: 20660224
+        max: 20660224
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j0px1k1lg
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.357887Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.890248Z'
```

## qai_hub_models/models/sam/export.py

```diff
@@ -27,15 +27,14 @@
 )
 from qai_hub_models.utils.qai_hub_helpers import (
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
 )
 
 ALL_COMPONENTS = ["SAMDecoder", "SAMEncoder"]
-DEFAULT_COMPONENTS = ["SAMDecoder"]
 
 
 def export_model(
     device: str = "Samsung Galaxy S23",
     chipset: Optional[str] = None,
     components: Optional[List[str]] = None,
     skip_profiling: bool = False,
@@ -93,15 +92,15 @@
     model_name = "sam"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
     if chipset:
         hub_device = hub.Device(attributes=f"chipset:{chipset}")
     else:
         hub_device = hub.Device(name=device)
     component_arg = components
-    components = components or DEFAULT_COMPONENTS
+    components = components or ALL_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
             "sam",
             "Segment-Anything-Model",
@@ -141,15 +140,15 @@
                 mobile_optimizer.MobileOptimizerType.INSERT_FOLD_PREPACK_OPS,  # type: ignore
                 mobile_optimizer.MobileOptimizerType.CONV_BN_FUSION,  # type: ignore
             },
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options
+            target_runtime, compile_options, hub_device
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub_device,
             name=f"{model_name}_{component_name}",
```

## qai_hub_models/models/sam/model.py

```diff
@@ -2,24 +2,24 @@
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import os
 import sys
-import tempfile
 from typing import Callable, Tuple
 
 import numpy as np
 import torch
 
 from qai_hub_models.utils.asset_loaders import (
     CachedWebModelAsset,
     load_path,
     maybe_clone_git_repo,
+    qaihm_temp_dir,
 )
 from qai_hub_models.utils.base_model import BaseModel, CollectionModel
 from qai_hub_models.utils.input_spec import InputSpec
 
 # This is a fork of https://github.com/facebookresearch/segment-anything
 # with changes to make the SAM decoder traceable
 SAM_SOURCE_REPO = "https://github.com/dmckinnon/segment-anything"
@@ -286,15 +286,15 @@
 
 
 def load_sam_model(
     sam_model_registry, model_type: str = DEFAULT_MODEL_TYPE
 ) -> torch.nn.Module:
     """Loads SAM model of given model type"""
     weights_url = _get_weights_url(model_type)
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         weights_path = load_path(weights_url, tmpdir)
         sam = sam_model_registry[model_type](weights_path)
     sam.eval()
     return sam
 
 
 def _patch_sam_with_qaihm_modules():
@@ -307,16 +307,18 @@
         SamOnnxModel: torch.nn.Module
             light-weight decoder with fix image size
         ResizeLongestSide: segment_anything.utils.transforms.ResizeLongestSide
             Resizing utility updated to work with input image size
         SamPredictor: segment_anything.SamPredictor
             Python class wrapper to call image encoder - decoder
     """
-    sam_repo_path = maybe_clone_git_repo(
-        SAM_SOURCE_REPO, SAM_SOURCE_REPO_COMMIT, MODEL_ID, MODEL_ASSET_VERSION
+    sam_repo_path = str(
+        maybe_clone_git_repo(
+            SAM_SOURCE_REPO, SAM_SOURCE_REPO_COMMIT, MODEL_ID, MODEL_ASSET_VERSION
+        )
     )
     cwd = os.getcwd()
     try:
         # Patch path for this load only
         sys.path.insert(0, sam_repo_path)
 
         # import required modules and utilities
```

## qai_hub_models/models/sam/perf.yaml

```diff
@@ -18,117 +18,296 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: SAMDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 47957.0
-      throughput: 20.852013261880433
+      inference_time: 48417.0
+      throughput: 20.653902554887747
       estimated_peak_memory_range:
-        min: 4009984
-        max: 23686696
+        min: 4046848
+        max: 13471792
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 340
+        layers_on_npu: 342
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 340
-      job_id: jogk7892p
+        total_layers: 342
+      job_id: jo5mznz9p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1089085.0
-      throughput: 0.9182019768888563
+      inference_time: 35687.0
+      throughput: 28.021408355983972
       estimated_peak_memory_range:
-        min: 15695872
-        max: 53847464
+        min: 21266432
+        max: 62118592
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 2
+        layers_on_npu: 351
         layers_on_gpu: 0
         layers_on_cpu: 1
-        total_layers: 3
-      job_id: j1gl6l18g
+        total_layers: 352
+      job_id: j1p87q7o5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.384775Z'
+    timestamp: '2024-05-20T16:35:30.929656Z'
   - torchscript_onnx_tflite:
-      inference_time: 33609.0
-      throughput: 29.75393495789818
+      inference_time: 34847.0
+      throughput: 28.696874910322265
       estimated_peak_memory_range:
-        min: 61440
-        max: 246507888
+        min: 2396160
+        max: 250176160
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 340
+        layers_on_npu: 342
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 340
-      job_id: jn5qevm45
+        total_layers: 342
+      job_id: jopryvy7g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 809800.0
-      throughput: 1.2348728081007656
+      inference_time: 25375.0
+      throughput: 39.40886699507389
       estimated_peak_memory_range:
-        min: 19857408
-        max: 115862864
+        min: 27185152
+        max: 114627488
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 2
+        layers_on_npu: 351
         layers_on_gpu: 0
         layers_on_cpu: 1
-        total_layers: 3
-      job_id: jw56ewd0g
+        total_layers: 352
+      job_id: jn5q262o5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.384834Z'
+    timestamp: '2024-05-20T16:35:30.929678Z'
   - torchscript_onnx_tflite:
-      inference_time: 48295.0
-      throughput: 20.706077233668083
+      inference_time: 48322.0
+      throughput: 20.694507677662347
       estimated_peak_memory_range:
-        min: 3977216
-        max: 12384360
+        min: 4030464
+        max: 7393624
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 340
+        layers_on_npu: 342
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 340
-      job_id: jnp1yk7kp
+        total_layers: 342
+      job_id: jqpyd1dlp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.384881Z'
+    timestamp: '2024-05-20T16:35:30.929690Z'
+  - torchscript_onnx_ort:
+      inference_time: 35991.0
+      throughput: 27.78472395876747
+      estimated_peak_memory_range:
+        min: 38920192
+        max: 38920192
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 351
+        layers_on_gpu: 0
+        layers_on_cpu: 1
+        total_layers: 352
+      job_id: jw561y3yp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jwgov21k5
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.929711Z'
+- name: SAMEncoder
+  performance_metrics:
+  - torchscript_onnx_tflite:
+      inference_time: 12002934.0
+      throughput: 0.08331296331380311
+      estimated_peak_memory_range:
+        min: 2745298944
+        max: 2749256400
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 37
+        layers_on_cpu: 771
+        total_layers: 808
+      job_id: jegne6eqg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jogkyeynp
+      job_status: Failed
+    reference_device_info:
+      name: Samsung Galaxy S23
+      os: '13'
+      form_factor: Phone
+      os_name: Android
+      manufacturer: Samsung
+      chipset: Snapdragon 8 Gen 2
+    timestamp: '2024-05-20T16:35:30.929729Z'
+  - torchscript_onnx_tflite:
+      inference_time: 10788785.0
+      throughput: 0.09268884309030165
+      estimated_peak_memory_range:
+        min: 2551681024
+        max: 2911589120
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 37
+        layers_on_cpu: 771
+        total_layers: 808
+      job_id: jep2mkmq5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1glkv0mp
+      job_status: Failed
+    reference_device_info:
+      name: Samsung Galaxy S24
+      os: '14'
+      form_factor: Phone
+      os_name: Android
+      manufacturer: Samsung
+      chipset: Snapdragon 8 Gen 3
+    timestamp: '2024-05-20T16:35:30.929747Z'
+  - torchscript_onnx_tflite:
+      inference_time: 11903922.0
+      throughput: 0.08400592678614661
+      estimated_peak_memory_range:
+        min: 2721533952
+        max: 2726534168
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 37
+        layers_on_cpu: 771
+        total_layers: 808
+      job_id: j2p0rzrnp
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:30.929757Z'
+  - torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1p3mj4ng
+      job_status: Failed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1pvw61rg
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.929773Z'
```

## qai_hub_models/models/sesr_m5/export.py

```diff
@@ -115,20 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -158,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -187,24 +193,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/sesr_m5/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: SESR-M5
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2236.0
-      throughput: 447.2271914132379
+      inference_time: 2229.0
+      throughput: 448.63167339614176
       estimated_peak_memory_range:
-        min: 24576
-        max: 1639560
+        min: 28672
+        max: 1751584
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 22
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 25
-      job_id: jwgok84xp
+      job_id: j7gjlv0ep
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2141.0
-      throughput: 467.07146193367583
+      inference_time: 2149.0
+      throughput: 465.33271288971616
       estimated_peak_memory_range:
-        min: 217088
-        max: 66412728
+        min: 24576
+        max: 3705880
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: j7gjzqwx5
+      job_id: jz5w9edmp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2959.0
-      throughput: 337.95201081446436
+      inference_time: 2907.0
+      throughput: 343.9972480220158
       estimated_peak_memory_range:
-        min: 28672
-        max: 6879728
+        min: 12288
+        max: 5644152
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 33
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jygzon4k5
+        total_layers: 33
+      job_id: jz5w9ed4p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.402615Z'
+    timestamp: '2024-05-20T16:35:30.969933Z'
   - torchscript_onnx_tflite:
-      inference_time: 1608.0
-      throughput: 621.8905472636816
+      inference_time: 1652.0
+      throughput: 605.3268765133172
       estimated_peak_memory_range:
         min: 16384
-        max: 24474768
+        max: 24934032
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 22
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 25
-      job_id: j1pv079j5
+      job_id: jlpevdrv5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1452.0
-      throughput: 688.7052341597796
+      inference_time: 1450.0
+      throughput: 689.6551724137931
       estimated_peak_memory_range:
-        min: 208896
-        max: 24978944
+        min: 9527296
+        max: 32336704
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: jlpeeyl1p
+      job_id: jmg94l385
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2024.0
-      throughput: 494.0711462450593
+      inference_time: 'null'
+      throughput: 'null'
       estimated_peak_memory_range:
-        min: 208896
-        max: 16041184
-      primary_compute_unit: NPU
-      precision: fp16
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jz5w24465
-      job_status: Passed
+        total_layers: 0
+      job_id: jmg94l3m5
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.402651Z'
+    timestamp: '2024-05-20T16:35:30.969973Z'
   - torchscript_onnx_tflite:
-      inference_time: 2223.0
-      throughput: 449.842555105713
+      inference_time: 2266.0
+      throughput: 441.306266548985
       estimated_peak_memory_range:
-        min: 20480
-        max: 8844744
+        min: 12607488
+        max: 14159192
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 22
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 25
-      job_id: j0pxn8d95
+      job_id: jygz73xxp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2148.0
-      throughput: 465.54934823091247
+      inference_time: 2141.0
+      throughput: 467.07146193367583
       estimated_peak_memory_range:
-        min: 229376
-        max: 4684448
+        min: 221184
+        max: 4063112
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: jep20qvmg
+      job_id: jvgdvxrzg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.402678Z'
+    timestamp: '2024-05-20T16:35:30.969990Z'
+  - torchscript_onnx_qnn:
+      inference_time: 2969.0
+      throughput: 336.81374200067364
+      estimated_peak_memory_range:
+        min: 212992
+        max: 212992
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 31
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 31
+      job_id: jnp184d7g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 2971.0
+      throughput: 336.58700774150117
+      estimated_peak_memory_range:
+        min: 13090816
+        max: 13090816
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 33
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 33
+      job_id: jnp184dng
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 17098.0
+      throughput: 58.486372675166685
+      estimated_peak_memory_range:
+        min: 83427328
+        max: 83427328
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jvgdvxr6g
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:30.970016Z'
```

## qai_hub_models/models/sesr_m5_quantized/export.py

```diff
@@ -118,17 +118,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -162,16 +169,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -197,14 +206,14 @@
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/sesr_m5_quantized/model.py

```diff
@@ -10,99 +10,91 @@
     AIMETQuantizableMixin,
 )
 
 # isort: on
 
 import torch
 from aimet_torch.cross_layer_equalization import equalize_model
+from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
 from qai_hub_models.models._shared.sesr.common import _load_sesr_source_model
-from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.sesr_m5.model import (
     NUM_CHANNELS,
     NUM_LBLOCKS,
     SCALING_FACTOR,
     SESR_M5,
 )
+from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
+from qai_hub_models.utils.quantization_aimet import (
+    constrain_quantized_inputs_to_image_range,
+)
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 2
+MODEL_ASSET_VERSION = 3
 
 # Weights and config stored in S3 are sourced from
 # https://github.com/quic/aimet-model-zoo/blob/develop/aimet_zoo_torch/sesr/model/model_cards/sesr_m5_4x_w8a8.json:
 # https://github.com/quic/aimet-model-zoo/releases/download/phase_2_january_artifacts/sesr_m5_4x_checkpoint_int8.pth
 # and
 # https://raw.githubusercontent.com/quic/aimet/release-aimet-1.23/TrainingExtensions/common/src/python/aimet_common/quantsim_config/default_config_per_channel.js
 # Encodings were generated with AIMET QuantSim library
 QUANTIZED_WEIGHTS = "sesr_m5_4x_checkpoint_int8.pth"
 AIMET_ENCODINGS = "sesr_m5_quantized_encodings.json"
-AIMET_CONFIG = "default_config_per_channel.json"
 
 
 class SESR_M5Quantizable(AIMETQuantizableMixin, SESR_M5):
     """QuickSRNetLarge with post train quantization support.
 
     Supports only 8 bit weights and activations, and only loads pre-quantized checkpoints.
     Support for quantizing using your own weights & data will come at a later date."""
 
     def __init__(
         self,
         sesr_model: QuantizationSimModel,
     ) -> None:
         SESR_M5.__init__(self, sesr_model.model)
-        AIMETQuantizableMixin.__init__(
-            self, sesr_model, needs_onnx_direct_aimet_export=False
-        )
+        AIMETQuantizableMixin.__init__(self, sesr_model)
 
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> SESR_M5Quantizable:
         # Load Model
         sesr = _load_sesr_source_model(SCALING_FACTOR, NUM_CHANNELS, NUM_LBLOCKS)
+        # The model is collapsed pre-quantization - see
+        # https://github.com/quic/aimet-model-zoo/blob/d09d2b0404d10f71a7640a87e9d5e5257b028802/aimet_zoo_torch/common/super_resolution/models.py#L110
+        sesr.collapse()
         input_shape = SESR_M5.get_input_spec()["image"][0]
+        sesr = prepare_model(sesr)
         equalize_model(sesr, input_shape)
 
         # Download weights and quantization parameters
         weights = CachedWebModelAsset.from_asset_store(
             MODEL_ID, MODEL_ASSET_VERSION, QUANTIZED_WEIGHTS
         ).fetch()
-        aimet_config = CachedWebModelAsset.from_asset_store(
-            MODEL_ID, MODEL_ASSET_VERSION, AIMET_CONFIG
-        ).fetch()
 
         # Load the model weights and quantization parameters
         state_dict = torch.load(weights, map_location=torch.device("cpu"))["state_dict"]
-        # Here we collapse before loading the quantized weights.
-        # The model is collapsed pre-quantization - see
-        # https://github.com/quic/aimet-model-zoo/blob/d09d2b0404d10f71a7640a87e9d5e5257b028802/aimet_zoo_torch/common/super_resolution/models.py#L110
-        sesr.collapse()
         sesr.load_state_dict(state_dict)
         sim = QuantizationSimModel(
             sesr,
             quant_scheme="tf_enhanced",
             default_param_bw=8,
             default_output_bw=8,
-            config_file=aimet_config,
+            config_file=get_default_aimet_config(),
             dummy_input=torch.rand(input_shape),
         )
+        constrain_quantized_inputs_to_image_range(sim)
+
         if aimet_encodings:
             if aimet_encodings == "DEFAULT":
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
                     MODEL_ID, MODEL_ASSET_VERSION, AIMET_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         sim.model.eval()
 
         return cls(sim)
-
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        if target_runtime == TargetRuntime.QNN:
-            return SourceModelFormat.ONNX
-        else:
-            return SourceModelFormat.TORCHSCRIPT
```

## qai_hub_models/models/sesr_m5_quantized/perf.yaml

```diff
@@ -22,135 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: SESR-M5-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1356.0
-      throughput: 737.4631268436578
+      inference_time: 1329.0
+      throughput: 752.4454477050414
       estimated_peak_memory_range:
-        min: 24576
-        max: 1678184
+        min: 32768
+        max: 2149856
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 11
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 14
-      job_id: jnp1y662p
+      job_id: jz57dyjn5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 774.0
+      throughput: 1291.9896640826873
+      estimated_peak_memory_range:
+        min: 28672
+        max: 18606256
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 14
+      job_id: jegne69jg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1153.0
+      throughput: 867.3026886383348
+      estimated_peak_memory_range:
+        min: 2109440
+        max: 19388976
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 19
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 19
+      job_id: j2p0rze0p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.426676Z'
+    timestamp: '2024-05-20T16:35:31.000448Z'
   - torchscript_onnx_tflite:
-      inference_time: 1067.0
-      throughput: 937.207122774133
+      inference_time: 1111.0
+      throughput: 900.0900090009001
       estimated_peak_memory_range:
         min: 12288
-        max: 21689744
+        max: 21726352
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 11
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 14
-      job_id: jvgde22e5
+      job_id: jqp4wlx2g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 530.0
+      throughput: 1886.7924528301887
+      estimated_peak_memory_range:
+        min: 65536
+        max: 16933392
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 14
+      job_id: jopryv4kg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 834.0
+      throughput: 1199.0407673860911
+      estimated_peak_memory_range:
+        min: 212992
+        max: 13346208
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 19
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 19
+      job_id: j1p87qwq5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.426696Z'
+    timestamp: '2024-05-20T16:35:31.000474Z'
   - torchscript_onnx_tflite:
-      inference_time: 3752.0
-      throughput: 266.52452025586354
+      inference_time: 1328.0
+      throughput: 753.0120481927711
       estimated_peak_memory_range:
-        min: 49152
-        max: 14587664
+        min: 24576
+        max: 1624240
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 13
+        layers_on_npu: 11
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 16
-      job_id: jwgok74dp
+        total_layers: 14
+      job_id: j0px1k78g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 778.0
+      throughput: 1285.3470437017995
+      estimated_peak_memory_range:
+        min: 28672
+        max: 12397048
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 14
+      job_id: jqpyd140p
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:31.000504Z'
+  - torchscript_onnx_tflite:
+      inference_time: 3342.0
+      throughput: 299.22202274087374
+      estimated_peak_memory_range:
+        min: 45056
+        max: 14433024
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 11
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 14
+      job_id: jw56n0q7g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1809.0
+      throughput: 552.791597567717
+      estimated_peak_memory_range:
+        min: 61440
+        max: 17655776
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 14
+      job_id: jygzr0v65
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:33.426710Z'
+    timestamp: '2024-05-20T16:35:31.000520Z'
   - torchscript_onnx_tflite:
-      inference_time: 12810.0
-      throughput: 78.06401249024199
+      inference_time: 5039.0
+      throughput: 198.45207382417146
       estimated_peak_memory_range:
-        min: 5787648
-        max: 13604584
+        min: 1916928
+        max: 9296352
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 12
+        layers_on_npu: 10
         layers_on_gpu: 0
         layers_on_cpu: 4
-        total_layers: 16
-      job_id: jvgdq6vk5
+        total_layers: 14
+      job_id: j1p3erqz5
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:33.426724Z'
-  - torchscript_onnx_tflite:
-      inference_time: 1743.0
-      throughput: 573.7234652897304
+    timestamp: '2024-05-20T16:35:31.000535Z'
+  - torchscript_onnx_qnn:
+      inference_time: 745.0
+      throughput: 1342.2818791946308
       estimated_peak_memory_range:
-        min: 28672
-        max: 1454440
+        min: 49152
+        max: 49152
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 13
+        layers_on_npu: 14
         layers_on_gpu: 0
-        layers_on_cpu: 3
-        total_layers: 16
-      job_id: j1p3vlwzg
+        layers_on_cpu: 0
+        total_layers: 14
+      job_id: jep2mk765
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1179.0
+      throughput: 848.1764206955047
+      estimated_peak_memory_range:
+        min: 8998912
+        max: 8998912
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 19
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 19
+      job_id: jogkyervp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 72803.0
+      throughput: 13.735697704764913
+      estimated_peak_memory_range:
+        min: 32956416
+        max: 32956416
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jn5q269e5
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.426738Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.000558Z'
```

## qai_hub_models/models/sesr_m5_quantized/test.py

```diff
@@ -1,28 +1,31 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 import os
-import tempfile
 import zipfile
 
 import numpy as np
 import pytest
 import torch
 
 from qai_hub_models.models._shared.super_resolution.app import SuperResolutionApp
 from qai_hub_models.models.sesr_m5_quantized.demo import IMAGE_ADDRESS
 from qai_hub_models.models.sesr_m5_quantized.demo import main as demo_main
 from qai_hub_models.models.sesr_m5_quantized.model import (
     MODEL_ASSET_VERSION,
     MODEL_ID,
     SESR_M5Quantizable,
 )
-from qai_hub_models.utils.asset_loaders import CachedWebModelAsset, load_image
+from qai_hub_models.utils.asset_loaders import (
+    CachedWebModelAsset,
+    load_image,
+    qaihm_temp_dir,
+)
 from qai_hub_models.utils.testing import assert_most_close, skip_clone_repo_check
 
 OUTPUT_IMAGE_LOCAL_PATH = "sesr_m5_quantized_demo_output.png"
 OUTPUT_IMAGE_ADDRESS = CachedWebModelAsset.from_asset_store(
     MODEL_ID, MODEL_ASSET_VERSION, OUTPUT_IMAGE_LOCAL_PATH
 )
 
@@ -65,15 +68,15 @@
     )
 
 
 @skip_clone_repo_check
 def test_aimet_export():
     model = SESR_M5Quantizable.from_pretrained()
     name = model.__class__.__name__
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         output_zip = model.convert_to_onnx_and_aimet_encodings(
             tmpdir,
             model.get_input_spec(),
         )
         assert os.path.exists(output_zip)
         with zipfile.ZipFile(output_zip, "r") as zip:
             assert f"{name}.aimet/" in zip.namelist()
```

## qai_hub_models/models/shufflenet_v2/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/shufflenet_v2/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Shufflenet-v2
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1290.0
-      throughput: 775.1937984496124
+      inference_time: 1228.0
+      throughput: 814.3322475570033
       estimated_peak_memory_range:
-        min: 16384
-        max: 6876504
+        min: 12288
+        max: 2415688
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 204
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 204
-      job_id: jz57099lg
+      job_id: j1p3mjqmg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 797.0
-      throughput: 1254.7051442910915
+      inference_time: 765.0
+      throughput: 1307.18954248366
       estimated_peak_memory_range:
-        min: 622592
-        max: 68665608
+        min: 16384
+        max: 4038080
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 158
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 158
-      job_id: j0pxnxx15
+      job_id: j7gjlvk1p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1264.0
-      throughput: 791.1392405063291
+      inference_time: 1085.0
+      throughput: 921.6589861751152
       estimated_peak_memory_range:
-        min: 12288
-        max: 11265544
+        min: 315392
+        max: 4250040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 223
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jegnlkkr5
+        total_layers: 223
+      job_id: jmg94l9m5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.447120Z'
+    timestamp: '2024-05-20T16:35:31.039692Z'
   - torchscript_onnx_tflite:
-      inference_time: 855.0
-      throughput: 1169.5906432748538
+      inference_time: 791.0
+      throughput: 1264.2225031605562
       estimated_peak_memory_range:
-        min: 16384
-        max: 33284208
+        min: 20480
+        max: 33699040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 204
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 204
-      job_id: jqp4k33vg
+      job_id: jwgov2e15
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 528.0
-      throughput: 1893.939393939394
+      inference_time: 515.0
+      throughput: 1941.7475728155339
       estimated_peak_memory_range:
-        min: 618496
-        max: 53183776
+        min: 12288
+        max: 56897984
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 158
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 158
-      job_id: jo5mq88wp
+      job_id: jlpevd485
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 836.0
-      throughput: 1196.1722488038276
+      inference_time: 742.0
+      throughput: 1347.7088948787061
       estimated_peak_memory_range:
         min: 12288
-        max: 17464352
+        max: 24844160
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 223
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jopr8ww95
+        total_layers: 223
+      job_id: jnp184qng
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.447190Z'
+    timestamp: '2024-05-20T16:35:31.039719Z'
   - torchscript_onnx_tflite:
-      inference_time: 1291.0
-      throughput: 774.5933384972889
+      inference_time: 1227.0
+      throughput: 814.9959250203749
       estimated_peak_memory_range:
         min: 20480
-        max: 6952312
+        max: 1798552
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 204
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 204
-      job_id: jz5w201j5
+      job_id: j1pvw6zzg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 803.0
-      throughput: 1245.3300124533
+      inference_time: 762.0
+      throughput: 1312.3359580052493
       estimated_peak_memory_range:
-        min: 618496
-        max: 103577192
+        min: 622592
+        max: 4805336
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 158
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 158
-      job_id: j0pxn8x95
+      job_id: jz5w9em4p
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.447245Z'
+    timestamp: '2024-05-20T16:35:31.039737Z'
+  - torchscript_onnx_qnn:
+      inference_time: 929.0
+      throughput: 1076.4262648008612
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 158
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 158
+      job_id: jygz73v4p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1125.0
+      throughput: 888.8888888888889
+      estimated_peak_memory_range:
+        min: 10477568
+        max: 10477568
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 223
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 223
+      job_id: jvgdvx76g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 1715.0
+      throughput: 583.0903790087464
+      estimated_peak_memory_range:
+        min: 12304384
+        max: 12304384
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jz57dyvn5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.039760Z'
```

## qai_hub_models/models/shufflenet_v2_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,20 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
```

## qai_hub_models/models/shufflenet_v2_quantized/perf.yaml

```diff
@@ -22,195 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Shufflenet-v2Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 644.0
-      throughput: 1552.7950310559006
+      inference_time: 629.0
+      throughput: 1589.825119236884
       estimated_peak_memory_range:
         min: 12288
-        max: 1838712
+        max: 1960224
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 205
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 205
-      job_id: jqpyrmm75
+      job_id: jqp4wlj2g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 592.0
-      throughput: 1689.1891891891892
+      inference_time: 584.0
+      throughput: 1712.3287671232877
       estimated_peak_memory_range:
-        min: 172032
-        max: 9372520
+        min: 24576
+        max: 3645424
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 122
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 122
-      job_id: j1p8011xg
+      job_id: jegne6rjg
       job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j2p0rzk0p
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.471327Z'
+    timestamp: '2024-05-20T16:48:45.827261Z'
   - torchscript_onnx_tflite:
-      inference_time: 464.0
-      throughput: 2155.1724137931033
+      inference_time: 458.0
+      throughput: 2183.406113537118
       estimated_peak_memory_range:
         min: 12288
-        max: 22792592
+        max: 22451232
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 205
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 205
-      job_id: j2p03666p
+      job_id: j0px1ke8g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 424.0
-      throughput: 2358.490566037736
+      inference_time: 419.0
+      throughput: 2386.634844868735
       estimated_peak_memory_range:
         min: 163840
-        max: 45354944
+        max: 45935136
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 122
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 122
-      job_id: jogk7882p
+      job_id: jopryv1kg
       job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1p87q8q5
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.471384Z'
+    timestamp: '2024-05-20T16:48:45.827325Z'
   - torchscript_onnx_tflite:
-      inference_time: 1064.0
-      throughput: 939.8496240601504
+      inference_time: 649.0
+      throughput: 1540.8320493066255
       estimated_peak_memory_range:
         min: 12288
-        max: 16582800
+        max: 1657808
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 207
+        layers_on_npu: 205
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 207
-      job_id: jogk7w8op
+        total_layers: 205
+      job_id: jn5q3dwmp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 585.0
+      throughput: 1709.4017094017095
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 16384
+        max: 13811248
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 122
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jwgok78dp
-      job_status: Failed
+        total_layers: 122
+      job_id: jqpyd1v0p
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:48:45.827382Z'
+  - torchscript_onnx_tflite:
+      inference_time: 946.0
+      throughput: 1057.0824524312895
+      estimated_peak_memory_range:
+        min: 12288
+        max: 16954944
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 205
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 205
+      job_id: j1gl3q7lg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1140.0
+      throughput: 877.1929824561404
+      estimated_peak_memory_range:
+        min: 294912
+        max: 42839088
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 122
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 122
+      job_id: jlpekn20p
+      job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:33.471427Z'
+    timestamp: '2024-05-20T16:48:45.827429Z'
   - torchscript_onnx_tflite:
-      inference_time: 10090.0
-      throughput: 99.10802775024777
+      inference_time: 8918.0
+      throughput: 112.13276519398968
       estimated_peak_memory_range:
-        min: 12288
-        max: 6455280
+        min: 53248
+        max: 6490632
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
-        layers_on_npu: 44
+        layers_on_npu: 43
         layers_on_gpu: 9
-        layers_on_cpu: 154
-        total_layers: 207
-      job_id: jz5w3y9jp
+        layers_on_cpu: 153
+        total_layers: 205
+      job_id: jw56n0v7g
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:33.471459Z'
-  - torchscript_onnx_tflite:
-      inference_time: 667.0
-      throughput: 1499.2503748125937
+    timestamp: '2024-05-20T16:48:45.827459Z'
+  - torchscript_onnx_qnn:
+      inference_time: 669.0
+      throughput: 1494.7683109118086
       estimated_peak_memory_range:
-        min: 24576
-        max: 2164120
+        min: 532480
+        max: 532480
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 207
+        layers_on_npu: 122
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 207
-      job_id: jn5qexvm5
+        total_layers: 122
+      job_id: jep2mk365
       job_status: Passed
-    torchscript_onnx_qnn:
-      inference_time: 618.0
-      throughput: 1618.1229773462783
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
       estimated_peak_memory_range:
-        min: 634880
-        max: 8982056
-      primary_compute_unit: NPU
-      precision: int8
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
       layer_info:
-        layers_on_npu: 124
+        layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 124
-      job_id: j7gjz6q85
+        total_layers: 0
+      job_id: jogkyedvp
+      job_status: Failed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 1478.0
+      throughput: 676.5899864682003
+      estimated_peak_memory_range:
+        min: 6258688
+        max: 6258688
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 263
+        total_layers: 263
+      job_id: jn5q26we5
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.471511Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:48:45.827519Z'
```

## qai_hub_models/models/sinet/export.py

```diff
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/sinet/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: SINet
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1826.0
-      throughput: 547.645125958379
+      inference_time: 1797.0
+      throughput: 556.4830272676684
       estimated_peak_memory_range:
         min: 12288
-        max: 2609144
+        max: 2452968
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 240
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 240
-      job_id: jn5qevv45
+      job_id: j1glkv72p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1184.0
-      throughput: 844.5945945945946
+      inference_time: 1171.0
+      throughput: 853.9709649871904
       estimated_peak_memory_range:
-        min: 618496
-        max: 4714320
+        min: 2113536
+        max: 14886760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 186
-      job_id: jw56eww0g
+      job_id: jwgov2m15
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 2285.0
+      throughput: 437.636761487965
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 618496
+        max: 35536752
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 229
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jwgok88xp
-      job_status: Failed
+        total_layers: 229
+      job_id: jygz73w4p
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.500477Z'
+    timestamp: '2024-05-20T16:35:31.109606Z'
   - torchscript_onnx_tflite:
-      inference_time: 1171.0
-      throughput: 853.9709649871904
+      inference_time: 1169.0
+      throughput: 855.4319931565441
       estimated_peak_memory_range:
         min: 12288
-        max: 25301888
+        max: 25617584
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 240
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 240
-      job_id: j1gl6ll8g
+      job_id: jw561yvnp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 799.0
-      throughput: 1251.5644555694619
+      inference_time: 780.0
+      throughput: 1282.051282051282
       estimated_peak_memory_range:
-        min: 12288
-        max: 64850320
+        min: 618496
+        max: 71418032
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 186
-      job_id: j1p3v66lg
+      job_id: j1pvw64zg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 1599.0
+      throughput: 625.3908692933084
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 12288
+        max: 27418000
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 229
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: j1pv077j5
-      job_status: Failed
+        total_layers: 229
+      job_id: jz5w9ex4p
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.500551Z'
+    timestamp: '2024-05-20T16:35:31.109633Z'
   - torchscript_onnx_tflite:
-      inference_time: 1823.0
-      throughput: 548.5463521667581
+      inference_time: 1810.0
+      throughput: 552.4861878453039
       estimated_peak_memory_range:
-        min: 24576
-        max: 1974184
+        min: 16384
+        max: 2390784
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 240
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 240
-      job_id: jz57014rg
+      job_id: j1p3mj8mg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1185.0
-      throughput: 843.8818565400844
+      inference_time: 1168.0
+      throughput: 856.1643835616438
       estimated_peak_memory_range:
-        min: 634880
-        max: 5992992
+        min: 626688
+        max: 8177944
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 186
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 186
-      job_id: jopr8m0e5
+      job_id: jlpevd285
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.500612Z'
+    timestamp: '2024-05-20T16:35:31.109650Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1401.0
+      throughput: 713.7758743754462
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 186
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 186
+      job_id: j7gjlv11p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 2469.0
+      throughput: 405.0222762251924
+      estimated_peak_memory_range:
+        min: 3219456
+        max: 3219456
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 229
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 229
+      job_id: jmg94l8m5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 2976.0
+      throughput: 336.02150537634407
+      estimated_peak_memory_range:
+        min: 13578240
+        max: 13578240
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jnp1843ng
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.109674Z'
```

## qai_hub_models/models/squeezenet1_1/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/squeezenet1_1/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: SqueezeNet-1_1
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 672.0
-      throughput: 1488.095238095238
+      inference_time: 664.0
+      throughput: 1506.0240963855422
       estimated_peak_memory_range:
         min: 12288
-        max: 1740976
+        max: 1506784
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 41
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 41
-      job_id: jlpeeyy1p
+      job_id: jvgdvx06g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 711.0
-      throughput: 1406.4697609001407
+      inference_time: 712.0
+      throughput: 1404.4943820224719
       estimated_peak_memory_range:
-        min: 638976
-        max: 12256680
+        min: 618496
+        max: 7468520
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 70
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 70
-      job_id: jz5w24765
+      job_id: j0px1km8g
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 861.0
-      throughput: 1161.4401858304298
+      inference_time: 651.0
+      throughput: 1536.0983102918588
       estimated_peak_memory_range:
         min: 12288
-        max: 10395112
+        max: 7201352
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 71
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jnp1y6j2p
+        total_layers: 71
+      job_id: jep2mkj65
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.524625Z'
+    timestamp: '2024-05-20T16:35:31.140012Z'
   - torchscript_onnx_tflite:
-      inference_time: 453.0
-      throughput: 2207.5055187637968
+      inference_time: 477.0
+      throughput: 2096.4360587002097
       estimated_peak_memory_range:
-        min: 12288
-        max: 22540768
+        min: 0
+        max: 22219968
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 41
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 41
-      job_id: jygzonnk5
+      job_id: jz57dy6n5
       job_status: Passed
     torchscript_onnx_qnn:
       inference_time: 490.0
       throughput: 2040.8163265306123
       estimated_peak_memory_range:
         min: 618496
-        max: 28785760
+        max: 27578288
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 70
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 70
-      job_id: jmg9jdml5
+      job_id: jo5mzn47p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 618.0
-      throughput: 1618.1229773462783
+      inference_time: 488.0
+      throughput: 2049.1803278688526
       estimated_peak_memory_range:
-        min: 618496
-        max: 20314848
+        min: 24576
+        max: 17829040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 71
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jvgde23e5
+        total_layers: 71
+      job_id: jqpyd1n0p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.524666Z'
+    timestamp: '2024-05-20T16:35:31.140037Z'
   - torchscript_onnx_tflite:
-      inference_time: 672.0
-      throughput: 1488.095238095238
+      inference_time: 664.0
+      throughput: 1506.0240963855422
       estimated_peak_memory_range:
-        min: 12288
-        max: 1757808
+        min: 20480
+        max: 1789832
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 41
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 41
-      job_id: j1pv0ydm5
+      job_id: jqp4wl82g
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 718.0
-      throughput: 1392.757660167131
+      inference_time: 701.0
+      throughput: 1426.5335235378031
       estimated_peak_memory_range:
-        min: 618496
-        max: 75568808
+        min: 626688
+        max: 3276112
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 70
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 70
-      job_id: jnp1ykjlp
+      job_id: jopryv9kg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.524695Z'
+    timestamp: '2024-05-20T16:35:31.140054Z'
+  - torchscript_onnx_qnn:
+      inference_time: 828.0
+      throughput: 1207.729468599034
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 70
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 70
+      job_id: jegne6xjg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 696.0
+      throughput: 1436.7816091954023
+      estimated_peak_memory_range:
+        min: 3063808
+        max: 3063808
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 71
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 71
+      job_id: j2p0rzd0p
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 2093.0
+      throughput: 477.78308647873865
+      estimated_peak_memory_range:
+        min: 9494528
+        max: 9494528
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 41
+        total_layers: 41
+      job_id: j1p87q6q5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.140076Z'
```

## qai_hub_models/models/squeezenet1_1_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,20 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -201,14 +212,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/squeezenet1_1_quantized/perf.yaml

```diff
@@ -22,240 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: SqueezeNet-1_1Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 218.0
-      throughput: 4587.155963302752
+      inference_time: 221.0
+      throughput: 4524.886877828054
       estimated_peak_memory_range:
-        min: 24576
-        max: 1453208
+        min: 12288
+        max: 2523424
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 41
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 41
-      job_id: jmg9jdmw5
+      job_id: jogkyeovp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 466.0
-      throughput: 2145.922746781116
+      inference_time: 467.0
+      throughput: 2141.3276231263385
       estimated_peak_memory_range:
-        min: 12288
-        max: 10115704
+        min: 176128
+        max: 80481816
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 45
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 45
-      job_id: jvgde23r5
+      job_id: jw561yrnp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 811.0
-      throughput: 1233.0456226880394
+      inference_time: 550.0
+      throughput: 1818.1818181818182
       estimated_peak_memory_range:
         min: 618496
-        max: 5355192
+        max: 7743200
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 49
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jqp4k318g
+        total_layers: 49
+      job_id: j7gjlvo1p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.548624Z'
+    timestamp: '2024-05-20T16:35:31.170251Z'
   - torchscript_onnx_tflite:
-      inference_time: 178.0
-      throughput: 5617.9775280898875
+      inference_time: 184.0
+      throughput: 5434.782608695652
       estimated_peak_memory_range:
         min: 12288
-        max: 21783424
+        max: 22090256
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 41
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 41
-      job_id: jnp1y6j8p
+      job_id: jn5q26ze5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 343.0
-      throughput: 2915.451895043732
+      inference_time: 341.0
+      throughput: 2932.551319648094
       estimated_peak_memory_range:
-        min: 167936
-        max: 23042032
+        min: 163840
+        max: 26837472
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 45
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 45
-      job_id: jz57094vg
+      job_id: j1p3mjxmg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 632.0
-      throughput: 1582.2784810126582
+      inference_time: 421.0
+      throughput: 2375.296912114014
       estimated_peak_memory_range:
         min: 12288
-        max: 16606592
+        max: 16755200
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 49
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j0pxnx435
+        total_layers: 49
+      job_id: jlpevd885
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.548662Z'
+    timestamp: '2024-05-20T16:35:31.170278Z'
   - torchscript_onnx_tflite:
-      inference_time: 645.0
-      throughput: 1550.3875968992247
+      inference_time: 225.0
+      throughput: 4444.444444444444
       estimated_peak_memory_range:
-        min: 12288
-        max: 14710928
+        min: 28672
+        max: 1537872
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 43
+        layers_on_npu: 41
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 43
-      job_id: jogk7w2op
+        total_layers: 41
+      job_id: j1glkvo2p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 471.0
+      throughput: 2123.1422505307855
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 12288
+        max: 9792120
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 45
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: j1pv0ylm5
-      job_status: Failed
-    torchscript_onnx_ort:
-      inference_time: 3597.0
-      throughput: 278.00945232137894
+        total_layers: 45
+      job_id: j1pvw6ezg
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:31.170295Z'
+  - torchscript_onnx_tflite:
+      inference_time: 538.0
+      throughput: 1858.736059479554
       estimated_peak_memory_range:
-        min: 0
-        max: 28318256
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 12288
+        max: 14558896
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 41
         layers_on_gpu: 0
-        layers_on_cpu: 51
-        total_layers: 51
-      job_id: jo5mq8mdp
+        layers_on_cpu: 0
+        total_layers: 41
+      job_id: jmg9wqkvp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 958.0
+      throughput: 1043.8413361169103
+      estimated_peak_memory_range:
+        min: 163840
+        max: 22853712
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 45
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 45
+      job_id: jo5m3ldqg
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:33.548701Z'
+    timestamp: '2024-05-20T16:35:31.170312Z'
   - torchscript_onnx_tflite:
-      inference_time: 4261.0
-      throughput: 234.6866932644919
+      inference_time: 4066.0
+      throughput: 245.94195769798327
       estimated_peak_memory_range:
-        min: 90112
-        max: 1970416
+        min: 28672
+        max: 6476760
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 43
+        layers_on_npu: 41
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 43
-      job_id: jmg9yo4v5
+        total_layers: 41
+      job_id: jnp1em7lg
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:33.548716Z'
-  - torchscript_onnx_tflite:
-      inference_time: 246.0
-      throughput: 4065.040650406504
+    timestamp: '2024-05-20T16:35:31.170322Z'
+  - torchscript_onnx_qnn:
+      inference_time: 580.0
+      throughput: 1724.1379310344828
       estimated_peak_memory_range:
-        min: 12288
-        max: 1876064
+        min: 622592
+        max: 622592
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 43
+        layers_on_npu: 45
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 43
-      job_id: j1gl69ylg
+        total_layers: 45
+      job_id: jwgov2o15
       job_status: Passed
-    torchscript_onnx_qnn:
-      inference_time: 507.0
-      throughput: 1972.3865877712033
+    torchscript_onnx_ort:
+      inference_time: 571.0
+      throughput: 1751.3134851138354
       estimated_peak_memory_range:
-        min: 528384
-        max: 12189432
+        min: 1773568
+        max: 1773568
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 47
+        layers_on_npu: 49
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 47
-      job_id: jygzoql65
+        total_layers: 49
+      job_id: jygz7384p
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 979.0
+      throughput: 1021.4504596527069
+      estimated_peak_memory_range:
+        min: 4251648
+        max: 4251648
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 51
+        total_layers: 51
+      job_id: jz5w9e84p
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.548742Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.170345Z'
```

## qai_hub_models/models/stylegan2/demo.py

```diff
@@ -37,16 +37,14 @@
         "--classes",
         type=int,
         nargs="*",
         default=None,
         help="Class[es] to use for image generation (if applicable).",
     )
     args = parser.parse_args([] if is_test else None)
-    if not args.inference_options:
-        args.inference_options = "--compute_unit gpu"
 
     # Create model and app
     model = model_from_cli_args(StyleGAN2, args)
     inference_model = demo_model_from_cli_args(StyleGAN2, MODEL_ID, args)
     app = StyleGAN2App(inference_model, model.output_size, model.num_classes)
 
     # Verify model input args
```

## qai_hub_models/models/stylegan2/export.py

```diff
@@ -114,17 +114,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_output output_0"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -179,16 +186,20 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
```

## qai_hub_models/models/stylegan2/model.py

```diff
@@ -1,17 +1,18 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
-from typing import Any, Callable, Dict, List
+from typing import Any, Callable, Dict, List, Optional
 
 import numpy as np
 import torch
+from qai_hub.client import Device
 
 from qai_hub_models.utils.asset_loaders import SourceAsRoot
 from qai_hub_models.utils.base_model import BaseModel, TargetRuntime
 from qai_hub_models.utils.input_spec import InputSpec
 
 STYLEGAN2_SOURCE_REPOSITORY = "https://github.com/NVlabs/stylegan3"
 STYLEGAN2_SOURCE_REPO_COMMIT = "c233a919a6faee6e36a316ddd4eddababad1adf9"
@@ -118,28 +119,41 @@
             if input_spec["classes"][0][1] != 0:
                 classes[:, 0] = 1  # Select first class as default
             inputs["classes"] = [classes]
 
         return inputs
 
     def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
+        self,
+        target_runtime: TargetRuntime,
+        other_compile_options: str = "",
+        device: Optional[Device] = None,
     ) -> str:
         compile_options = super().get_hub_compile_options(
-            target_runtime, other_compile_options
+            target_runtime, other_compile_options, device
         )
-        return compile_options + " --compute_unit gpu"
+        if (
+            target_runtime == TargetRuntime.TFLITE
+            and "--compute_unit" not in compile_options
+        ):
+            compile_options = compile_options + " --compute_unit gpu"
+        return compile_options
 
     def get_hub_profile_options(
         self, target_runtime: TargetRuntime, other_profile_options: str = ""
     ) -> str:
         profile_options = super().get_hub_profile_options(
             target_runtime, other_profile_options
         )
-        return profile_options + " --compute_unit gpu"
+        if (
+            target_runtime == TargetRuntime.TFLITE
+            and "--compute_unit" not in profile_options
+        ):
+            profile_options = profile_options + " --compute_unit gpu"
+        return profile_options
 
 
 def _get_qaihm_upfirdn2d_ref(misc: Any, conv2d_gradfix: Callable, upfirdn2d: Any):
     """
     Get patched upfirdn2d function implementation that is export compatible.
     This replaces an implementation provided by the stylegan3 repository.
     Params are imports from the stylegan3 repository (see _load_stylegan2_source_model_from_weights).
```

## qai_hub_models/models/stylegan2/perf.yaml

```diff
@@ -18,117 +18,202 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: StyleGAN2
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1317970.0
-      throughput: 0.7587426117438182
+      inference_time: 1588522.0
+      throughput: 0.6295159903356705
       estimated_peak_memory_range:
-        min: 1448136704
-        max: 2566842336
+        min: 1459597312
+        max: 2294159464
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 78
         layers_on_cpu: 402
         total_layers: 480
-      job_id: jegnlknk5
+      job_id: jmg94lkm5
       job_status: Passed
-    torchscript_onnx_ort:
+    torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jep20ewrg
+      job_id: jz57dykn5
       job_status: Failed
+    torchscript_onnx_ort:
+      inference_time: 640892.0
+      throughput: 1.560325296617839
+      estimated_peak_memory_range:
+        min: 206315520
+        max: 337724960
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 454
+        layers_on_gpu: 0
+        layers_on_cpu: 89
+        total_layers: 543
+      job_id: jo5mzno7p
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.583802Z'
+    timestamp: '2024-05-20T16:35:31.209775Z'
   - torchscript_onnx_tflite:
-      inference_time: 1012977.0
-      throughput: 0.9871892451654875
+      inference_time: 1240378.0
+      throughput: 0.8062058501521311
       estimated_peak_memory_range:
-        min: 954945536
-        max: 980253632
+        min: 1137418240
+        max: 1169458160
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 78
         layers_on_cpu: 402
         total_layers: 480
-      job_id: jopr8w005
+      job_id: jnp1847ng
       job_status: Passed
-    torchscript_onnx_ort:
+    torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jqpyrmx85
+      job_id: jqp4wlm2g
       job_status: Failed
+    torchscript_onnx_ort:
+      inference_time: 508041.0
+      throughput: 1.9683450745117028
+      estimated_peak_memory_range:
+        min: 300343296
+        max: 1069005056
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 454
+        layers_on_gpu: 0
+        layers_on_cpu: 89
+        total_layers: 543
+      job_id: jegne6ojg
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.583878Z'
+    timestamp: '2024-05-20T16:35:31.209803Z'
   - torchscript_onnx_tflite:
-      inference_time: 1253049.0
-      throughput: 0.7980533881755622
+      inference_time: 1643379.0
+      throughput: 0.6085023600764036
       estimated_peak_memory_range:
-        min: 941391872
-        max: 2204990360
+        min: 1178169344
+        max: 1181322952
       primary_compute_unit: CPU
       precision: fp32
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 78
         layers_on_cpu: 402
         total_layers: 480
-      job_id: j0pxn8015
+      job_id: jvgdvx86g
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j0px1k38g
+      job_status: Failed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.583943Z'
+    timestamp: '2024-05-20T16:35:31.209820Z'
+  - torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jopryvokg
+      job_status: Failed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jep2mk465
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.209838Z'
```

## qai_hub_models/models/stylegan2/requirements.txt

```diff
@@ -1 +1 @@
-click==8.0
+click==8.1.7
```

## qai_hub_models/models/swin_base/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/swin_base/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Swin-Base
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 61028.0
-      throughput: 16.38592121649079
+      inference_time: 38211.0
+      throughput: 26.170474470702153
       estimated_peak_memory_range:
-        min: 106496
-        max: 3418200
+        min: 0
+        max: 7586888
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1568
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1568
-      job_id: j1p801xkg
+      job_id: jqpyd1q0p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 31640.0
+      throughput: 31.605562579013906
+      estimated_peak_memory_range:
+        min: 40960
+        max: 49217704
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1255
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1255
+      job_id: jogkye9vp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 72900.0
-      throughput: 13.717421124828531
+      inference_time: 64134.0
+      throughput: 15.592353509838775
       estimated_peak_memory_range:
-        min: 118784
-        max: 421108168
+        min: 114688
+        max: 476901736
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 1163
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jn5qevyn5
+        total_layers: 1163
+      job_id: j1p3mjwmg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.601688Z'
+    timestamp: '2024-05-20T16:35:31.233759Z'
   - torchscript_onnx_tflite:
-      inference_time: 39474.0
-      throughput: 25.333130668287986
+      inference_time: 26230.0
+      throughput: 38.12428516965307
       estimated_peak_memory_range:
-        min: 73728
-        max: 512044160
+        min: 53248
+        max: 498968400
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1568
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1568
-      job_id: jogk784wp
+      job_id: j2p0rzv0p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 21887.0
+      throughput: 45.68922191255083
+      estimated_peak_memory_range:
+        min: 0
+        max: 408673168
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1255
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1255
+      job_id: jn5q26me5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 51726.0
-      throughput: 19.332637358388432
+      inference_time: 44459.0
+      throughput: 22.49263366247554
       estimated_peak_memory_range:
-        min: 651264
-        max: 268896832
+        min: 626688
+        max: 202092528
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 1163
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1gl6lxjg
+        total_layers: 1163
+      job_id: jwgov2415
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.601867Z'
+    timestamp: '2024-05-20T16:35:31.233787Z'
   - torchscript_onnx_tflite:
-      inference_time: 61645.0
-      throughput: 16.221915808256956
+      inference_time: 38283.0
+      throughput: 26.121254865083717
       estimated_peak_memory_range:
-        min: 28672
-        max: 3282368
+        min: 98304
+        max: 3696992
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1568
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1568
-      job_id: jw56e9o0g
+      job_id: j1p87q4q5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 31310.0
+      throughput: 31.938677738741617
+      estimated_peak_memory_range:
+        min: 45056
+        max: 48773208
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1255
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1255
+      job_id: jw561ydnp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.602032Z'
+    timestamp: '2024-05-20T16:35:31.233806Z'
+  - torchscript_onnx_qnn:
+      inference_time: 38967.0
+      throughput: 25.662740267405752
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1255
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1255
+      job_id: j1glkv12p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 66278.0
+      throughput: 15.087962823259604
+      estimated_peak_memory_range:
+        min: 685105152
+        max: 685105152
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1163
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1163
+      job_id: j1pvw69zg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j7gjlvw1p
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.233833Z'
```

## qai_hub_models/models/swin_small/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/swin_small/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Swin-Small
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 46059.0
-      throughput: 21.711283353959054
+      inference_time: 29128.0
+      throughput: 34.33122768470201
       estimated_peak_memory_range:
-        min: 28672
-        max: 8907776
+        min: 36864
+        max: 2408576
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1563
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1563
-      job_id: j1p3v693g
+      job_id: jlpevdl85
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 23681.0
+      throughput: 42.22794645496389
+      estimated_peak_memory_range:
+        min: 16384
+        max: 45345336
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1246
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1246
+      job_id: jmg94lxm5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 61104.0
-      throughput: 16.365540717465304
+      inference_time: 56992.0
+      throughput: 17.54632229084784
       estimated_peak_memory_range:
-        min: 12288
-        max: 250842792
+        min: 40960
+        max: 225148824
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 1158
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1pv07lk5
+        total_layers: 1158
+      job_id: jmg94lxq5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.619812Z'
+    timestamp: '2024-05-20T16:35:31.258354Z'
   - torchscript_onnx_tflite:
-      inference_time: 29579.0
-      throughput: 33.80776902532202
+      inference_time: 19660.0
+      throughput: 50.8646998982706
       estimated_peak_memory_range:
-        min: 45056
-        max: 479603376
+        min: 49152
+        max: 467994720
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1563
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1563
-      job_id: jwgok8rqp
+      job_id: jygz7344p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 16138.0
+      throughput: 61.96554715578139
+      estimated_peak_memory_range:
+        min: 0
+        max: 376584720
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1246
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1246
+      job_id: jnp184vng
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 43618.0
-      throughput: 22.926314824155167
+      inference_time: 39508.0
+      throughput: 25.311329351017516
       estimated_peak_memory_range:
-        min: 696320
-        max: 646499600
+        min: 88776704
+        max: 260548080
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 1158
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j7gjzqrv5
+        total_layers: 1158
+      job_id: jnp184vkg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.619995Z'
+    timestamp: '2024-05-20T16:35:31.258381Z'
   - torchscript_onnx_tflite:
-      inference_time: 45406.0
-      throughput: 22.023521120556754
+      inference_time: 29352.0
+      throughput: 34.06922867266285
       estimated_peak_memory_range:
-        min: 94208
-        max: 3127248
+        min: 20480
+        max: 8413168
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 1563
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 1563
-      job_id: jz5701nlg
+      job_id: jz5w9e14p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 23705.0
+      throughput: 42.185192997257964
+      estimated_peak_memory_range:
+        min: 53248
+        max: 45854248
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1246
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1246
+      job_id: jz5w9e1zp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.620167Z'
+    timestamp: '2024-05-20T16:35:31.258398Z'
+  - torchscript_onnx_qnn:
+      inference_time: 23881.0
+      throughput: 41.87429337129936
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1246
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1246
+      job_id: jvgdvxz6g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 59131.0
+      throughput: 16.91160305085319
+      estimated_peak_memory_range:
+        min: 473104384
+        max: 473104384
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1158
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1158
+      job_id: jvgdvxzkg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 449448.0
+      throughput: 2.2249514960573857
+      estimated_peak_memory_range:
+        min: 1191936
+        max: 1191936
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 1050
+        total_layers: 1050
+      job_id: jz57dy7q5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.258421Z'
```

## qai_hub_models/models/swin_tiny/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/swin_tiny/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Swin-Tiny
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 28481.0
-      throughput: 35.11112671605632
+      inference_time: 17594.0
+      throughput: 56.83755825849722
       estimated_peak_memory_range:
-        min: 217088
-        max: 74292680
+        min: 0
+        max: 2690144
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 837
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 837
-      job_id: jygzonlo5
+      job_id: jqp4wl9qg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 15006.0
+      throughput: 66.6400106624017
+      estimated_peak_memory_range:
+        min: 0
+        max: 28760920
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 700
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 700
+      job_id: jegne6kvg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 27887.0
-      throughput: 35.85900240255316
+      inference_time: 34124.0
+      throughput: 29.304888055327627
       estimated_peak_memory_range:
-        min: 16384
-        max: 164109776
+        min: 65536
+        max: 157394912
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 624
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jmg9jdzw5
+        total_layers: 624
+      job_id: j2p0rz62p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.637970Z'
+    timestamp: '2024-05-20T16:35:31.283034Z'
   - torchscript_onnx_tflite:
-      inference_time: 18310.0
-      throughput: 54.614964500273075
+      inference_time: 11804.0
+      throughput: 84.71704506946797
       estimated_peak_memory_range:
         min: 40960
-        max: 293649808
+        max: 289709760
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 837
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 837
-      job_id: jz5w24l35
+      job_id: j0px1kdjg
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jopryvwvg
+      job_status: Failed
     torchscript_onnx_ort:
-      inference_time: 19785.0
-      throughput: 50.543340914834474
+      inference_time: 23681.0
+      throughput: 42.22794645496389
       estimated_peak_memory_range:
-        min: 634880
-        max: 162638432
+        min: 28672
+        max: 109585264
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 624
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jnp1y6n8p
+        total_layers: 624
+      job_id: j1p87q1z5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.638080Z'
+    timestamp: '2024-05-20T16:35:31.283062Z'
   - torchscript_onnx_tflite:
-      inference_time: 28405.0
-      throughput: 35.205069530012324
+      inference_time: 17554.0
+      throughput: 56.96707303178763
       estimated_peak_memory_range:
-        min: 57344
-        max: 3112384
+        min: 28672
+        max: 2913592
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 837
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 837
-      job_id: jep20qd4g
+      job_id: jo5mzndyp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 14942.0
+      throughput: 66.9254450542096
+      estimated_peak_memory_range:
+        min: 225280
+        max: 27331792
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 700
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 700
+      job_id: jqpyd1mrp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.638179Z'
+    timestamp: '2024-05-20T16:35:31.283079Z'
+  - torchscript_onnx_qnn:
+      inference_time: 14251.0
+      throughput: 70.17051434987019
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 700
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 700
+      job_id: jep2mkex5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 35507.0
+      throughput: 28.16346072605402
+      estimated_peak_memory_range:
+        min: 241229824
+        max: 241229824
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 624
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 624
+      job_id: jogkye8yp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 17912.0
+      throughput: 55.828494863778474
+      estimated_peak_memory_range:
+        min: 1433600
+        max: 1433600
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 564
+        total_layers: 564
+      job_id: jn5q26v75
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.283101Z'
```

## qai_hub_models/models/trocr/export.py

```diff
@@ -130,15 +130,15 @@
         component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options
+            target_runtime, compile_options, hub_device
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub_device,
             name=f"{model_name}_{component_name}",
@@ -223,18 +223,15 @@
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
     parser = export_parser(
-        model_cls=Model,
-        components=ALL_COMPONENTS,
-        supports_qnn=False,
-        supports_ort=False,
+        model_cls=Model, components=ALL_COMPONENTS, supports_qnn=False
     )
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/trocr/perf.yaml

```diff
@@ -18,218 +18,386 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: TrOCREncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 216492.0
-      throughput: 4.619108327328492
+      inference_time: 149663.0
+      throughput: 6.68167817028925
       estimated_peak_memory_range:
-        min: 7274496
-        max: 10306224
+        min: 7266304
+        max: 10722008
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 592
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 592
-      job_id: jz5709evg
+      job_id: j1glkvlep
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 123961.0
+      throughput: 8.067053347423787
+      estimated_peak_memory_range:
+        min: 32768
+        max: 24931512
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 469
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 469
+      job_id: jlpevdy75
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 189041.0
-      throughput: 5.289857755724949
+      inference_time: 111209.0
+      throughput: 8.992077979300236
       estimated_peak_memory_range:
-        min: 69632
-        max: 125141888
+        min: 143360
+        max: 114159672
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 396
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jegnlkzk5
+        total_layers: 396
+      job_id: jz57dy9q5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.655937Z'
+    timestamp: '2024-05-20T16:35:31.307706Z'
   - torchscript_onnx_tflite:
-      inference_time: 162590.0
-      throughput: 6.1504397564425854
+      inference_time: 111478.0
+      throughput: 8.970379805880981
       estimated_peak_memory_range:
-        min: 5963776
-        max: 327025904
+        min: 6787072
+        max: 349351296
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 592
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 592
-      job_id: j0pxnxl35
+      job_id: j1p3mj6xg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 92809.0
+      throughput: 10.77481709747977
+      estimated_peak_memory_range:
+        min: 1785856
+        max: 169310384
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 469
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 469
+      job_id: jz5w9e4zp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 143879.0
-      throughput: 6.95028461415495
+      inference_time: 84299.0
+      throughput: 11.86253692214617
       estimated_peak_memory_range:
-        min: 14708736
-        max: 90842000
+        min: 11382784
+        max: 88625792
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 396
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jep20errg
+        total_layers: 396
+      job_id: j0px1kxjg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.656015Z'
+    timestamp: '2024-05-20T16:35:31.307733Z'
   - torchscript_onnx_tflite:
-      inference_time: 216411.0
-      throughput: 4.620837203284491
+      inference_time: 149781.0
+      throughput: 6.676414231444576
       estimated_peak_memory_range:
         min: 7274496
-        max: 10398120
+        max: 10723104
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 592
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 592
-      job_id: jlpee0x1p
+      job_id: j1pvw677g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 123679.0
+      throughput: 8.085447003937613
+      estimated_peak_memory_range:
+        min: 1929216
+        max: 24597888
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 469
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 469
+      job_id: jnp1846kg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.656082Z'
+    timestamp: '2024-05-20T16:35:31.307750Z'
+  - torchscript_onnx_ort:
+      inference_time: 111834.0
+      throughput: 8.941824489868912
+      estimated_peak_memory_range:
+        min: 34922496
+        max: 34922496
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 396
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 396
+      job_id: jegne6nvg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 39277.0
+      throughput: 25.46019298826285
+      estimated_peak_memory_range:
+        min: 2703360
+        max: 2703360
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 365
+        total_layers: 365
+      job_id: jep2mkwx5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.307768Z'
 - name: TrOCRDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2684.0
-      throughput: 372.5782414307005
+      inference_time: 2717.0
+      throughput: 368.052999631947
       estimated_peak_memory_range:
-        min: 16384
-        max: 2557552
+        min: 20480
+        max: 2492240
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 370
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 370
-      job_id: jqp4k3y8g
+      job_id: jw561ywvp
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jygz73nzp
+      job_status: Failed
     torchscript_onnx_ort:
-      inference_time: 2944.0
-      throughput: 339.67391304347825
+      inference_time: 2875.0
+      throughput: 347.82608695652175
       estimated_peak_memory_range:
-        min: 28672
-        max: 392358928
+        min: 0
+        max: 575282800
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 352
         layers_on_gpu: 0
         layers_on_cpu: 1
-        total_layers: 2
-      job_id: jopr8wl05
+        total_layers: 353
+      job_id: jqp4wl3qg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.656134Z'
+    timestamp: '2024-05-20T16:35:31.307792Z'
   - torchscript_onnx_tflite:
-      inference_time: 1948.0
-      throughput: 513.347022587269
+      inference_time: 1998.0
+      throughput: 500.5005005005005
       estimated_peak_memory_range:
         min: 12288
-        max: 192910976
+        max: 192263456
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 370
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 370
-      job_id: jo5mq80dp
+      job_id: jwgov2845
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jmg94ldq5
+      job_status: Failed
     torchscript_onnx_ort:
-      inference_time: 2482.0
-      throughput: 402.90088638195004
+      inference_time: 2139.0
+      throughput: 467.50818139317437
       estimated_peak_memory_range:
         min: 0
-        max: 36159696
+        max: 45855536
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 352
         layers_on_gpu: 0
         layers_on_cpu: 1
-        total_layers: 2
-      job_id: jqpyrmo85
+        total_layers: 353
+      job_id: jo5mzn8yp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.656185Z'
+    timestamp: '2024-05-20T16:35:31.307816Z'
   - torchscript_onnx_tflite:
-      inference_time: 2691.0
-      throughput: 371.6090672612412
+      inference_time: 2735.0
+      throughput: 365.6307129798903
       estimated_peak_memory_range:
         min: 16384
-        max: 2038272
+        max: 2426968
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 370
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 370
-      job_id: jygzoqyk5
+      job_id: j7gjlvq7p
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jvgdvx2kg
+      job_status: Failed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.656227Z'
+    timestamp: '2024-05-20T16:35:31.307832Z'
+  - torchscript_onnx_ort:
+      inference_time: 2647.0
+      throughput: 377.7861730260673
+      estimated_peak_memory_range:
+        min: 355991552
+        max: 355991552
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 352
+        layers_on_gpu: 0
+        layers_on_cpu: 1
+        total_layers: 353
+      job_id: jopryv0vg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 1426.0
+      throughput: 701.2622720897616
+      estimated_peak_memory_range:
+        min: 7168000
+        max: 7168000
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 300
+        total_layers: 300
+      job_id: jqpyd1xrp
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.307850Z'
```

## qai_hub_models/models/unet_segmentation/export.py

```diff
@@ -116,20 +116,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -159,16 +163,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -188,27 +194,31 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     if not skip_summary:
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/unet_segmentation/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Unet-Segmentation
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 155616.0
-      throughput: 6.4260744396463085
+      inference_time: 161691.0
+      throughput: 6.184636126933472
       estimated_peak_memory_range:
-        min: 6692864
-        max: 229373376
+        min: 16384
+        max: 237098920
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: jogk782wp
+      job_id: j2p0rzj2p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 150609.0
-      throughput: 6.63970944631463
+      inference_time: 149965.0
+      throughput: 6.668222585269897
       estimated_peak_memory_range:
-        min: 9854976
-        max: 34064640
+        min: 9981952
+        max: 30872736
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 51
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 51
-      job_id: j1gl6lyjg
+      job_id: jn5q26y75
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 150132.0
-      throughput: 6.6608051581275145
+      inference_time: 157701.0
+      throughput: 6.341113880064172
       estimated_peak_memory_range:
-        min: 13246464
-        max: 147066768
+        min: 13557760
+        max: 158096808
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 53
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1p3v6z3g
+        total_layers: 53
+      job_id: jwgov2r45
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.687955Z'
+    timestamp: '2024-05-20T16:35:31.352363Z'
   - torchscript_onnx_tflite:
-      inference_time: 112866.0
-      throughput: 8.860064146864424
+      inference_time: 115442.0
+      throughput: 8.662358586996067
       estimated_peak_memory_range:
-        min: 5500928
-        max: 359682512
+        min: 4841472
+        max: 335577584
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: jn5qevln5
+      job_id: j1p87qxz5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 111273.0
-      throughput: 8.98690607784458
+      inference_time: 109130.0
+      throughput: 9.163383121048291
       estimated_peak_memory_range:
-        min: 9814016
-        max: 110733232
+        min: 9969664
+        max: 88942624
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 51
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 51
-      job_id: jw56ew86g
+      job_id: j1glkvxep
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 110582.0
-      throughput: 9.043063066321825
+      inference_time: 118569.0
+      throughput: 8.433907682446508
       estimated_peak_memory_range:
-        min: 16162816
-        max: 113694432
+        min: 22605824
+        max: 100595248
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 53
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jwgok8lqp
+        total_layers: 53
+      job_id: j1pvw6d7g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.687994Z'
+    timestamp: '2024-05-20T16:35:31.352389Z'
   - torchscript_onnx_tflite:
-      inference_time: 160844.0
-      throughput: 6.2172042475939415
+      inference_time: 157031.0
+      throughput: 6.368169342359152
       estimated_peak_memory_range:
-        min: 323584
-        max: 237497504
+        min: 6692864
+        max: 464186128
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 31
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 31
-      job_id: jw56e900g
+      job_id: jogkye4yp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 150008.0
-      throughput: 6.666311130073063
+      inference_time: 146356.0
+      throughput: 6.832654622974118
       estimated_peak_memory_range:
-        min: 9900032
-        max: 34159264
+        min: 9895936
+        max: 31713392
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 51
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 51
-      job_id: jo5mq11wp
+      job_id: j1p3mj9xg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.688021Z'
+    timestamp: '2024-05-20T16:35:31.352406Z'
+  - torchscript_onnx_qnn:
+      inference_time: 190735.0
+      throughput: 5.24287624190631
+      estimated_peak_memory_range:
+        min: 9850880
+        max: 9850880
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 51
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 51
+      job_id: jw561y7vp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 146581.0
+      throughput: 6.82216658366364
+      estimated_peak_memory_range:
+        min: 9854976
+        max: 9854976
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 53
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 53
+      job_id: j7gjlv77p
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 1963452.0
+      throughput: 0.5093070775348723
+      estimated_peak_memory_range:
+        min: 1940811776
+        max: 1940811776
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 31
+        total_layers: 31
+      job_id: jlpevdz75
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.352430Z'
```

## qai_hub_models/models/vit/export.py

```diff
@@ -117,17 +117,24 @@
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(
         model.to("cpu"), make_torch_inputs(input_spec), check_trace=False
     )
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -157,16 +164,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -195,14 +206,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/vit/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: VIT
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 119744.0
-      throughput: 8.351149118118654
+      inference_time: 79223.0
+      throughput: 12.622596973101246
       estimated_peak_memory_range:
-        min: 196608
-        max: 3447072
+        min: 126976
+        max: 3307040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 535
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 535
-      job_id: j7gjzq3v5
+      job_id: jygz73mzp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 67117.0
+      throughput: 14.899354857934652
+      estimated_peak_memory_range:
+        min: 32768
+        max: 42487808
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 386
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 386
+      job_id: jnp184jkg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 128755.0
-      throughput: 7.766688672284571
+      inference_time: 104492.0
+      throughput: 9.570110630478888
       estimated_peak_memory_range:
-        min: 36864
-        max: 430908512
+        min: 73728
+        max: 437745512
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 376
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jygzonzo5
+        total_layers: 376
+      job_id: j0px1k4jg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.712224Z'
+    timestamp: '2024-05-20T16:35:31.382740Z'
   - torchscript_onnx_tflite:
-      inference_time: 89024.0
-      throughput: 11.23292595255212
+      inference_time: 56817.0
+      throughput: 17.60036608761462
       estimated_peak_memory_range:
-        min: 151552
-        max: 407939792
+        min: 114688
+        max: 373000000
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 535
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 535
-      job_id: jlpeey6op
+      job_id: jz5w9e7zp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 48402.0
+      throughput: 20.660303293252344
+      estimated_peak_memory_range:
+        min: 0
+        max: 164302880
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 386
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 386
+      job_id: jvgdvx3kg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 98667.0
-      throughput: 10.135100894929408
+      inference_time: 76327.0
+      throughput: 13.101523707207148
       estimated_peak_memory_range:
-        min: 663552
-        max: 874006192
+        min: 638976
+        max: 514001424
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 376
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jz5w24y35
+        total_layers: 376
+      job_id: jo5mznmyp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.712295Z'
+    timestamp: '2024-05-20T16:35:31.382767Z'
   - torchscript_onnx_tflite:
-      inference_time: 119402.0
-      throughput: 8.375069094320029
+      inference_time: 78953.0
+      throughput: 12.665763175560143
       estimated_peak_memory_range:
-        min: 135168
-        max: 4419520
+        min: 143360
+        max: 3490600
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 535
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 535
-      job_id: jqpyrkk75
+      job_id: jmg94lmq5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 67350.0
+      throughput: 14.847809948032666
+      estimated_peak_memory_range:
+        min: 12288
+        max: 46277240
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 386
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 386
+      job_id: jqp4wl1qg
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.712360Z'
+    timestamp: '2024-05-20T16:35:31.382784Z'
+  - torchscript_onnx_qnn:
+      inference_time: 65972.0
+      throughput: 15.157945795185837
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 385
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 385
+      job_id: jz57dy4q5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 103551.0
+      throughput: 9.657077189017972
+      estimated_peak_memory_range:
+        min: 176091136
+        max: 176091136
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 376
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 376
+      job_id: jegne6zvg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jopryvlvg
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.382809Z'
```

## qai_hub_models/models/whisper_base_en/export.py

```diff
@@ -130,15 +130,15 @@
         component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options
+            target_runtime, compile_options, hub_device
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub_device,
             name=f"{model_name}_{component_name}",
@@ -222,19 +222,14 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(
-        model_cls=Model,
-        components=ALL_COMPONENTS,
-        supports_qnn=False,
-        supports_ort=False,
-    )
+    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/whisper_base_en/perf.yaml

```diff
@@ -18,218 +18,416 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: WhisperEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 154415.0
-      throughput: 6.476054787423502
+      inference_time: 159429.0
+      throughput: 6.272384572442905
       estimated_peak_memory_range:
-        min: 36925440
-        max: 139242008
+        min: 25227264
+        max: 130754096
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 303
+        layers_on_gpu: 419
         layers_on_cpu: 0
-        total_layers: 303
-      job_id: jnp1y6o8p
+        total_layers: 419
+      job_id: jep2mkrx5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 622656.0
+      throughput: 1.6060232295199919
+      estimated_peak_memory_range:
+        min: 12288
+        max: 87059512
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 580
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 580
+      job_id: j1glkvyep
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 394707.0
+      throughput: 2.53352486781334
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 75538432
+        max: 255421288
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 380
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: j0pxnx035
-      job_status: Failed
+        total_layers: 380
+      job_id: jz5w9elzp
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.730149Z'
+    timestamp: '2024-05-20T16:35:31.407290Z'
   - torchscript_onnx_tflite:
-      inference_time: 118628.0
-      throughput: 8.42971305256769
+      inference_time: 122468.0
+      throughput: 8.16539830812947
       estimated_peak_memory_range:
-        min: 36814848
-        max: 61467824
+        min: 0
+        max: 42440336
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 303
+        layers_on_gpu: 419
         layers_on_cpu: 0
-        total_layers: 303
-      job_id: jz5709ovg
+        total_layers: 419
+      job_id: j2p0rzm2p
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+    torchscript_onnx_qnn:
+      inference_time: 454603.0
+      throughput: 2.1997215152561687
       estimated_peak_memory_range:
         min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        max: 198547792
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 580
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jegnlk1k5
-      job_status: Failed
+        total_layers: 580
+      job_id: j1p3mjzxg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 304852.0
+      throughput: 3.280280267146025
+      estimated_peak_memory_range:
+        min: 73445376
+        max: 277367024
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 380
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 380
+      job_id: jnp184nkg
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.730201Z'
+    timestamp: '2024-05-20T16:35:31.407316Z'
   - torchscript_onnx_tflite:
-      inference_time: 157798.0
-      throughput: 6.337215934295745
+      inference_time: 157524.0
+      throughput: 6.348238998501816
       estimated_peak_memory_range:
-        min: 25370624
-        max: 124671888
+        min: 29507584
+        max: 129166896
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 303
+        layers_on_gpu: 419
         layers_on_cpu: 0
-        total_layers: 303
-      job_id: jlpee001p
+        total_layers: 419
+      job_id: jogkye2yp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 625414.0
+      throughput: 1.5989408615732938
+      estimated_peak_memory_range:
+        min: 1048576
+        max: 78119600
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 580
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 580
+      job_id: jlpevd775
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.730243Z'
+    timestamp: '2024-05-20T16:35:31.407333Z'
+  - torchscript_onnx_qnn:
+      inference_time: 459784.0
+      throughput: 2.1749343169836273
+      estimated_peak_memory_range:
+        min: 962560
+        max: 962560
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 579
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 579
+      job_id: j1pvw6l7g
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 390367.0
+      throughput: 2.56169194629669
+      estimated_peak_memory_range:
+        min: 139673600
+        max: 139673600
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 380
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 380
+      job_id: jz57dyeq5
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j0px1kljg
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.407358Z'
 - name: WhisperDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 13793.0
-      throughput: 72.50054375407815
+      inference_time: 23342.0
+      throughput: 42.84123040013709
       estimated_peak_memory_range:
-        min: 5775360
-        max: 8469096
+        min: 5783552
+        max: 8760040
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 447
+        layers_on_npu: 983
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 449
-      job_id: jvgde26r5
+        layers_on_cpu: 0
+        total_layers: 983
+      job_id: jqpyd1orp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 23335.0
+      throughput: 42.854081851296336
+      estimated_peak_memory_range:
+        min: 41029632
+        max: 57664648
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 821
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 821
+      job_id: jw561y8vp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 17653.0
-      throughput: 56.64759530957911
+      inference_time: 24574.0
+      throughput: 40.6934158053227
       estimated_peak_memory_range:
-        min: 11657216
-        max: 330606792
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 11902976
+        max: 207621344
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 844
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 3
-      job_id: jo5mq89dp
+        layers_on_cpu: 0
+        total_layers: 844
+      job_id: jmg94lzq5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.730305Z'
+    timestamp: '2024-05-20T16:35:31.407380Z'
   - torchscript_onnx_tflite:
-      inference_time: 10194.0
-      throughput: 98.09691975671964
+      inference_time: 19155.0
+      throughput: 52.205690420255806
       estimated_peak_memory_range:
-        min: 3768320
-        max: 98615936
+        min: 3674112
+        max: 90342624
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 447
+        layers_on_npu: 983
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 449
-      job_id: jqp4k3e8g
+        layers_on_cpu: 0
+        total_layers: 983
+      job_id: j1p87qez5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 18519.0
+      throughput: 53.99859603650305
+      estimated_peak_memory_range:
+        min: 131715072
+        max: 412276656
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 821
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 821
+      job_id: jwgov2l45
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 14072.0
-      throughput: 71.0631040363843
+      inference_time: 20701.0
+      throughput: 48.30684507994783
       estimated_peak_memory_range:
-        min: 52715520
-        max: 167779568
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 55021568
+        max: 137177376
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 844
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 3
-      job_id: jopr8wx05
+        layers_on_cpu: 0
+        total_layers: 844
+      job_id: jvgdvxdkg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.730361Z'
+    timestamp: '2024-05-20T16:35:31.407400Z'
   - torchscript_onnx_tflite:
-      inference_time: 13928.0
-      throughput: 71.79781734635267
+      inference_time: 23210.0
+      throughput: 43.084877208099954
+      estimated_peak_memory_range:
+        min: 1146880
+        max: 5317720
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 983
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 983
+      job_id: jn5q26l75
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 23685.0
+      throughput: 42.22081486172683
       estimated_peak_memory_range:
-        min: 5758976
-        max: 8442936
+        min: 42434560
+        max: 57209568
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 447
+        layers_on_npu: 821
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 449
-      job_id: jygzoqqk5
+        layers_on_cpu: 0
+        total_layers: 821
+      job_id: jygz73lzp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.730409Z'
+    timestamp: '2024-05-20T16:35:31.407416Z'
+  - torchscript_onnx_qnn:
+      inference_time: 13480.0
+      throughput: 74.1839762611276
+      estimated_peak_memory_range:
+        min: 42463232
+        max: 42463232
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 821
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 821
+      job_id: j7gjlvr7p
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 20213.0
+      throughput: 49.47311136397368
+      estimated_peak_memory_range:
+        min: 112713728
+        max: 112713728
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 844
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 844
+      job_id: jqp4wlyqg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jo5mzn0yp
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.407452Z'
```

## qai_hub_models/models/whisper_small_en/export.py

```diff
@@ -130,15 +130,15 @@
         component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options
+            target_runtime, compile_options, hub_device
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub_device,
             name=f"{model_name}_{component_name}",
@@ -222,19 +222,14 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(
-        model_cls=Model,
-        components=ALL_COMPONENTS,
-        supports_qnn=False,
-        supports_ort=False,
-    )
+    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/whisper_small_en/perf.yaml

```diff
@@ -18,218 +18,416 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: WhisperEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 600006.0
-      throughput: 1.666650000166665
+      inference_time: 615600.0
+      throughput: 1.6244314489928524
       estimated_peak_memory_range:
-        min: 79036416
-        max: 532898328
+        min: 12288
+        max: 448683040
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 585
+        layers_on_gpu: 911
         layers_on_cpu: 0
-        total_layers: 585
-      job_id: j2p036o9p
+        total_layers: 911
+      job_id: jegne61vg
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jogkye6yp
+      job_status: Failed
     torchscript_onnx_ort:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: j1gl6lwjg
+      job_id: jlpevd675
       job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.762329Z'
+    timestamp: '2024-05-20T16:35:31.453698Z'
   - torchscript_onnx_tflite:
-      inference_time: 465622.0
-      throughput: 2.1476648440151025
+      inference_time: 470667.0
+      throughput: 2.124644387645618
       estimated_peak_memory_range:
-        min: 110800896
-        max: 143440272
+        min: 108802048
+        max: 205784096
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 585
+        layers_on_gpu: 911
         layers_on_cpu: 0
-        total_layers: 585
-      job_id: jogk786wp
+        total_layers: 911
+      job_id: jep2mkox5
       job_status: Passed
-    torchscript_onnx_ort:
-      inference_time: 'null'
-      throughput: 'null'
+    torchscript_onnx_qnn:
+      inference_time: 1479203.0
+      throughput: 0.6760397322071413
       estimated_peak_memory_range:
         min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        max: 569102256
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 1474
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: j1p3v6o3g
-      job_status: Failed
+        total_layers: 1474
+      job_id: j1glkvwep
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1261557.0
+      throughput: 0.7926712784281645
+      estimated_peak_memory_range:
+        min: 999424
+        max: 563911776
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 884
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 884
+      job_id: jz5w9eyzp
+      job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.762404Z'
+    timestamp: '2024-05-20T16:35:31.453725Z'
   - torchscript_onnx_tflite:
-      inference_time: 602366.0
-      throughput: 1.66012025911157
+      inference_time: 612583.0
+      throughput: 1.63243185005134
       estimated_peak_memory_range:
-        min: 72904704
-        max: 522853520
+        min: 16384
+        max: 444838416
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 585
+        layers_on_gpu: 911
         layers_on_cpu: 0
-        total_layers: 585
-      job_id: j2p038w9p
+        total_layers: 911
+      job_id: j2p0rzo2p
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1pvw627g
+      job_status: Failed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.762472Z'
+    timestamp: '2024-05-20T16:35:31.453757Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1707514.0
+      throughput: 0.5856467355465313
+      estimated_peak_memory_range:
+        min: 962560
+        max: 962560
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 1473
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 1473
+      job_id: j1p3mjoxg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1518658.0
+      throughput: 0.6584761019268328
+      estimated_peak_memory_range:
+        min: 555753472
+        max: 555753472
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 884
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 884
+      job_id: jnp184okg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jz5w9eyjp
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.453784Z'
 - name: WhisperDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 45614.0
-      throughput: 21.92309378699522
+      inference_time: 26229.0
+      throughput: 38.12573868618704
       estimated_peak_memory_range:
-        min: 16830464
-        max: 20007784
+        min: 16203776
+        max: 19541664
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 879
+        layers_on_npu: 2573
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 881
-      job_id: j1p801jkg
+        layers_on_cpu: 0
+        total_layers: 2573
+      job_id: jopryvxvg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 24425.0
+      throughput: 40.941658137154555
+      estimated_peak_memory_range:
+        min: 121384960
+        max: 195379040
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 2255
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 2255
+      job_id: jn5q26475
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 75579.0
-      throughput: 13.231188557668135
+      inference_time: 62618.0
+      throughput: 15.969848925229167
       estimated_peak_memory_range:
-        min: 40751104
-        max: 289480944
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 49823744
+        max: 691829120
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 2302
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 3
-      job_id: jw56ewo6g
+        layers_on_cpu: 0
+        total_layers: 2302
+      job_id: jygz73zzp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.762570Z'
+    timestamp: '2024-05-20T16:35:31.453805Z'
   - torchscript_onnx_tflite:
-      inference_time: 34559.0
-      throughput: 28.936022454353424
+      inference_time: 19526.0
+      throughput: 51.21376626037079
       estimated_peak_memory_range:
-        min: 15560704
-        max: 1589538480
+        min: 16277504
+        max: 1152242688
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 879
+        layers_on_npu: 2573
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 881
-      job_id: jn5qev4n5
+        layers_on_cpu: 0
+        total_layers: 2573
+      job_id: jqpyd18rp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 19235.0
+      throughput: 51.988562516246425
+      estimated_peak_memory_range:
+        min: 110612480
+        max: 902217440
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 2255
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 2255
+      job_id: jw561yovp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 60639.0
-      throughput: 16.49103712132456
+      inference_time: 53225.0
+      throughput: 18.788163457022076
       estimated_peak_memory_range:
-        min: 160247808
-        max: 557923088
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 84680704
+        max: 354730464
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 2302
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 3
-      job_id: jwgok8dqp
+        layers_on_cpu: 0
+        total_layers: 2302
+      job_id: jmg94loq5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.762666Z'
+    timestamp: '2024-05-20T16:35:31.453825Z'
   - torchscript_onnx_tflite:
-      inference_time: 45957.0
-      throughput: 21.75947080966991
+      inference_time: 27363.0
+      throughput: 36.54570039834813
       estimated_peak_memory_range:
         min: 16830464
-        max: 19552208
+        max: 19976992
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 2573
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 2573
+      job_id: j1p87qjz5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 25042.0
+      throughput: 39.93291270665282
+      estimated_peak_memory_range:
+        min: 127197184
+        max: 202463224
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 879
+        layers_on_npu: 2255
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 881
-      job_id: j1p80dnkg
+        layers_on_cpu: 0
+        total_layers: 2255
+      job_id: j7gjlv37p
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.762757Z'
+    timestamp: '2024-05-20T16:35:31.453845Z'
+  - torchscript_onnx_qnn:
+      inference_time: 20874.0
+      throughput: 47.906486538277285
+      estimated_peak_memory_range:
+        min: 127381504
+        max: 127381504
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 2255
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 2255
+      job_id: jwgov2d45
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 54047.0
+      throughput: 18.502414565100747
+      estimated_peak_memory_range:
+        min: 347856896
+        max: 347856896
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 2302
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 2302
+      job_id: jvgdvx6kg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jmg94lov5
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.453870Z'
```

## qai_hub_models/models/whisper_tiny_en/export.py

```diff
@@ -130,15 +130,15 @@
         component.eval()
         source_model = torch.jit.trace(
             component.to("cpu"), make_torch_inputs(input_spec)
         )
 
         # 2. Compile the models to an on-device asset
         model_compile_options = component.get_hub_compile_options(
-            target_runtime, compile_options
+            target_runtime, compile_options, hub_device
         )
         print(f"Optimizing model {component_name} to run on-device")
         submitted_compile_job = hub.submit_compile_job(
             model=source_model,
             input_specs=input_spec,
             device=hub_device,
             name=f"{model_name}_{component_name}",
@@ -222,19 +222,14 @@
         )
         for component_name in components
     }
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(
-        model_cls=Model,
-        components=ALL_COMPONENTS,
-        supports_qnn=False,
-        supports_ort=False,
-    )
+    parser = export_parser(model_cls=Model, components=ALL_COMPONENTS)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/whisper_tiny_en/perf.yaml

```diff
@@ -18,218 +18,416 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: WhisperEncoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 67351.0
-      throughput: 14.847589493845675
+      inference_time: 68887.0
+      throughput: 14.516527066064715
       estimated_peak_memory_range:
-        min: 16117760
-        max: 104999648
+        min: 11296768
+        max: 56646392
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 209
+        layers_on_gpu: 271
         layers_on_cpu: 0
-        total_layers: 209
-      job_id: jlpeeyxop
+        total_layers: 271
+      job_id: jnp184olg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 288969.0
+      throughput: 3.4605788164128333
+      estimated_peak_memory_range:
+        min: 159744
+        max: 54792792
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 338
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 338
+      job_id: jegne6qmg
       job_status: Passed
     torchscript_onnx_ort:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jnp1y618p
+      job_id: j1glkv8lp
       job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.794639Z'
+    timestamp: '2024-05-20T16:35:31.499723Z'
   - torchscript_onnx_tflite:
-      inference_time: 52682.0
-      throughput: 18.981815420826848
+      inference_time: 54355.0
+      throughput: 18.397571520559286
       estimated_peak_memory_range:
         min: 0
-        max: 28255008
+        max: 32722000
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 209
+        layers_on_gpu: 271
         layers_on_cpu: 0
-        total_layers: 209
-      job_id: jz5w24z35
+        total_layers: 271
+      job_id: jz57dynr5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 218798.0
+      throughput: 4.570425689448715
+      estimated_peak_memory_range:
+        min: 999424
+        max: 138033888
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 338
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 338
+      job_id: jep2mkdm5
       job_status: Passed
     torchscript_onnx_ort:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: jz5709nvg
+      job_id: j1p3mj7zg
       job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.794681Z'
+    timestamp: '2024-05-20T16:35:31.499751Z'
   - torchscript_onnx_tflite:
-      inference_time: 67311.0
-      throughput: 14.856412770572417
+      inference_time: 68575.0
+      throughput: 14.582573824279985
       estimated_peak_memory_range:
-        min: 17125376
-        max: 63332656
+        min: 12288
+        max: 95017352
       primary_compute_unit: GPU
       precision: fp16
       layer_info:
         layers_on_npu: 0
-        layers_on_gpu: 209
+        layers_on_gpu: 271
+        layers_on_cpu: 0
+        total_layers: 271
+      job_id: j0px1kr9g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 293385.0
+      throughput: 3.40849054995995
+      estimated_peak_memory_range:
+        min: 978944
+        max: 49011728
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 338
+        layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 209
-      job_id: jygzoq1o5
+        total_layers: 338
+      job_id: jogkye0op
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.794711Z'
+    timestamp: '2024-05-20T16:35:31.499770Z'
+  - torchscript_onnx_qnn:
+      inference_time: 240121.0
+      throughput: 4.164567030788644
+      estimated_peak_memory_range:
+        min: 962560
+        max: 962560
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 337
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 337
+      job_id: j2p0rz9ep
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1pvw6mmg
+      job_status: Failed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jlpevdx05
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.499797Z'
 - name: WhisperDecoder
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 7115.0
-      throughput: 140.54813773717498
+      inference_time: 3871.0
+      throughput: 258.3311805734952
       estimated_peak_memory_range:
         min: 2977792
-        max: 5417544
+        max: 5435904
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 303
+        layers_on_npu: 557
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 305
-      job_id: jygzonyo5
+        layers_on_cpu: 0
+        total_layers: 557
+      job_id: jvgdvx6lg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 3646.0
+      throughput: 274.27317608337904
+      estimated_peak_memory_range:
+        min: 9920512
+        max: 47146336
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 447
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 447
+      job_id: jopryvdeg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 8714.0
-      throughput: 114.75786091347257
+      inference_time: 5287.0
+      throughput: 189.14318138831095
       estimated_peak_memory_range:
-        min: 6172672
-        max: 212702328
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 6336512
+        max: 214447104
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 462
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 3
-      job_id: jvgde24r5
+        layers_on_cpu: 0
+        total_layers: 462
+      job_id: jw561ym7p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.794757Z'
+    timestamp: '2024-05-20T16:35:31.499823Z'
   - torchscript_onnx_tflite:
-      inference_time: 5479.0
-      throughput: 182.5150574922431
+      inference_time: 3044.0
+      throughput: 328.515111695138
       estimated_peak_memory_range:
-        min: 2871296
-        max: 232253952
+        min: 36864
+        max: 223105088
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 303
+        layers_on_npu: 557
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 305
-      job_id: jmg9jd2w5
+        layers_on_cpu: 0
+        total_layers: 557
+      job_id: jqp4wl4lg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 2767.0
+      throughput: 361.4022406938923
+      estimated_peak_memory_range:
+        min: 9170944
+        max: 143104560
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 447
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 447
+      job_id: jqpyd124p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 6141.0
-      throughput: 162.83992835043153
+      inference_time: 4230.0
+      throughput: 236.4066193853428
       estimated_peak_memory_range:
-        min: 24158208
-        max: 103238656
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 27504640
+        max: 86953184
+      primary_compute_unit: NPU
+      precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 462
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 3
-      job_id: jqp4k348g
+        layers_on_cpu: 0
+        total_layers: 462
+      job_id: jwgov2wd5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.794805Z'
+    timestamp: '2024-05-20T16:35:31.499844Z'
   - torchscript_onnx_tflite:
-      inference_time: 7148.0
-      throughput: 139.89927252378288
+      inference_time: 3892.0
+      throughput: 256.9373072970195
       estimated_peak_memory_range:
         min: 2977792
-        max: 5388280
+        max: 7226936
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 557
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 557
+      job_id: jo5mznkqp
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 3696.0
+      throughput: 270.56277056277054
+      estimated_peak_memory_range:
+        min: 11145216
+        max: 48599472
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 303
+        layers_on_npu: 447
         layers_on_gpu: 0
-        layers_on_cpu: 2
-        total_layers: 305
-      job_id: jz5w20j35
+        layers_on_cpu: 0
+        total_layers: 447
+      job_id: jn5q261m5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.794844Z'
+    timestamp: '2024-05-20T16:35:31.499860Z'
+  - torchscript_onnx_qnn:
+      inference_time: 3823.0
+      throughput: 261.5746795710175
+      estimated_peak_memory_range:
+        min: 21233664
+        max: 21233664
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 447
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 447
+      job_id: j1p87qr85
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 4460.0
+      throughput: 224.2152466367713
+      estimated_peak_memory_range:
+        min: 21245952
+        max: 21245952
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 462
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 462
+      job_id: j7gjlvy8p
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jygz73y6p
+      job_status: Failed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.499881Z'
```

## qai_hub_models/models/wideresnet50/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,20 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -193,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/wideresnet50/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: WideResNet50
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 4900.0
-      throughput: 204.08163265306123
+      inference_time: 4874.0
+      throughput: 205.1702913418137
       estimated_peak_memory_range:
-        min: 49152
-        max: 2616288
+        min: 20480
+        max: 2339968
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 79
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 79
-      job_id: jegnlkqk5
+      job_id: jz5w9ezjp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 5767.0
-      throughput: 173.40038148083926
+      inference_time: 5693.0
+      throughput: 175.65431231336729
       estimated_peak_memory_range:
-        min: 618496
-        max: 261398592
+        min: 643072
+        max: 344558120
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 126
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 126
-      job_id: jep20edrg
+      job_id: jvgdvx4lg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 5427.0
-      throughput: 184.26386585590566
+      inference_time: 5517.0
+      throughput: 181.257930034439
       estimated_peak_memory_range:
-        min: 36864
-        max: 457326944
+        min: 24576
+        max: 414560576
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 128
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j2p03699p
+        total_layers: 128
+      job_id: jo5mznlqp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.826943Z'
+    timestamp: '2024-05-20T16:35:31.545698Z'
   - torchscript_onnx_tflite:
-      inference_time: 3655.0
-      throughput: 273.59781121751024
+      inference_time: 3649.0
+      throughput: 274.0476842970677
       estimated_peak_memory_range:
-        min: 16384
-        max: 97733152
+        min: 12288
+        max: 97464480
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 79
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 79
-      job_id: jopr8wd05
+      job_id: jmg94l2v5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 4245.0
-      throughput: 235.57126030624264
+      inference_time: 4302.0
+      throughput: 232.4500232450023
       estimated_peak_memory_range:
-        min: 618496
-        max: 53403616
+        min: 270987264
+        max: 325564848
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 126
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 126
-      job_id: jqpyrm285
+      job_id: jz57dy8r5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 4122.0
-      throughput: 242.600679281902
+      inference_time: 4156.0
+      throughput: 240.61597690086623
       estimated_peak_memory_range:
         min: 618496
-        max: 39529440
+        max: 36255216
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 128
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1p801rkg
+        total_layers: 128
+      job_id: jegne6wmg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.826991Z'
+    timestamp: '2024-05-20T16:35:31.545724Z'
   - torchscript_onnx_tflite:
-      inference_time: 4907.0
-      throughput: 203.79050336254332
+      inference_time: 4864.0
+      throughput: 205.5921052631579
       estimated_peak_memory_range:
-        min: 28672
-        max: 2415760
+        min: 24576
+        max: 2245440
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 79
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 79
-      job_id: j2p038n9p
+      job_id: jnp1841lg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 5790.0
-      throughput: 172.71157167530225
+      inference_time: 5687.0
+      throughput: 175.83963425356075
       estimated_peak_memory_range:
-        min: 622592
-        max: 209332032
+        min: 647168
+        max: 355205232
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 126
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 126
-      job_id: jw56e9k6g
+      job_id: j0px1kz9g
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.827030Z'
+    timestamp: '2024-05-20T16:35:31.545745Z'
+  - torchscript_onnx_qnn:
+      inference_time: 5857.0
+      throughput: 170.73587160662456
+      estimated_peak_memory_range:
+        min: 602112
+        max: 602112
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 126
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 126
+      job_id: jqp4wl2lg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 5137.0
+      throughput: 194.66614755693985
+      estimated_peak_memory_range:
+        min: 46718976
+        max: 46718976
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 128
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 128
+      job_id: jopryv7eg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 27924.0
+      throughput: 35.8114883254548
+      estimated_peak_memory_range:
+        min: 36831232
+        max: 36831232
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jep2mkzm5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.545770Z'
```

## qai_hub_models/models/wideresnet50_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image_tensor"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image_tensor"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,20 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image_tensor", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last(
+                "image_tensor", sample_inputs, target_runtime
+            )
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -201,14 +212,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/wideresnet50_quantized/perf.yaml

```diff
@@ -22,240 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: WideResNet50-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1807.0
-      throughput: 553.4034311012729
+      inference_time: 1821.0
+      throughput: 549.1488193300385
       estimated_peak_memory_range:
-        min: 49152
-        max: 2181928
+        min: 24576
+        max: 2584464
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 80
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 80
-      job_id: jn5qev1n5
+      job_id: jqpyd1y4p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2119.0
-      throughput: 471.92071731949034
+      inference_time: 2043.0
+      throughput: 489.47626040137055
       estimated_peak_memory_range:
-        min: 0
-        max: 480120320
+        min: 16384
+        max: 250792696
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 78
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 78
-      job_id: jw56ewm6g
+      job_id: jogkyekop
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 2464.0
-      throughput: 405.84415584415586
+      inference_time: 2117.0
+      throughput: 472.3665564478035
       estimated_peak_memory_range:
-        min: 24576
-        max: 187692992
+        min: 110592
+        max: 324998136
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jwgok8wqp
+        total_layers: 86
+      job_id: j1p3mjrzg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.850968Z'
+    timestamp: '2024-05-20T16:35:31.575903Z'
   - torchscript_onnx_tflite:
-      inference_time: 1351.0
-      throughput: 740.1924500370096
+      inference_time: 1377.0
+      throughput: 726.2164124909223
       estimated_peak_memory_range:
         min: 12288
-        max: 55206416
+        max: 54112960
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 80
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 80
-      job_id: j1gl6l8jg
+      job_id: j2p0rzxep
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1589.0
-      throughput: 629.3266205160478
+      inference_time: 1526.0
+      throughput: 655.307994757536
       estimated_peak_memory_range:
-        min: 167936
-        max: 45857248
+        min: 0
+        max: 44606448
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 78
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 78
-      job_id: j1p3v673g
+      job_id: jn5q26dm5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1858.0
-      throughput: 538.2131324004306
+      inference_time: 1713.0
+      throughput: 583.7711617046118
       estimated_peak_memory_range:
         min: 0
-        max: 28645856
+        max: 30424256
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: j1pv07nk5
+        total_layers: 86
+      job_id: jwgov29d5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.851014Z'
+    timestamp: '2024-05-20T16:35:31.575930Z'
   - torchscript_onnx_tflite:
-      inference_time: 8152.0
-      throughput: 122.6692836113837
+      inference_time: 1831.0
+      throughput: 546.1496450027307
       estimated_peak_memory_range:
-        min: 12288
-        max: 25276096
+        min: 61440
+        max: 1506248
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 82
+        layers_on_npu: 80
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 82
-      job_id: j1pv0yxk5
+        total_layers: 80
+      job_id: j1p87qk85
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 'null'
-      throughput: 'null'
+      inference_time: 2035.0
+      throughput: 491.4004914004914
       estimated_peak_memory_range:
-        min: 0
-        max: 0
-      primary_compute_unit: 'null'
-      precision: 'null'
+        min: 16384
+        max: 250480080
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 78
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 0
-      job_id: jw56ex4yg
-      job_status: Failed
-    torchscript_onnx_ort:
-      inference_time: 75852.0
-      throughput: 13.183568000843747
+        total_layers: 78
+      job_id: jw561y07p
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:31.575948Z'
+  - torchscript_onnx_tflite:
+      inference_time: 8208.0
+      throughput: 121.83235867446393
       estimated_peak_memory_range:
-        min: 4431872
-        max: 54054544
-      primary_compute_unit: CPU
-      precision: fp32
+        min: 12288
+        max: 26585200
+      primary_compute_unit: NPU
+      precision: int8
       layer_info:
-        layers_on_npu: 0
+        layers_on_npu: 80
         layers_on_gpu: 0
-        layers_on_cpu: 88
-        total_layers: 88
-      job_id: j7gjzq8v5
+        layers_on_cpu: 0
+        total_layers: 80
+      job_id: j2p0lxj6p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 8312.0
+      throughput: 120.30798845043311
+      estimated_peak_memory_range:
+        min: 94208
+        max: 42576560
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 78
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 78
+      job_id: j1p3er9l5
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:33.851058Z'
+    timestamp: '2024-05-20T16:35:31.575965Z'
   - torchscript_onnx_tflite:
-      inference_time: 24077.0
-      throughput: 41.533413631266356
+      inference_time: 23889.0
+      throughput: 41.8602704173469
       estimated_peak_memory_range:
         min: 45056
-        max: 2559568
+        max: 2992736
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 82
+        layers_on_npu: 80
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 82
-      job_id: jnp1wo8lg
+        total_layers: 80
+      job_id: j1p8zkxxp
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:33.851077Z'
-  - torchscript_onnx_tflite:
-      inference_time: 1831.0
-      throughput: 546.1496450027307
+    timestamp: '2024-05-20T16:35:31.575975Z'
+  - torchscript_onnx_qnn:
+      inference_time: 1966.0
+      throughput: 508.646998982706
       estimated_peak_memory_range:
-        min: 32768
-        max: 1466192
+        min: 344064
+        max: 344064
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 82
+        layers_on_npu: 78
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 82
-      job_id: j2p038q9p
+        total_layers: 78
+      job_id: j1glkvqlp
       job_status: Passed
-    torchscript_onnx_qnn:
-      inference_time: 2151.0
-      throughput: 464.9000464900046
+    torchscript_onnx_ort:
+      inference_time: 1912.0
+      throughput: 523.0125523012553
       estimated_peak_memory_range:
-        min: 622592
-        max: 7136072
+        min: 115851264
+        max: 115851264
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 80
+        layers_on_npu: 86
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 80
-      job_id: jqp4k601g
+        total_layers: 86
+      job_id: j1pvw6nmg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 256303.0
+      throughput: 3.9016320526876394
+      estimated_peak_memory_range:
+        min: 20701184
+        max: 20701184
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j7gjlv88p
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.851108Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.575998Z'
```

## qai_hub_models/models/xlsr/export.py

```diff
@@ -115,20 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -158,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -187,24 +193,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/xlsr/model.py

```diff
@@ -45,15 +45,15 @@
         model.eval()
 
         return cls(model)
 
     def get_evaluator(self) -> BaseEvaluator:
         return SuperResolutionOutputEvaluator()
 
-    def forward(self, image: torch.Tensor) -> torch.Tensor:
+    def forward(self, image):
         """
         Run XLSR on `image`, and produce an upscaled image
 
         Parameters:
             image: Pixel values pre-processed for model consumption.
                    Range: float[0, 1]
                    3-channel Color Space: RGB
```

## qai_hub_models/models/xlsr/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: XLSR
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2596.0
-      throughput: 385.2080123266564
+      inference_time: 2482.0
+      throughput: 402.90088638195004
       estimated_peak_memory_range:
-        min: 12288
-        max: 1829544
+        min: 16384
+        max: 1867704
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 13
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 16
-      job_id: jlpeeynop
+      job_id: jlpevdn05
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 971.0
-      throughput: 1029.8661174047375
+      inference_time: 1346.0
+      throughput: 742.9420505200594
       estimated_peak_memory_range:
-        min: 217088
-        max: 11994560
+        min: 16384
+        max: 5062976
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 21
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 21
-      job_id: jz5w24r35
+      job_id: jmg94lqv5
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1502.0
-      throughput: 665.7789613848203
+      inference_time: 1552.0
+      throughput: 644.3298969072165
       estimated_peak_memory_range:
-        min: 212992
-        max: 8613544
+        min: 16384
+        max: 72227024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 23
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jnp1y6m8p
+        total_layers: 23
+      job_id: jqp4wl6lg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.886071Z'
+    timestamp: '2024-05-20T16:35:31.615271Z'
   - torchscript_onnx_tflite:
-      inference_time: 1833.0
-      throughput: 545.5537370430987
+      inference_time: 1775.0
+      throughput: 563.3802816901408
       estimated_peak_memory_range:
         min: 16384
-        max: 19549104
+        max: 20190320
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 13
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 16
-      job_id: jygzon0o5
+      job_id: jygz7306p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 632.0
-      throughput: 1582.2784810126582
+      inference_time: 834.0
+      throughput: 1199.0407673860911
       estimated_peak_memory_range:
-        min: 208896
-        max: 17756816
+        min: 0
+        max: 16978032
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 21
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 21
-      job_id: jmg9jdqw5
+      job_id: jnp184mlg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 1006.0
-      throughput: 994.0357852882704
+      inference_time: 1029.0
+      throughput: 971.8172983479105
       estimated_peak_memory_range:
-        min: 344064
-        max: 16233520
+        min: 0
+        max: 15374048
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 23
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jvgde2mr5
+        total_layers: 23
+      job_id: j0px1k89g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.886105Z'
+    timestamp: '2024-05-20T16:35:31.615298Z'
   - torchscript_onnx_tflite:
-      inference_time: 2709.0
-      throughput: 369.139904023625
+      inference_time: 2490.0
+      throughput: 401.60642570281124
       estimated_peak_memory_range:
-        min: 6631424
-        max: 8101008
+        min: 12623872
+        max: 14367408
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 13
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 16
-      job_id: jmg9jrn85
+      job_id: jz5w9erjp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 963.0
-      throughput: 1038.4215991692627
+      inference_time: 1362.0
+      throughput: 734.2143906020558
       estimated_peak_memory_range:
-        min: 212992
-        max: 33066344
+        min: 49152
+        max: 9493856
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 21
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 21
-      job_id: jqp4k7r1g
+      job_id: jz57dy1r5
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.886129Z'
+    timestamp: '2024-05-20T16:35:31.615315Z'
+  - torchscript_onnx_qnn:
+      inference_time: 3991.0
+      throughput: 250.56376847907794
+      estimated_peak_memory_range:
+        min: 237568
+        max: 237568
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 21
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 21
+      job_id: jvgdvxmlg
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1578.0
+      throughput: 633.7135614702155
+      estimated_peak_memory_range:
+        min: 8957952
+        max: 8957952
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 23
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 23
+      job_id: jo5mzn1qp
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 3324.0
+      throughput: 300.84235860409143
+      estimated_peak_memory_range:
+        min: 16203776
+        max: 16203776
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 14
+        total_layers: 14
+      job_id: jegne6dmg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.615338Z'
```

## qai_hub_models/models/xlsr_quantized/export.py

```diff
@@ -119,20 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image" + " --force_channel_last_output output_0"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime,
-        compile_options
-        + " --force_channel_last_input image"
-        + " --force_channel_last_output output_0",
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -166,16 +170,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -195,24 +201,28 @@
         print_profile_metrics_from_job(profile_job, profile_data)
 
     if not skip_summary and not skip_inferencing:
         torch_out = torch_inference(model, sample_inputs)
         assert inference_job is not None and inference_job.wait().success
         inference_result: hub.client.DatasetEntries = inference_job.download_output_data()  # type: ignore
         # Convert outputs from channel last to channel first
-        inference_result = transpose_channel_last_to_first(
-            "output_0", inference_result, target_runtime
+        inference_result = (
+            inference_result
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_last_to_first(
+                "output_0", inference_result, target_runtime
+            )
         )
         print_inference_metrics(inference_job, inference_result, torch_out)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/xlsr_quantized/model.py

```diff
@@ -4,96 +4,76 @@
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 # isort: off
 # This verifies aimet is installed, and this must be included first.
 from qai_hub_models.utils.quantization_aimet import (
     AIMETQuantizableMixin,
+    constrain_quantized_inputs_to_image_range,
 )
 
 # isort: on
 
 import torch
+from aimet_torch.cross_layer_equalization import equalize_model
+from aimet_torch.model_preparer import prepare_model
 from aimet_torch.quantsim import QuantizationSimModel, load_encodings_to_sim
 
-from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
-from qai_hub_models.models.xlsr.model import XLSR, _load_xlsr_source_model
+from qai_hub_models.models.xlsr.model import XLSR
+from qai_hub_models.utils.aimet.config_loader import get_default_aimet_config
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 
 MODEL_ID = __name__.split(".")[-2]
-MODEL_ASSET_VERSION = 2
-# Weights and config stored in S3 are sourced from
-# https://github.com/quic/aimet-model-zoo/blob/develop/aimet_zoo_torch/xlsr/model/model_cards/xlsr_4x_w8a8.json:
-# https://github.com/quic/aimet-model-zoo/releases/download/phase_2_february_artifacts/xlsr_4x_checkpoint_int8.pth
-# and
-# https://raw.githubusercontent.com/quic/aimet/release-aimet-1.23/TrainingExtensions/common/src/python/aimet_common/quantsim_config/default_config_per_channel.js
-# Encodings were generated with AIMET QuantSim library
-XLSR_QUANTIZED_WEIGHTS = "xlsr_4x_checkpoint_int8.pth"
-AIMET_ENCODINGS = "aimet_quantization_encodings.json"
-AIMET_CONFIG = "default_config_per_channel.json"
+MODEL_ASSET_VERSION = 3
+DEFAULT_ENCODINGS = "xlsr_quantized_encodings.json"
 SCALING_FACTOR = 4
 
 
 class XLSRQuantizable(AIMETQuantizableMixin, XLSR):
     """XLSR with post training quantization suport
 
     Supports only 8 bit weights and activations, and only loads pre-quantized checkpoints.
     Support for quantizing using your own weights & data will come at a later date."""
 
     def __init__(
         self,
         xlsr_model: QuantizationSimModel,
     ) -> None:
         XLSR.__init__(self, xlsr_model.model)
-        AIMETQuantizableMixin.__init__(
-            self, xlsr_model, needs_onnx_direct_aimet_export=True
-        )
+        AIMETQuantizableMixin.__init__(self, xlsr_model)
 
     @classmethod
     def from_pretrained(
         cls,
         aimet_encodings: str | None = "DEFAULT",
     ) -> XLSRQuantizable:
         """
         Parameters:
           aimet_encodings:
             if "DEFAULT": Loads the model with aimet encodings calibrated on BSD300.
             elif None: Doesn't load any encodings. Used when computing encodings.
             else: Interprets as a filepath and loads the encodings stored there.
         """
-        xlsr = _load_xlsr_source_model()
-        input_shape = XLSR.get_input_spec()["image"][0]
+        fp16_model = XLSR.from_pretrained()
+        input_shape = cls.get_input_spec()["image"][0]
+
+        model = prepare_model(fp16_model)
+        equalize_model(model, input_shape)
 
-        weights = CachedWebModelAsset.from_asset_store(
-            MODEL_ID, MODEL_ASSET_VERSION, XLSR_QUANTIZED_WEIGHTS
-        ).fetch()
-        aimet_config = CachedWebModelAsset.from_asset_store(
-            MODEL_ID, MODEL_ASSET_VERSION, AIMET_CONFIG
-        ).fetch()
-
-        # Load the model weights and quantization parameters
-        state_dict = torch.load(weights, map_location=torch.device("cpu"))["state_dict"]
-        xlsr.load_state_dict(state_dict)
         sim = QuantizationSimModel(
-            xlsr,
+            model,
             quant_scheme="tf_enhanced",
             default_param_bw=8,
             default_output_bw=8,
-            config_file=aimet_config,
+            config_file=get_default_aimet_config(),
             dummy_input=torch.rand(input_shape),
         )
+        constrain_quantized_inputs_to_image_range(sim)
+
         if aimet_encodings:
             if aimet_encodings == "DEFAULT":
                 aimet_encodings = CachedWebModelAsset.from_asset_store(
-                    MODEL_ID, MODEL_ASSET_VERSION, AIMET_ENCODINGS
+                    MODEL_ID, MODEL_ASSET_VERSION, DEFAULT_ENCODINGS
                 ).fetch()
             load_encodings_to_sim(sim, aimet_encodings)
 
         return cls(sim)
-
-    def preferred_hub_source_model_format(
-        self, target_runtime: TargetRuntime
-    ) -> SourceModelFormat:
-        if target_runtime == TargetRuntime.QNN:
-            return SourceModelFormat.ONNX
-        else:
-            return SourceModelFormat.TORCHSCRIPT
```

## qai_hub_models/models/xlsr_quantized/perf.yaml

```diff
@@ -22,135 +22,280 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
   - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: XLSR-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 1128.0
-      throughput: 886.5248226950355
+      inference_time: 1142.0
+      throughput: 875.6567425569177
       estimated_peak_memory_range:
-        min: 12288
-        max: 1590504
+        min: 20480
+        max: 1494816
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 17
-      job_id: jmg9jdq85
+      job_id: jopryvmeg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 798.0
+      throughput: 1253.1328320802006
+      estimated_peak_memory_range:
+        min: 65536
+        max: 74050712
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 17
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 17
+      job_id: j2p0rz8ep
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1166.0
+      throughput: 857.6329331046312
+      estimated_peak_memory_range:
+        min: 12288
+        max: 10231824
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 21
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 21
+      job_id: j1glkv9lp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.910002Z'
+    timestamp: '2024-05-20T16:35:31.645635Z'
   - torchscript_onnx_tflite:
-      inference_time: 1209.0
-      throughput: 827.129859387924
+      inference_time: 948.0
+      throughput: 1054.8523206751054
       estimated_peak_memory_range:
-        min: 53248
-        max: 20193472
+        min: 16384
+        max: 20809824
       primary_compute_unit: NPU
       precision: int8
       layer_info:
         layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
         total_layers: 17
-      job_id: jnp1y6m7p
+      job_id: jep2mkqm5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 548.0
+      throughput: 1824.8175182481752
+      estimated_peak_memory_range:
+        min: 65536
+        max: 18623024
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 17
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 17
+      job_id: j1p87qd85
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 864.0
+      throughput: 1157.4074074074074
+      estimated_peak_memory_range:
+        min: 344064
+        max: 17534000
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 21
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 21
+      job_id: jw561y97p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.910020Z'
+    timestamp: '2024-05-20T16:35:31.645660Z'
   - torchscript_onnx_tflite:
-      inference_time: 3053.0
-      throughput: 327.54667540124467
+      inference_time: 1133.0
+      throughput: 882.61253309797
       estimated_peak_memory_range:
-        min: 57344
-        max: 15609680
+        min: 12288
+        max: 1909504
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 16
+        layers_on_npu: 14
         layers_on_gpu: 0
         layers_on_cpu: 3
-        total_layers: 19
-      job_id: jopr8r375
+        total_layers: 17
+      job_id: jqpyd1k4p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 793.0
+      throughput: 1261.034047919294
+      estimated_peak_memory_range:
+        min: 69632
+        max: 73885472
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 17
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 17
+      job_id: jn5q26xm5
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:31.645677Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2418.0
+      throughput: 413.564929693962
+      estimated_peak_memory_range:
+        min: 12288
+        max: 14878432
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 14
+        layers_on_gpu: 0
+        layers_on_cpu: 3
+        total_layers: 17
+      job_id: jqpy6yo75
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 1550.0
+      throughput: 645.1612903225806
+      estimated_peak_memory_range:
+        min: 65536
+        max: 17596976
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 17
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 17
+      job_id: jw56n080g
       job_status: Passed
     reference_device_info:
       name: RB3 Gen 2 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:33.910035Z'
+    timestamp: '2024-05-20T16:35:31.645694Z'
   - torchscript_onnx_tflite:
-      inference_time: 15998.0
-      throughput: 62.50781347668458
+      inference_time: 14145.0
+      throughput: 70.69635913750442
       estimated_peak_memory_range:
-        min: 45056
-        max: 17827664
+        min: 4235264
+        max: 15314136
       primary_compute_unit: GPU
       precision: int8
       layer_info:
-        layers_on_npu: 5
+        layers_on_npu: 3
         layers_on_gpu: 9
         layers_on_cpu: 5
-        total_layers: 19
-      job_id: jvgdq6vl5
+        total_layers: 17
+      job_id: j2p0lxm6p
       job_status: Passed
     reference_device_info:
       name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8250
-    timestamp: '2024-04-23T18:42:33.910050Z'
-  - torchscript_onnx_tflite:
-      inference_time: 1313.0
-      throughput: 761.6146230007616
+    timestamp: '2024-05-20T16:35:31.645705Z'
+  - torchscript_onnx_qnn:
+      inference_time: 933.0
+      throughput: 1071.8113612004288
       estimated_peak_memory_range:
-        min: 28672
-        max: 5004672
+        min: 49152
+        max: 49152
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 16
+        layers_on_npu: 17
         layers_on_gpu: 0
-        layers_on_cpu: 3
-        total_layers: 19
-      job_id: j2p03w0np
+        layers_on_cpu: 0
+        total_layers: 17
+      job_id: jogkyewop
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 1191.0
+      throughput: 839.6305625524769
+      estimated_peak_memory_range:
+        min: 8818688
+        max: 8818688
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 21
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 21
+      job_id: j1p3mjlzg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 11552.0
+      throughput: 86.56509695290859
+      estimated_peak_memory_range:
+        min: 33103872
+        max: 33103872
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jwgov27d5
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
-      os: '12'
-      form_factor: Iot
-      os_name: Android
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.910063Z'
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.645729Z'
```

## qai_hub_models/models/yolov6/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -195,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov6/model.py

```diff
@@ -1,24 +1,24 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
-import tempfile
 from importlib import reload
 
 import torch
 import torch.nn as nn
 
 from qai_hub_models.models._shared.yolo.utils import detect_postprocess
 from qai_hub_models.utils.asset_loaders import (
     CachedWebModelAsset,
     SourceAsRoot,
     load_path,
+    qaihm_temp_dir,
 )
 from qai_hub_models.utils.base_model import BaseModel
 from qai_hub_models.utils.input_spec import InputSpec
 
 YOLOV6_SOURCE_REPOSITORY = "https://github.com/meituan/YOLOv6"
 YOLOV6_SOURCE_REPO_COMMIT = "55d80c317edd0fb5847e599a1802d394f34a3141"
 MODEL_ASSET_VERSION = 1
@@ -89,15 +89,15 @@
         """
         return {"image": ((batch_size, num_channels, height, width), "float32")}
 
 
 def _load_yolov6_source_model_from_weights(
     ckpt_path: str | CachedWebModelAsset,
 ) -> torch.nn.Module:
-    with tempfile.TemporaryDirectory() as tmpdir:
+    with qaihm_temp_dir() as tmpdir:
         model_path = load_path(ckpt_path, tmpdir)
         with SourceAsRoot(
             YOLOV6_SOURCE_REPOSITORY,
             YOLOV6_SOURCE_REPO_COMMIT,
             MODEL_ID,
             MODEL_ASSET_VERSION,
         ):
```

## qai_hub_models/models/yolov6/perf.yaml

```diff
@@ -18,162 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Yolo-v6
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 7953.0
-      throughput: 125.7387149503332
+      inference_time: 7322.0
+      throughput: 136.5747063643813
       estimated_peak_memory_range:
-        min: 2138112
-        max: 5576840
+        min: 225280
+        max: 2559408
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 182
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 182
-      job_id: jvgde2mz5
+      job_id: jw561yx7p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 6885.0
-      throughput: 145.24328249818447
+      inference_time: 5353.0
+      throughput: 186.81113394358303
       estimated_peak_memory_range:
-        min: 4939776
-        max: 18625080
+        min: 4947968
+        max: 15312024
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 229
+        layers_on_npu: 228
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 229
-      job_id: jqp4k321g
+        total_layers: 228
+      job_id: j1pvw68mg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 6690.0
-      throughput: 149.47683109118086
+      inference_time: 6762.0
+      throughput: 147.88524105294292
       estimated_peak_memory_range:
-        min: 5345280
-        max: 37259592
+        min: 5337088
+        max: 34449840
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 228
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jo5mq8l9p
+        total_layers: 228
+      job_id: jz5w9ekjp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.930548Z'
+    timestamp: '2024-05-20T16:35:31.754074Z'
   - torchscript_onnx_tflite:
-      inference_time: 5649.0
-      throughput: 177.02248185519562
+      inference_time: 5305.0
+      throughput: 188.5014137606032
       estimated_peak_memory_range:
-        min: 16384
-        max: 82704608
+        min: 49152
+        max: 78708192
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 182
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 182
-      job_id: jz570989g
+      job_id: j1p3mjdzg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 4867.0
-      throughput: 205.4653790836244
+      inference_time: 3962.0
+      throughput: 252.39777889954567
       estimated_peak_memory_range:
         min: 4931584
-        max: 98473200
+        max: 96111232
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 229
+        layers_on_npu: 228
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 229
-      job_id: j0pxnxzl5
+        total_layers: 228
+      job_id: j7gjlv98p
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 4842.0
-      throughput: 206.52622883106156
+      inference_time: 4919.0
+      throughput: 203.29335230737954
       estimated_peak_memory_range:
-        min: 4931584
-        max: 66299664
+        min: 3256320
+        max: 65642736
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 228
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jegnlkwq5
+        total_layers: 228
+      job_id: jmg94lrv5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.930616Z'
+    timestamp: '2024-05-20T16:35:31.754099Z'
   - torchscript_onnx_tflite:
-      inference_time: 7952.0
-      throughput: 125.75452716297787
+      inference_time: 7402.0
+      throughput: 135.09862199405566
       estimated_peak_memory_range:
-        min: 217088
-        max: 3444928
+        min: 229376
+        max: 3627424
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 182
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 182
-      job_id: jn5qenqo5
+      job_id: jwgov2xd5
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 6878.0
-      throughput: 145.39110206455365
+      inference_time: 5362.0
+      throughput: 186.4975755315181
       estimated_peak_memory_range:
-        min: 4952064
-        max: 19343808
+        min: 4939776
+        max: 15305728
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 229
+        layers_on_npu: 228
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 229
-      job_id: j7gjz9ve5
+        total_layers: 228
+      job_id: jygz7366p
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.930670Z'
+    timestamp: '2024-05-20T16:35:31.754129Z'
+  - torchscript_onnx_qnn:
+      inference_time: 6754.0
+      throughput: 148.06040864672786
+      estimated_peak_memory_range:
+        min: 4923392
+        max: 4923392
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 228
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 228
+      job_id: jlpevdq05
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 6563.0
+      throughput: 152.36934328813044
+      estimated_peak_memory_range:
+        min: 7618560
+        max: 7618560
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 228
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 228
+      job_id: jnp1849lg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 17525.0
+      throughput: 57.06134094151213
+      estimated_peak_memory_range:
+        min: 35479552
+        max: 35479552
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 149
+        total_layers: 149
+      job_id: jvgdvxklg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.754151Z'
```

## qai_hub_models/models/yolov7/app.py

```diff
@@ -12,15 +12,17 @@
 from qai_hub_models.models._shared.yolo.utils import detect_postprocess
 from qai_hub_models.models.yolov7.model import YoloV7
 
 
 class YoloV7DetectionApp(YoloObjectDetectionApp):
     def check_image_size(self, pixel_values: torch.Tensor) -> None:
         """
-        Verify image size is valid model input.
+        Verify image size is a valid model input. Image size should be shape
+        [batch_size, num_channels, height, width], where height and width are multiples
+        of `YoloNAS.STRIDE_MULTIPLE`.
         """
         if len(pixel_values.shape) != 4:
             raise ValueError("Pixel Values must be rank 4: [batch, channels, x, y]")
         if (
             pixel_values.shape[2] % YoloV7.STRIDE_MULTIPLE != 0
             or pixel_values.shape[3] % YoloV7.STRIDE_MULTIPLE != 0
         ):
```

## qai_hub_models/models/yolov7/export.py

```diff
@@ -115,17 +115,24 @@
         **get_input_spec_kwargs(model, additional_model_kwargs)
     )
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(model.to("cpu"), make_torch_inputs(input_spec))
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -155,16 +162,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -195,14 +204,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov7/model.py

```diff
@@ -45,15 +45,15 @@
         self.split_output = split_output
         self.class_dtype = class_dtype
 
     # All image input spatial dimensions should be a multiple of this stride.
     STRIDE_MULTIPLE = 32
 
     def get_evaluator(self) -> BaseEvaluator:
-        return DetectionEvaluator(640, 640)
+        return DetectionEvaluator(*self.get_input_spec()["image"][0][2:])
 
     @classmethod
     def from_pretrained(
         cls,
         weights_name: Optional[str] = DEFAULT_WEIGHTS,
         include_postprocessing: bool = True,
         split_output: bool = False,
@@ -92,15 +92,15 @@
             image: Pixel values pre-processed for encoder consumption.
                    Range: float[0, 1]
                    3-channel Color Space: BGR
 
         Returns:
             If self.include_postprocessing:
                 boxes: torch.Tensor
-                    Bounding box locations.  Shape [batch, num preds, 4] where 4 == (center_x, center_y, w, h)
+                    Bounding box locations.  Shape [batch, num preds, 4] where 4 == (left_x, top_y, right_x, bottom_y)
                 scores: torch.Tensor
                     class scores multiplied by confidence: Shape is [batch, num_preds]
                 class_idx: torch.tensor
                     Shape is [batch, num_preds] where the last dim is the index of the most probable class of the prediction.
 
             else if self.split_output:
                 output_xy: torch.Tensor
```

## qai_hub_models/models/yolov7/perf.yaml

```diff
@@ -18,117 +18,202 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Yolo-v7
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 20875.0
-      throughput: 47.90419161676647
+      inference_time: 15991.0
+      throughput: 62.53517603652054
       estimated_peak_memory_range:
-        min: 9580544
-        max: 45193728
+        min: 1212416
+        max: 3555464
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 292
+        layers_on_npu: 203
         layers_on_gpu: 0
-        layers_on_cpu: 21
-        total_layers: 313
-      job_id: jep20ezqg
+        layers_on_cpu: 12
+        total_layers: 215
+      job_id: jz5w9ej6p
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jvgdvxjeg
+      job_status: Failed
     torchscript_onnx_ort:
-      inference_time: 22899.0
-      throughput: 43.670029258919605
+      inference_time: 13667.0
+      throughput: 73.16894709885125
       estimated_peak_memory_range:
-        min: 9625600
-        max: 55617832
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 2
-        layers_on_gpu: 0
-        layers_on_cpu: 21
-        total_layers: 23
-      job_id: j2p036xnp
+        min: 6905856
+        max: 39411600
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 213
+        layers_on_gpu: 0
+        layers_on_cpu: 12
+        total_layers: 225
+      job_id: j0px1kw1g
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.954673Z'
+    timestamp: '2024-05-20T16:35:31.784368Z'
   - torchscript_onnx_tflite:
-      inference_time: 16244.0
-      throughput: 61.56119182467373
+      inference_time: 10824.0
+      throughput: 92.38728750923873
       estimated_peak_memory_range:
-        min: 40960
-        max: 202538080
+        min: 188416
+        max: 59790128
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 292
+        layers_on_npu: 203
         layers_on_gpu: 0
-        layers_on_cpu: 21
-        total_layers: 313
-      job_id: jqpyrmyl5
+        layers_on_cpu: 12
+        total_layers: 215
+      job_id: jmg94l6l5
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jz57dyql5
+      job_status: Failed
     torchscript_onnx_ort:
-      inference_time: 18014.0
-      throughput: 55.51237926057511
+      inference_time: 9691.0
+      throughput: 103.18852543597151
       estimated_peak_memory_range:
-        min: 17952768
-        max: 200617376
-      primary_compute_unit: CPU
-      precision: fp32
-      layer_info:
-        layers_on_npu: 2
-        layers_on_gpu: 0
-        layers_on_cpu: 21
-        total_layers: 23
-      job_id: j1p801kog
+        min: 6680576
+        max: 67183456
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 213
+        layers_on_gpu: 0
+        layers_on_cpu: 12
+        total_layers: 225
+      job_id: jo5mznjwp
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.954727Z'
+    timestamp: '2024-05-20T16:35:31.784396Z'
   - torchscript_onnx_tflite:
-      inference_time: 20857.0
-      throughput: 47.94553387351968
+      inference_time: 15945.0
+      throughput: 62.715584822828475
       estimated_peak_memory_range:
-        min: 9539584
-        max: 12396608
+        min: 1220608
+        max: 3533376
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 292
+        layers_on_npu: 203
         layers_on_gpu: 0
-        layers_on_cpu: 21
-        total_layers: 313
-      job_id: jvgdekxz5
+        layers_on_cpu: 12
+        total_layers: 215
+      job_id: jnp184r2g
       job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jqp4wlzvg
+      job_status: Failed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.954767Z'
+    timestamp: '2024-05-20T16:35:31.784414Z'
+  - torchscript_onnx_ort:
+      inference_time: 13497.0
+      throughput: 74.0905386382159
+      estimated_peak_memory_range:
+        min: 4927488
+        max: 4927488
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 213
+        layers_on_gpu: 0
+        layers_on_cpu: 12
+        total_layers: 225
+      job_id: jegne6jrg
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 38053.0
+      throughput: 26.279136993141144
+      estimated_peak_memory_range:
+        min: 150495232
+        max: 150495232
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jopryvz9g
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.784434Z'
```

## qai_hub_models/models/yolov7/requirements.txt

```diff
@@ -1,4 +1,5 @@
 matplotlib==3.7.4
 object-detection-metrics==0.4.post1
 scipy==1.8.1
 seaborn==0.11.0
+shapely==2.0.3
```

## qai_hub_models/models/yolov7_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
```

## qai_hub_models/models/yolov7_quantized/info.yaml

```diff
@@ -2,15 +2,15 @@
 # id must match with the model dir name in qai_hub_models
 id: yolov7_quantized
 status: public
 headline: Quantized real-time object detection optimized for mobile and edge.
 domain: Computer Vision
 description: YoloV7 is a machine learning model that predicts bounding boxes and classes
   of objects in an image. This model is post-training quantized to int8 using samples
-  from the [COCO dataset](https://cocodataset.org/#home).
+  from the COCO dataset.
 use_case: Object Detection
 tags:
   - real-time
   - quantized
 research_paper: https://arxiv.org/abs/2207.02696
 research_paper_title: 'YOLOv7: Trainable bag-of-freebies sets new state-of-the-art
   for real-time object detectors'
```

## qai_hub_models/models/yolov7_quantized/perf.yaml

```diff
@@ -5,156 +5,282 @@
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
   - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
   - QCS8550 (Proxy)
   - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs6490
+  - Qcs8250
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: Yolo-v7-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 6122.0
-      throughput: 163.3453119895459
+      inference_time: 4575.0
+      throughput: 218.5792349726776
       estimated_peak_memory_range:
-        min: 278528
-        max: 13519408
+        min: 323584
+        max: 2051176
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 224
+        layers_on_npu: 225
         layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 224
-      job_id: j1gl6j32g
+        layers_on_cpu: 1
+        total_layers: 226
+      job_id: jep2mk245
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 5732.0
-      throughput: 174.45917655268667
+      inference_time: 'null'
+      throughput: 'null'
       estimated_peak_memory_range:
-        min: 16384
-        max: 12543776
-      primary_compute_unit: NPU
-      precision: int8
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
       layer_info:
-        layers_on_npu: 219
+        layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 219
-      job_id: j1p80l0og
-      job_status: Passed
+        total_layers: 0
+      job_id: j1p87qlx5
+      job_status: Failed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1glkvj8p
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:33.972519Z'
+    timestamp: '2024-05-20T16:35:31.813972Z'
   - torchscript_onnx_tflite:
-      inference_time: 4059.0
-      throughput: 246.3661000246366
+      inference_time: 2984.0
+      throughput: 335.1206434316354
       estimated_peak_memory_range:
         min: 40960
-        max: 67566064
+        max: 60470096
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 224
+        layers_on_npu: 225
         layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 224
-      job_id: jogk7jyvp
+        layers_on_cpu: 1
+        total_layers: 226
+      job_id: jqpyd197p
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 3804.0
-      throughput: 262.88117770767616
+      inference_time: 'null'
+      throughput: 'null'
       estimated_peak_memory_range:
-        min: 1245184
-        max: 89862128
-      primary_compute_unit: NPU
-      precision: int8
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
       layer_info:
-        layers_on_npu: 219
+        layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 219
-      job_id: jmg9j64m5
-      job_status: Passed
+        total_layers: 0
+      job_id: jogkyej2p
+      job_status: Failed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jw561yk0p
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:33.972581Z'
-  - torchscript_onnx_qnn:
+    timestamp: '2024-05-20T16:35:31.813999Z'
+  - torchscript_onnx_tflite:
+      inference_time: 4604.0
+      throughput: 217.2024326672459
+      estimated_peak_memory_range:
+        min: 282624
+        max: 2513496
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 225
+        layers_on_gpu: 0
+        layers_on_cpu: 1
+        total_layers: 226
+      job_id: j2p0rzn6p
+      job_status: Passed
+    torchscript_onnx_qnn:
       inference_time: 'null'
       throughput: 'null'
       estimated_peak_memory_range:
         min: 0
         max: 0
       primary_compute_unit: 'null'
       precision: 'null'
       layer_info:
         layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 0
-      job_id: j1p80nqog
+      job_id: jn5q26j45
       job_status: Failed
     reference_device_info:
-      name: RB3 Gen 2 (Proxy)
+      name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs6490
-    timestamp: '2024-04-23T18:42:33.972595Z'
-  - torchscript_onnx_qnn:
-      inference_time: 5978.0
-      throughput: 167.2800267648043
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:31.814017Z'
+  - torchscript_onnx_tflite:
+      inference_time: 11128.0
+      throughput: 89.86340762041696
       estimated_peak_memory_range:
-        min: 4939776
-        max: 15407880
+        min: 262144
+        max: 60976880
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 220
+        layers_on_npu: 225
+        layers_on_gpu: 0
+        layers_on_cpu: 1
+        total_layers: 226
+      job_id: jn5q3d1np
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 220
-      job_id: j2p03wznp
+        total_layers: 0
+      job_id: j7gje88v5
+      job_status: Failed
+    reference_device_info:
+      name: RB3 Gen 2 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs6490
+    timestamp: '2024-05-20T16:35:31.814035Z'
+  - torchscript_onnx_tflite:
+      inference_time: 86803.0
+      throughput: 11.520339158784834
+      estimated_peak_memory_range:
+        min: 4190208
+        max: 40909296
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 32
+        layers_on_gpu: 126
+        layers_on_cpu: 68
+        total_layers: 226
+      job_id: j1gl3q8jg
       job_status: Passed
     reference_device_info:
-      name: QCS8550 (Proxy)
+      name: RB5 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
-      chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:33.972626Z'
+      chipset: Qcs8250
+    timestamp: '2024-05-20T16:35:31.814046Z'
+  - torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1p3mjylg
+      job_status: Failed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 15343.0
+      throughput: 65.17630189663039
+      estimated_peak_memory_range:
+        min: 51806208
+        max: 51806208
+      primary_compute_unit: CPU
+      precision: fp32
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 256
+        total_layers: 256
+      job_id: jwgov2jx5
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.814068Z'
```

## qai_hub_models/models/yolov7_quantized/requirements.txt

```diff
@@ -1,4 +1,5 @@
 matplotlib==3.7.4
 object-detection-metrics==0.4.post1
 scipy==1.8.1
 seaborn==0.11.0
+shapely==2.0.3
```

## qai_hub_models/models/yolov8_det/export.py

```diff
@@ -117,17 +117,24 @@
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(
         model.to("cpu"), make_torch_inputs(input_spec), check_trace=False
     )
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -157,16 +164,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -197,14 +206,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov8_det/model.py

```diff
@@ -156,15 +156,15 @@
         """
         Returns the input specification (name -> (shape, type). This can be
         used to submit profiling job on Qualcomm AI Hub.
         """
         return {"image": ((batch_size, num_channels, height, width), "float32")}
 
     def get_evaluator(self) -> BaseEvaluator:
-        return DetectionEvaluator(640, 640)
+        return DetectionEvaluator(*self.get_input_spec()["image"][0][2:])
 
 
 def yolov8_detect_postprocess(
     boxes: torch.Tensor,
     scores: torch.Tensor,
     use_quantized_postprocessing: bool = False,
 ):
```

## qai_hub_models/models/yolov8_det/perf.yaml

```diff
@@ -4,106 +4,231 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
+  - QCS8550 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
+  - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: YOLOv8-Detection
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 6113.0
-      throughput: 163.5858007524947
+      inference_time: 5873.0
+      throughput: 170.2707304614337
       estimated_peak_memory_range:
-        min: 233472
-        max: 8968336
+        min: 245760
+        max: 8436704
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 300
+        layers_on_npu: 290
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 300
-      job_id: jqpyzm28g
+        total_layers: 290
+      job_id: j1pvw6jjg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 5218.0
+      throughput: 191.64430816404752
+      estimated_peak_memory_range:
+        min: 6332416
+        max: 18723960
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 285
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 285
+      job_id: jygz731kp
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 6644.0
+      throughput: 150.51173991571343
+      estimated_peak_memory_range:
+        min: 6328320
+        max: 32755768
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 286
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 286
+      job_id: jvgdvxweg
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-02T15:27:43.907101Z'
+    timestamp: '2024-05-20T16:35:31.892347Z'
+  - torchscript_onnx_tflite:
+      inference_time: 4141.0
+      throughput: 241.48756339048538
+      estimated_peak_memory_range:
+        min: 36864
+        max: 84965392
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 290
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 290
+      job_id: j7gjlvjxp
+      job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 5316.0
-      throughput: 188.11136192626034
+      inference_time: 3671.0
+      throughput: 272.40533914464726
       estimated_peak_memory_range:
-        min: 4935680
-        max: 19108344
+        min: 78393344
+        max: 180541088
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 293
+        layers_on_npu: 285
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 293
-      job_id: j1p821rkp
+        total_layers: 285
+      job_id: jz5w9eo6p
       job_status: Passed
-  - torchscript_onnx_tflite:
-      inference_time: 4320.0
-      throughput: 231.4814814814815
+    torchscript_onnx_ort:
+      inference_time: 4354.0
+      throughput: 229.67386311437758
       estimated_peak_memory_range:
-        min: 73728
-        max: 88723920
+        min: 4956160
+        max: 70229504
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 300
+        layers_on_npu: 286
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 300
-      job_id: j2p04699g
+        total_layers: 286
+      job_id: jz57dyzl5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-02T15:30:24.719256Z'
+    timestamp: '2024-05-20T16:35:31.892377Z'
+  - torchscript_onnx_tflite:
+      inference_time: 5872.0
+      throughput: 170.29972752043597
+      estimated_peak_memory_range:
+        min: 16384
+        max: 5135584
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 290
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 290
+      job_id: jlpevdj15
+      job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 3677.0
-      throughput: 271.9608376393799
+      inference_time: 5208.0
+      throughput: 192.01228878648234
+      estimated_peak_memory_range:
+        min: 4935680
+        max: 18248224
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 285
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 285
+      job_id: jnp18402g
+      job_status: Passed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:31.892393Z'
+  - torchscript_onnx_qnn:
+      inference_time: 5820.0
+      throughput: 171.82130584192439
+      estimated_peak_memory_range:
+        min: 4923392
+        max: 4923392
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 285
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 285
+      job_id: jmg94lvl5
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 6424.0
+      throughput: 155.6662515566625
       estimated_peak_memory_range:
-        min: 4931584
-        max: 110753456
+        min: 10039296
+        max: 10039296
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 293
+        layers_on_npu: 286
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 293
-      job_id: jogkv80wp
+        total_layers: 286
+      job_id: jqp4wlqvg
       job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 14327.0
+      throughput: 69.79828296223913
+      estimated_peak_memory_range:
+        min: 82149376
+        max: 82149376
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j0px1kv1g
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.892415Z'
```

## qai_hub_models/models/yolov8_det/requirements.txt

```diff
@@ -1,4 +1,5 @@
 object-detection-metrics==0.4.post1
 seaborn==0.11.0
 thop==0.1.1.post2209072238
 ultralytics==8.0.193
+shapely==2.0.3
```

## qai_hub_models/models/yolov8_det_quantized/export.py

```diff
@@ -119,17 +119,24 @@
         target_runtime, output_path, input_spec
     )
     if target_runtime == TargetRuntime.TFLITE:
         quant_calibration_data = None
     else:
         quant_calibration_data = model.get_calibration_data(target_runtime, input_spec)
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -163,16 +170,18 @@
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         hub_inputs = sample_inputs
         if target_runtime == TargetRuntime.QNN:
             hub_inputs = get_qnn_inputs(compile_job, sample_inputs)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
```

## qai_hub_models/models/yolov8_det_quantized/info.yaml

```diff
@@ -3,15 +3,15 @@
 id: yolov8_det_quantized
 status: public
 headline: Quantized real-time object detection optimized for mobile and edge by Ultralytics.
 domain: Computer Vision
 use_case: Object Detection
 description: Ultralytics YOLOv8 is a machine learning model that predicts bounding
   boxes and classes of objects in an image. This model is post-training quantized
-  to int8 using samples from the [COCO dataset](https://cocodataset.org/#home).
+  to int8 using samples from the COCO dataset.
 tags:
   - real-time
   - quantized
 research_paper: https://docs.ultralytics.com/tasks/detect/
 research_paper_title: 'Ultralytics YOLOv8 Docs: Object Detection'
 license: https://github.com/ultralytics/ultralytics/blob/main/LICENSE
 deploy_license: https://github.com/ultralytics/ultralytics/blob/main/LICENSE
```

## qai_hub_models/models/yolov8_det_quantized/perf.yaml

```diff
@@ -4,106 +4,283 @@
   supported_devices:
   - Google Pixel 3
   - Google Pixel 3a
   - Google Pixel 3a XL
   - Google Pixel 4
   - Google Pixel 4a
   - Google Pixel 5a 5G
+  - QCS6490 (Proxy)
+  - QCS8250 (Proxy)
+  - QCS8550 (Proxy)
+  - RB3 Gen 2 (Proxy)
+  - RB5 (Proxy)
   - Samsung Galaxy S21
   - Samsung Galaxy S21 Ultra
   - Samsung Galaxy S21+
   - Samsung Galaxy S22 5G
   - Samsung Galaxy S22 Ultra 5G
   - Samsung Galaxy S22+ 5G
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
+  - Qcs6490
+  - Qcs8250
+  - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: YOLOv8-Detection-Quantized
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 2122.0
-      throughput: 471.25353440150803
+      inference_time: 2343.0
+      throughput: 426.8032437046522
       estimated_peak_memory_range:
         min: 12288
-        max: 2262728
+        max: 2559336
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 274
+        layers_on_npu: 276
         layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 274
-      job_id: jwgokj31p
+        layers_on_cpu: 1
+        total_layers: 277
+      job_id: jo5mznrwp
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 2121.0
-      throughput: 471.4757190004715
+      inference_time: 'null'
+      throughput: 'null'
       estimated_peak_memory_range:
-        min: 1249280
-        max: 12007368
-      primary_compute_unit: NPU
-      precision: int8
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
       layer_info:
-        layers_on_npu: 272
+        layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 272
-      job_id: jnp1yrwnp
-      job_status: Passed
+        total_layers: 0
+      job_id: jep2mk845
+      job_status: Failed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1p87qox5
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:34.010775Z'
+    timestamp: '2024-05-20T16:35:31.922821Z'
   - torchscript_onnx_tflite:
-      inference_time: 1422.0
-      throughput: 703.2348804500704
+      inference_time: 1587.0
+      throughput: 630.119722747322
       estimated_peak_memory_range:
         min: 12288
-        max: 49561728
+        max: 49417568
       primary_compute_unit: NPU
       precision: int8
       layer_info:
-        layers_on_npu: 274
+        layers_on_npu: 276
         layers_on_gpu: 0
-        layers_on_cpu: 0
-        total_layers: 274
-      job_id: j1gl6jk2g
+        layers_on_cpu: 1
+        total_layers: 277
+      job_id: jegne62rg
       job_status: Passed
     torchscript_onnx_qnn:
-      inference_time: 1420.0
-      throughput: 704.2253521126761
+      inference_time: 'null'
+      throughput: 'null'
       estimated_peak_memory_range:
-        min: 1245184
-        max: 107412320
-      primary_compute_unit: NPU
-      precision: int8
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
       layer_info:
-        layers_on_npu: 272
+        layers_on_npu: 0
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 272
-      job_id: jvgdejv65
-      job_status: Passed
+        total_layers: 0
+      job_id: jqpyd1e7p
+      job_status: Failed
+    torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jogkyez2p
+      job_status: Failed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:34.010850Z'
+    timestamp: '2024-05-20T16:35:31.922851Z'
+  - torchscript_onnx_tflite:
+      inference_time: 2345.0
+      throughput: 426.43923240938165
+      estimated_peak_memory_range:
+        min: 12288
+        max: 3644216
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 276
+        layers_on_gpu: 0
+        layers_on_cpu: 1
+        total_layers: 277
+      job_id: jopryvk9g
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j2p0rzy6p
+      job_status: Failed
+    reference_device_info:
+      name: QCS8550 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8550
+    timestamp: '2024-05-20T16:35:31.922869Z'
+  - torchscript_onnx_tflite:
+      inference_time: 5342.0
+      throughput: 187.19580681392736
+      estimated_peak_memory_range:
+        min: 12288
+        max: 37726400
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 274
+        layers_on_gpu: 0
+        layers_on_cpu: 1
+        total_layers: 275
+      job_id: jogk3kkw5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: j1pvvnykp
+      job_status: Failed
+    reference_device_info:
+      name: RB3 Gen 2 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs6490
+    timestamp: '2024-05-20T16:35:31.922886Z'
+  - torchscript_onnx_tflite:
+      inference_time: 44633.0
+      throughput: 22.404947012300315
+      estimated_peak_memory_range:
+        min: 3031040
+        max: 12276104
+      primary_compute_unit: NPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 273
+        layers_on_gpu: 1
+        layers_on_cpu: 1
+        total_layers: 275
+      job_id: jn5q3ddnp
+      job_status: Passed
+    reference_device_info:
+      name: RB5 (Proxy)
+      os: '12'
+      form_factor: Iot
+      os_name: Android
+      manufacturer: Qualcomm
+      chipset: Qcs8250
+    timestamp: '2024-05-20T16:35:31.922898Z'
+  - torchscript_onnx_ort:
+      inference_time: 'null'
+      throughput: 'null'
+      estimated_peak_memory_range:
+        min: 0
+        max: 0
+      primary_compute_unit: 'null'
+      precision: 'null'
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 0
+      job_id: jn5q26845
+      job_status: Failed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 63514.0
+      throughput: 15.744560254432093
+      estimated_peak_memory_range:
+        min: 82382848
+        max: 82382848
+      primary_compute_unit: GPU
+      precision: int8
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: j1glkvn8p
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.922917Z'
```

## qai_hub_models/models/yolov8_det_quantized/requirements.txt

```diff
@@ -1,4 +1,5 @@
 object-detection-metrics==0.4.post1
 seaborn==0.11.0
 thop==0.1.1.post2209072238
 ultralytics==8.0.193
+shapely==2.0.3
```

## qai_hub_models/models/yolov8_seg/export.py

```diff
@@ -117,17 +117,24 @@
 
     # Trace the model
     model.eval()
     source_model = torch.jit.trace(
         model.to("cpu"), make_torch_inputs(input_spec), check_trace=False
     )
 
+    # Convert outputs from channel last to channel first (preferred I/O format for QNN and TensorFlow Lite)
+    channel_last_flags = (
+        " --force_channel_last_input image"
+        if target_runtime != TargetRuntime.ORT
+        else ""
+    )
+
     # 2. Compile the model to an on-device asset
     model_compile_options = model.get_hub_compile_options(
-        target_runtime, compile_options + " --force_channel_last_input image"
+        target_runtime, compile_options + channel_last_flags, hub_device
     )
     print(f"Optimizing model {model_name} to run on-device")
     submitted_compile_job = hub.submit_compile_job(
         model=source_model,
         input_specs=input_spec,
         device=hub_device,
         name=model_name,
@@ -157,16 +164,18 @@
             target_runtime, profile_options
         )
         print(
             f"Running inference for {model_name} on a hosted device with example inputs."
         )
         sample_inputs = model.sample_inputs(input_spec)
         # Convert inputs from channel first to channel last
-        hub_inputs = transpose_channel_first_to_last(
-            "image", sample_inputs, target_runtime
+        hub_inputs = (
+            sample_inputs
+            if target_runtime == TargetRuntime.ORT
+            else transpose_channel_first_to_last("image", sample_inputs, target_runtime)
         )
         submitted_inference_job = hub.submit_inference_job(
             model=compile_job.get_target_model(),
             inputs=hub_inputs,
             device=hub_device,
             name=model_name,
             options=profile_options_all,
@@ -197,14 +206,14 @@
         print_on_target_demo_cmd(compile_job, Path(__file__).parent.resolve(), device)
 
     return (compile_job, profile_job, inference_job)
 
 
 def main():
     warnings.filterwarnings("ignore")
-    parser = export_parser(model_cls=Model, supports_qnn=False, supports_ort=False)
+    parser = export_parser(model_cls=Model, supports_qnn=False)
     args = parser.parse_args()
     export_model(**vars(args))
 
 
 if __name__ == "__main__":
     main()
```

## qai_hub_models/models/yolov8_seg/info.yaml

```diff
@@ -33,11 +33,11 @@
   - ddrnet23_slim
 form_factors:
   - Phone
   - Tablet
   - IoT
   - XR
 has_static_banner: yes
-has_animated_banner: no
+has_animated_banner: yes
 license_type: agpl-3.0
 deploy_license_type: agpl-3.0
 dataset: []
```

## qai_hub_models/models/yolov8_seg/perf.yaml

```diff
@@ -18,117 +18,217 @@
   - Samsung Galaxy S23
   - Samsung Galaxy S23 Ultra
   - Samsung Galaxy S23+
   - Samsung Galaxy S24
   - Samsung Galaxy S24 Ultra
   - Samsung Galaxy S24+
   - Samsung Galaxy Tab S8
+  - Snapdragon X Elite CRD
   - Xiaomi 12
   - Xiaomi 12 Pro
   supported_chipsets:
   - Qcs8550
   - Snapdragon 8 Gen 1
   - Snapdragon 8 Gen 2
   - Snapdragon 8 Gen 3
   - Snapdragon 888
+  - Snapdragon X Elite
 models:
 - name: YOLOv8-Segmentation
   performance_metrics:
   - torchscript_onnx_tflite:
-      inference_time: 7033.0
-      throughput: 142.18683349921798
+      inference_time: 7377.0
+      throughput: 135.556459265284
       estimated_peak_memory_range:
-        min: 4595712
-        max: 6959144
+        min: 4571136
+        max: 14729800
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 337
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 337
-      job_id: jqp4k361g
+      job_id: jw561y60p
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 6398.0
+      throughput: 156.29884338855894
+      estimated_peak_memory_range:
+        min: 6324224
+        max: 17126264
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 333
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 333
+      job_id: j1pvw63jg
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 8072.0
-      throughput: 123.8850346878097
+      inference_time: 8007.0
+      throughput: 124.89072061945798
       estimated_peak_memory_range:
-        min: 15532032
-        max: 36380192
+        min: 14934016
+        max: 41914744
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 336
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jo5mq819p
+        total_layers: 336
+      job_id: jz5w9ev6p
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S23
       os: '13'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 2
-    timestamp: '2024-04-23T18:42:34.024849Z'
+    timestamp: '2024-05-20T16:35:31.961611Z'
   - torchscript_onnx_tflite:
-      inference_time: 5210.0
-      throughput: 191.93857965451056
+      inference_time: 5365.0
+      throughput: 186.39328984156572
       estimated_peak_memory_range:
-        min: 40960
-        max: 98992992
+        min: 16384
+        max: 95805104
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 337
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 337
-      job_id: j0pxnx8l5
+      job_id: j1p3mjklg
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 4560.0
+      throughput: 219.2982456140351
+      estimated_peak_memory_range:
+        min: 4931584
+        max: 119239328
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 333
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 333
+      job_id: j7gjlvxxp
       job_status: Passed
     torchscript_onnx_ort:
-      inference_time: 5653.0
-      throughput: 176.89722271360338
+      inference_time: 5499.0
+      throughput: 181.8512456810329
       estimated_peak_memory_range:
-        min: 17702912
-        max: 83989088
+        min: 16408576
+        max: 80100880
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
-        layers_on_npu: 1
+        layers_on_npu: 336
         layers_on_gpu: 0
         layers_on_cpu: 0
-        total_layers: 1
-      job_id: jegnlkdq5
+        total_layers: 336
+      job_id: jmg94l1l5
       job_status: Passed
     reference_device_info:
       name: Samsung Galaxy S24
       os: '14'
       form_factor: Phone
       os_name: Android
       manufacturer: Samsung
       chipset: Snapdragon 8 Gen 3
-    timestamp: '2024-04-23T18:42:34.024902Z'
+    timestamp: '2024-05-20T16:35:31.961638Z'
   - torchscript_onnx_tflite:
-      inference_time: 7217.0
-      throughput: 138.56172925038103
+      inference_time: 7372.0
+      throughput: 135.6483993488877
       estimated_peak_memory_range:
         min: 4579328
-        max: 18295080
+        max: 7772616
       primary_compute_unit: NPU
       precision: fp16
       layer_info:
         layers_on_npu: 337
         layers_on_gpu: 0
         layers_on_cpu: 0
         total_layers: 337
-      job_id: j0pxnq9l5
+      job_id: jwgov2yx5
+      job_status: Passed
+    torchscript_onnx_qnn:
+      inference_time: 6402.0
+      throughput: 156.20118712902217
+      estimated_peak_memory_range:
+        min: 4939776
+        max: 15507456
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 333
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 333
+      job_id: jygz73ekp
       job_status: Passed
     reference_device_info:
       name: QCS8550 (Proxy)
       os: '12'
       form_factor: Iot
       os_name: Android
       manufacturer: Qualcomm
       chipset: Qcs8550
-    timestamp: '2024-04-23T18:42:34.024944Z'
+    timestamp: '2024-05-20T16:35:31.961654Z'
+  - torchscript_onnx_qnn:
+      inference_time: 7604.0
+      throughput: 131.5097317201473
+      estimated_peak_memory_range:
+        min: 4923392
+        max: 4923392
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 333
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 333
+      job_id: jlpevd915
+      job_status: Passed
+    torchscript_onnx_ort:
+      inference_time: 8070.0
+      throughput: 123.91573729863693
+      estimated_peak_memory_range:
+        min: 22331392
+        max: 22331392
+      primary_compute_unit: NPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 336
+        layers_on_gpu: 0
+        layers_on_cpu: 0
+        total_layers: 336
+      job_id: jnp184l2g
+      job_status: Passed
+    torchscript_onnx_ort_dml_gpu:
+      inference_time: 22496.0
+      throughput: 44.45234708392603
+      estimated_peak_memory_range:
+        min: 104538112
+        max: 104538112
+      primary_compute_unit: GPU
+      precision: fp16
+      layer_info:
+        layers_on_npu: 0
+        layers_on_gpu: 1
+        layers_on_cpu: 0
+        total_layers: 1
+      job_id: jvgdvx9eg
+      job_status: Passed
+    reference_device_info:
+      name: Snapdragon X Elite CRD
+      os: '11'
+      form_factor: Compute
+      os_name: Windows
+      manufacturer: Qualcomm
+      chipset: Snapdragon X Elite
+    timestamp: '2024-05-20T16:35:31.961681Z'
```

## qai_hub_models/utils/args.py

```diff
@@ -52,14 +52,25 @@
         type=str,
         default=None,
         help="If specified, saves demo output (e.g. image) to this directory instead of displaying.",
     )
     return parser
 
 
+def _get_default_runtime(available_runtimes: List[TargetRuntime]):
+    if len(available_runtimes) == 0:
+        raise RuntimeError("available_runtimes empty, expecting at-least one runtime.")
+
+    return (
+        TargetRuntime.TFLITE
+        if TargetRuntime.TFLITE in available_runtimes
+        else available_runtimes[0]
+    )
+
+
 def add_target_runtime_arg(
     parser: argparse.ArgumentParser,
     help: str,
     available_target_runtimes: List[TargetRuntime] = list(
         TargetRuntime.__members__.values()
     ),
     default: TargetRuntime = TargetRuntime.TFLITE,
@@ -112,19 +123,15 @@
     )
     parser.add_argument(
         "--inference-options",
         type=str,
         default="",
         help="If running on-device, use these options when submitting the inference job.",
     )
-    default_runtime = (
-        TargetRuntime.TFLITE
-        if TargetRuntime.TFLITE in available_target_runtimes
-        else available_target_runtimes[0]
-    )
+    default_runtime = _get_default_runtime(available_runtimes=available_target_runtimes)
     add_target_runtime_arg(
         parser,
         help="The runtime to demo (if --on-device is specified).",
         default=default_runtime,
         available_target_runtimes=available_target_runtimes,
     )
 
@@ -374,17 +381,20 @@
         ]
     )
 
 
 def export_parser(
     model_cls: Type[FromPretrainedTypeVar] | Type[FromPrecompiledTypeVar],
     components: Optional[List[str]] = None,
-    supports_qnn=True,
-    supports_ort=True,
-    exporting_compiled_model=False,
+    supports_tflite: bool = True,
+    supports_qnn: bool = True,
+    supports_ort: bool = True,
+    default_runtime: TargetRuntime = TargetRuntime.TFLITE,
+    exporting_compiled_model: bool = False,
+    default_export_device: str = DEFAULT_EXPORT_DEVICE,
 ) -> argparse.ArgumentParser:
     """
     Arg parser to be used in export scripts.
 
     Parameters:
         model_cls: Class of the model to be exported. Used to add additional
             args for model instantiation.
@@ -397,23 +407,25 @@
         supports_ort:
             Whether ORT export is supported.
             Default=True.
         exporting_compiled_model:
             True when exporting compiled model.
             If set, removing skip_profiling flag from export arguments.
             Default = False.
+        default_export_device:
+            Default device to set for export.
 
     Returns:
         Arg parser object.
     """
     parser = get_parser()
     parser.add_argument(
         "--device",
         type=str,
-        default=DEFAULT_EXPORT_DEVICE,
+        default=default_export_device,
         help="Device for which to export.",
     )
     parser.add_argument(
         "--chipset",
         type=str,
         default=None,
         choices=sorted(get_qcom_chipsets(), reverse=True),
@@ -446,22 +458,27 @@
         type=str,
         default=None,
         help="Directory to store generated assets (e.g. compiled model). "
         "Defaults to `<cwd>/build/<model_name>`.",
     )
     if not exporting_compiled_model:
         # Default runtime for compiled model is fixed for given model
-        available_runtimes = [TargetRuntime.TFLITE]
+        available_runtimes = []
+        if supports_tflite:
+            available_runtimes.append(TargetRuntime.TFLITE)
         if supports_qnn:
             available_runtimes.append(TargetRuntime.QNN)
         if supports_ort:
             available_runtimes.append(TargetRuntime.ORT)
+
+        default_runtime = _get_default_runtime(available_runtimes)
         add_target_runtime_arg(
             parser,
             available_target_runtimes=available_runtimes,
+            default=default_runtime,
             help="The runtime for which to export.",
         )
         # No compilation for compiled models
         parser.add_argument(
             "--compile-options",
             type=str,
             default="",
```

## qai_hub_models/utils/asset_loaders.py

```diff
@@ -26,14 +26,15 @@
 import numpy as np
 import requests
 import torch
 import yaml
 from git import Repo
 from PIL import Image
 from schema import And, Schema, SchemaError
+from tqdm import tqdm
 
 ASSET_BASES_DEFAULT_PATH = os.path.join(
     os.path.dirname(os.path.dirname(__file__)), "asset_bases.yaml"
 )
 
 QAIHM_STORE_ROOT = os.environ.get("QAIHM_STORE_ROOT", os.path.expanduser("~"))
 LOCAL_STORE_DEFAULT_PATH = os.path.join(QAIHM_STORE_ROOT, ".qaihm")
@@ -108,15 +109,15 @@
 
 def maybe_clone_git_repo(
     git_file_path: str,
     commit_hash,
     model_name: str,
     model_version: VersionType,
     patches: List[str] = [],
-) -> str:
+) -> Path:
     """Clone (or pull) a repository, save it to disk in a standard location,
     and return the absolute path to the cloned location. Patches can be applied
     by providing a list of paths to diff files."""
 
     # http://blah.come/author/name.git -> name, author
     repo_name = os.path.basename(git_file_path).split(".")[0]
     repo_author = os.path.basename(os.path.dirname(git_file_path))
@@ -238,20 +239,22 @@
     Context manager that runs code with:
      * the source repository added to the system path,
      * cwd set to the source repo's root directory.
 
     Only one of this class should be active per Python session.
     """
 
-    repository_path = maybe_clone_git_repo(
-        source_repo_url,
-        source_repo_commit_hash,
-        source_repo_name,
-        source_repo_version,
-        patches=source_repo_patches,
+    repository_path = str(
+        maybe_clone_git_repo(
+            source_repo_url,
+            source_repo_commit_hash,
+            source_repo_name,
+            source_repo_version,
+            patches=source_repo_patches,
+        )
     )
     SOURCE_AS_ROOT_LOCK.acquire()
     original_path = list(sys.path)
     original_modules = dict(sys.modules)
     cwd = os.getcwd()
     try:
         # If repo path already in sys.path from previous load,
@@ -380,71 +383,93 @@
             file = self.static_web_banner_filename
         elif type == QAIHM_WEB_ASSET.ANIMATED_MOV:
             file = self.animated_web_banner_filename
         else:
             raise NotImplementedError("unsupported web asset type")
         return f"{self.asset_url}/{ModelZooAssetConfig._replace_path_keywords(self.web_asset_folder, model_id=model_id)}/{file}"
 
+    def get_local_store_path(self) -> Path:
+        return Path(self.local_store_path)
+
     def get_local_store_model_path(
         self, model_name: str, version: VersionType, filename: str
-    ) -> str:
-        model_dir = os.path.join(
-            self.local_store_path,
-            self.get_relative_model_asset_path(model_name, version, filename),
+    ) -> Path:
+        return self.local_store_path / self.get_relative_model_asset_path(
+            model_name, version, filename
         )
-        return model_dir
 
     def get_local_store_dataset_path(
         self, dataset_name: str, version: VersionType, filename: str
-    ) -> str:
-        model_dir = os.path.join(
-            self.local_store_path,
-            self.get_relative_dataset_asset_path(dataset_name, version, filename),
+    ) -> Path:
+        return self.local_store_path / self.get_relative_dataset_asset_path(
+            dataset_name, version, filename
         )
-        return model_dir
 
     def get_relative_model_asset_path(
         self, model_id: str, version: Union[int, str], file_name: str
-    ):
+    ) -> Path:
         assert not file_name.startswith("/") and not file_name.startswith("\\")
-        return f"{ModelZooAssetConfig._replace_path_keywords(self.model_asset_folder, model_id=model_id, version=version)}/{file_name}"
+        return (
+            Path(
+                ModelZooAssetConfig._replace_path_keywords(
+                    self.model_asset_folder, model_id=model_id, version=version
+                )
+            )
+            / file_name
+        )
 
     def get_relative_dataset_asset_path(
         self, dataset_id: str, version: Union[int, str], file_name: str
-    ):
+    ) -> Path:
         assert not file_name.startswith("/") and not file_name.startswith("\\")
-        return f"{ModelZooAssetConfig._replace_path_keywords(self.dataset_asset_folder, dataset_id=dataset_id, version=version)}/{file_name}"
+        return (
+            Path(
+                ModelZooAssetConfig._replace_path_keywords(
+                    self.dataset_asset_folder, dataset_id=dataset_id, version=version
+                )
+            )
+            / file_name
+        )
 
     def get_model_asset_url(
         self, model_id: str, version: Union[int, str], file_name: str
-    ):
+    ) -> str:
         assert not file_name.startswith("/") and not file_name.startswith("\\")
-        return f"{self.asset_url}/{self.get_relative_model_asset_path(model_id, version, file_name)}"
+        return f"{self.asset_url}/{self.get_relative_model_asset_path(model_id, version, file_name).as_posix()}"
 
     def get_dataset_asset_url(
         self, dataset_id: str, version: Union[int, str], file_name: str
-    ):
+    ) -> str:
         assert not file_name.startswith("/") and not file_name.startswith("\\")
-        return f"{self.asset_url}/{self.get_relative_dataset_asset_path(dataset_id, version, file_name)}"
+        return f"{self.asset_url}/{self.get_relative_dataset_asset_path(dataset_id, version, file_name).as_posix()}"
 
-    def get_qaihm_repo(self, model_id: str, relative=True):
-        relative_path = f"{ModelZooAssetConfig._replace_path_keywords(self.qaihm_repo, model_id=model_id)}"
+    def get_qaihm_repo(self, model_id: str, relative=True) -> Path | str:
+        relative_path = Path(
+            ModelZooAssetConfig._replace_path_keywords(
+                self.qaihm_repo, model_id=model_id
+            )
+        )
         if not relative:
-            return self.repo_url + "/" + relative_path
-
+            return f"{self.repo_url}/{relative_path.as_posix()}"
         return relative_path
 
-    def get_website_url(self, model_id: str, relative=False):
-        relative_path = f"{ModelZooAssetConfig._replace_path_keywords(self.models_website_relative_path, model_id=model_id)}"
+    def get_website_url(self, model_id: str, relative=False) -> str:
+        relative_path = Path(
+            ModelZooAssetConfig._replace_path_keywords(
+                self.models_website_relative_path, model_id=model_id
+            )
+        ).as_posix()
         if not relative:
-            return self.models_website_url + "/" + relative_path
+            return f"{self.models_website_url}/{relative_path}"
         return relative_path
 
-    def get_example_use(self, model_id: str):
-        return f"{ModelZooAssetConfig._replace_path_keywords(self.example_use, model_id=model_id)}"
+    def get_example_use(self, model_id: str) -> str:
+        return ModelZooAssetConfig._replace_path_keywords(
+            self.example_use, model_id=model_id
+        )
 
     ###
     # Helpers
     ###
     @staticmethod
     def _replace_path_keywords(
         path: str,
@@ -554,35 +579,35 @@
     """
     Helper class for downloading files for storage in the QAIHM asset cache.
     """
 
     def __init__(
         self,
         url: str,
-        local_cache_path: str,
+        local_cache_path: Path,
         asset_config=ASSET_CONFIG,
         model_downloader: Callable[[str, str, int], str] | None = None,
         downloader_num_retries=4,
     ):
         self.url = url
         self.local_cache_path = local_cache_path
         self.asset_config: ModelZooAssetConfig = asset_config
         self._downloader: Callable = model_downloader or download_file
         self.downloader_num_retries = downloader_num_retries
 
         # Append file name to local path if no file name is present
         path, ext = os.path.splitext(self.local_cache_path)
         if not ext:
             file_name = self.url.rsplit("/", 1)[-1]
-            self.local_cache_path = os.path.join(path, file_name)
+            self.local_cache_path = Path(path) / file_name
 
         # Set is_extracted if already extracted on disk
         file, _ = os.path.splitext(self.local_cache_path)
         self.is_extracted = list(
-            filter(local_cache_path.endswith, [".zip", ".tar", ".tar.gz", ".tgz"])
+            filter(str(local_cache_path).endswith, [".zip", ".tar", ".tar.gz", ".tgz"])
         ) != [] and os.path.isdir(file)
 
     def __repr__(self):
         return self.url
 
     @staticmethod
     def from_asset_store(
@@ -598,24 +623,24 @@
             num_retries: Number of retries when downloading thie file.
 
             asset_config: Asset config to use to save this file.
         """
         web_store_path = f"{asset_config.asset_url}/{relative_store_file_path}"
         return CachedWebAsset(
             web_store_path,
-            relative_store_file_path,
+            Path(relative_store_file_path),
             asset_config,
             download_file,
             num_retries,
         )
 
     @staticmethod
     def from_google_drive(
         gdrive_file_id: str,
-        relative_store_file_path: str,
+        relative_store_file_path: str | Path,
         num_retries=4,
         asset_config=ASSET_CONFIG,
     ):
         """
         File from google drive.
 
         Parameters:
@@ -626,15 +651,15 @@
 
             num_retries: Number of retries when downloading thie file.
 
             asset_config: Asset config to use to save this file.
         """
         return CachedWebAsset(
             f"https://drive.google.com/uc?id={gdrive_file_id}",
-            relative_store_file_path,
+            Path(relative_store_file_path),
             asset_config,
             download_and_cache_google_drive,
             num_retries,
         )
 
     def path(self, extracted=None) -> Path:
         """
@@ -643,20 +668,21 @@
         By default, for archived (.zip, .tar, .etc) assets, path() will return the extracted path if the asset
         has been extracted, and the original archive file's path if it has not been extracted.
 
         Parameters:
             extracted: If true, return the path of the extracted asset on disk.
                        If false, return the path of the archive path on disk.
         """
+        file: str | Path
         if (extracted is None and self.is_extracted) or extracted:
             file, _ = os.path.splitext(self.local_cache_path)
         else:
             file = self.local_cache_path
 
-        return Path(self.asset_config.local_store_path) / file
+        return self.asset_config.get_local_store_path() / file
 
     def fetch(self, force=False, extract=False) -> Path:
         """
         Fetch this file from the web if it does not exist on disk.
 
         Parameters:
             force: If the file exists on disk already, discard it and download it again.
@@ -926,19 +952,30 @@
 def download_file(web_url: str, dst_path: str, num_retries: int = 4) -> str:
     """
     Downloads data from the internet and stores in `dst_folder`.
     `dst_folder` should be relative to the local cache root for qai_hub_models.
     """
     if not os.path.exists(dst_path):
         print(f"Downloading data at {web_url} to {dst_path}... ", end="")
-        file_data = requests.get(web_url)
-        if file_data.status_code != 200:
+
+        # Streaming, so we can iterate over the response.
+        response = requests.get(web_url, stream=True)
+
+        # Sizes in bytes.
+        total_size = int(response.headers.get("content-length", 0))
+        block_size = 1024
+
+        with tqdm(total=total_size, unit="B", unit_scale=True) as progress_bar:
+            with open(dst_path, "wb") as file:
+                for data in response.iter_content(block_size):
+                    progress_bar.update(len(data))
+                    file.write(data)
+
+        if response.status_code != 200:
             raise ValueError(f"Unable to download file at {web_url}")
-        with open(dst_path, "wb") as dst_file:
-            dst_file.write(file_data.content)
         print("Done")
     return dst_path
 
 
 def download_and_cache_google_drive(web_url: str, dst_path: str, num_retries: int = 4):
     """
     Download file from google drive to the local directory.
@@ -1016,8 +1053,20 @@
             print(error_msg)
             if hasattr(error, "status_code"):
                 print(f"Status code: {error.status_code}")  # type: ignore
             time.sleep(10)
             return callback_with_retry(num_retries - 1, callback, *args, **kwargs)
 
 
+@contextmanager
+def qaihm_temp_dir():
+    """
+    Keep temp file under LOCAL_STORE_DEFAULT_PATH instead of /tmp which has
+    limited space.
+    """
+    path = os.path.join(LOCAL_STORE_DEFAULT_PATH, "tmp")
+    os.makedirs(path, exist_ok=True)
+    with tempfile.TemporaryDirectory(dir=path) as tempdir:
+        yield tempdir
+
+
 PathType = Union[str, Path, CachedWebAsset]
```

## qai_hub_models/utils/base_model.py

```diff
@@ -1,18 +1,19 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 from pathlib import Path
-from typing import Any
+from typing import Any, List, Optional
 
+import qai_hub
 import torch
-from qai_hub.client import SourceModel
+from qai_hub.client import Device, SourceModel
 
 from qai_hub_models.models.common import (
     SampleInputsType,
     SourceModelFormat,
     TargetRuntime,
 )
 from qai_hub_models.models.protocols import (
@@ -120,46 +121,77 @@
 
     def convert_to_hub_source_model(
         self,
         target_runtime: TargetRuntime,
         output_path: str | Path,
         input_spec: InputSpec | None = None,
         check_trace: bool = True,
+        external_onnx_weights: bool = False,
+        output_names: Optional[List[str]] = None,
     ) -> SourceModel:
         """
         Convert to a AI Hub source model appropriate for the export method.
         """
         # Local import to prevent circular dependency
         from qai_hub_models.utils.inference import prepare_compile_zoo_model_to_hub
 
         source_model, _ = prepare_compile_zoo_model_to_hub(
             self,
             source_model_format=self.preferred_hub_source_model_format(target_runtime),
             target_runtime=target_runtime,
             output_path=output_path,
             input_spec=input_spec,
             check_trace=check_trace,
+            external_onnx_weights=external_onnx_weights,
+            output_names=output_names,
         )
         return source_model
 
     def get_hub_compile_options(
         self,
         target_runtime: TargetRuntime,
         other_compile_options: str = "",
+        device: Optional[Device] = None,
     ) -> str:
         """
         AI Hub compile options recommended for the model.
         """
-        compile_options = ""
-        if target_runtime == TargetRuntime.QNN:
-            compile_options = "--target_runtime qnn_lib_aarch64_android"
-        if target_runtime == TargetRuntime.ORT:
-            compile_options = "--target_runtime onnx"
+        target_runtime_flag = None
+        if "--target_runtime" not in other_compile_options:
+            if target_runtime == TargetRuntime.QNN:
+                if device:
+                    if not device.attributes:
+                        # Only name / os specified
+                        devices = qai_hub.get_devices(device.name, device.os)
+                    elif not device.name:
+                        # Only attribute specified
+                        devices = qai_hub.get_devices(attributes=device.attributes)
+                    else:
+                        devices = [device]
+
+                    for device in devices:
+                        if "os:android" not in device.attributes:
+                            target_runtime_flag = "qnn_bin"
+                            break
+
+                target_runtime_flag = target_runtime_flag or "qnn_lib_aarch64_android"
+            elif target_runtime == TargetRuntime.ORT:
+                target_runtime_flag = "onnx"
+            elif target_runtime == TargetRuntime.TFLITE:
+                target_runtime_flag = "tflite"
+            else:
+                raise NotImplementedError()
+
+        compile_options = (
+            f"--target_runtime {target_runtime_flag}" if target_runtime_flag else ""
+        )
+
         if other_compile_options != "":
             return compile_options + " " + other_compile_options
+
         return compile_options
 
     def preferred_hub_source_model_format(
         self, target_runtime: TargetRuntime
     ) -> SourceModelFormat:
         """
         Source model format preferred for conversion on AI Hub.
```

## qai_hub_models/utils/compare.py

```diff
@@ -1,20 +1,31 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
-from typing import Dict, List, Union
+from typing import Dict, List, Optional, Union
 
 import numpy as np
 import pandas as pd
 import torch
 
 
+def _flatten_tuple(out_tuple):
+    if not isinstance(out_tuple, tuple):
+        return (out_tuple.detach(),)
+
+    flattened_tuple = []
+    for elem in out_tuple:
+        flattened_tuple.extend(_flatten_tuple(elem))
+
+    return tuple(flattened_tuple)
+
+
 def torch_inference(
     model: torch.nn.Module, sample_inputs: Dict[str, List[np.ndarray]]
 ) -> List[np.ndarray]:
     """
     Performs inference on a torch model given a set of sample inputs.
 
     Parameters:
@@ -29,16 +40,18 @@
     for i in range(len(list(sample_inputs.values())[0])):
         inputs = {}
         for input_name in input_names:
             inputs[input_name] = torch.from_numpy(sample_inputs[input_name][i]).to(
                 "cpu"
             )
         with torch.no_grad():
-            out = model(**inputs)
+            out = model(*inputs.values())
         out_tuple = (out,) if isinstance(out, torch.Tensor) else out
+        out_tuple = _flatten_tuple(out_tuple)
+
         for i, out_val in enumerate(out_tuple):
             if i == len(torch_outs):
                 torch_outs.append([])
             torch_outs[i].append(out_val)
     return [torch.cat(out_list, dim=0).numpy() for out_list in torch_outs]
 
 
@@ -116,15 +129,15 @@
     ),
 )
 
 
 def generate_comparison_metrics(
     expected: List[np.ndarray],
     actual: List[np.ndarray],
-    names: List[str] | None = None,
+    names: Optional[List[str]] = None,
     metrics: str = "psnr",
 ) -> pd.DataFrame:
     """
     Compares the outputs of a model run in two different ways.
     For example, expected might be run on local cpu and actual run on device.
 
     Parameters:
```

## qai_hub_models/utils/config_loaders.py

```diff
@@ -151,14 +151,18 @@
     def __str__(self) -> str:
         return self.name.replace("_", "-").lower()
 
     def __repr__(self) -> str:
         return self.__str__()
 
 
+def is_gen_ai_model(tags: List[MODEL_TAG]) -> bool:
+    return MODEL_TAG.LLM in tags or MODEL_TAG.GENERATIVE_AI in tags
+
+
 class MODEL_STATUS(Enum):
     PUBLIC = 0
     PRIVATE = 1
     # proprietary models are released only internally
     PROPRIETARY = 2
 
     @staticmethod
@@ -172,14 +176,15 @@
 class MODEL_USE_CASE(Enum):
     # Image: 100 - 199
     IMAGE_CLASSIFICATION = 100
     IMAGE_EDITING = 101
     IMAGE_GENERATION = 102
     SUPER_RESOLUTION = 103
     SEMANTIC_SEGMENTATION = 104
+    DEPTH_ESTIMATION = 105
     # Ex: OCR, image caption
     IMAGE_TO_TEXT = 105
     OBJECT_DETECTION = 106
     POSE_ESTIMATION = 107
 
     # Audio: 200 - 299
     SPEECH_RECOGNITION = 200
@@ -475,49 +480,53 @@
 class QAIHMModelCodeGen:
     def __init__(
         self,
         is_aimet: bool,
         has_on_target_demo: bool,
         qnn_export_failure_reason: str,
         tflite_export_failure_reason: str,
+        ort_export_failure_reason: str,
         has_demo: bool,
         check_trace: bool,
         channel_last_input: List[str],
         channel_last_output: List[str],
         outputs_to_skip_validation: List[str],
         export_test_model_kwargs: Dict[str, str],
         components: Dict[str, Any],
         default_components: List[str],
         skip_tests: bool,
         is_precompiled: bool,
         no_assets: bool,
+        skip_export: bool,
         global_requirements_incompatible: bool,
         torchscript_opt: List[str],
         inference_metrics: str,
-        supports_ort: bool,
+        additional_readme_section: str,
     ) -> None:
         self.is_aimet = is_aimet
         self.has_on_target_demo = has_on_target_demo
         self.qnn_export_failure_reason = qnn_export_failure_reason
         self.tflite_export_failure_reason = tflite_export_failure_reason
+        self.ort_export_failure_reason = ort_export_failure_reason
         self.has_demo = has_demo
         self.check_trace = check_trace
         self.channel_last_input = channel_last_input
         self.channel_last_output = channel_last_output
         self.outputs_to_skip_validation = outputs_to_skip_validation
         self.export_test_model_kwargs = export_test_model_kwargs
         self.components = components
         self.default_components = default_components
         self.skip_tests = skip_tests
         self.is_precompiled = is_precompiled
         self.no_assets = no_assets
         self.global_requirements_incompatible = global_requirements_incompatible
         self.torchscript_opt = torchscript_opt
         self.inference_metrics = inference_metrics
-        self.supports_ort = supports_ort
+        self.additional_readme_section = additional_readme_section
+        self.skip_export = skip_export
 
     def validate(self) -> Tuple[bool, Optional[str]]:
         """Returns false with a reason if the info spec for this model is not valid."""
         return True, None
 
     @classmethod
     def from_model(cls: Type[QAIHMModelCodeGen], model_id: str) -> QAIHMModelCodeGen:
@@ -533,55 +542,59 @@
         # Load CFG and params
         code_gen_config = QAIHMModelCodeGen.load_code_gen_yaml(code_gen_path)
         return cls(
             code_gen_config["is_aimet"],
             code_gen_config["has_on_target_demo"],
             code_gen_config["qnn_export_failure_reason"],
             code_gen_config["tflite_export_failure_reason"],
+            code_gen_config["ort_export_failure_reason"],
             code_gen_config["has_demo"],
             code_gen_config["check_trace"],
             code_gen_config["channel_last_input"],
             code_gen_config["channel_last_output"],
             code_gen_config["outputs_to_skip_validation"],
             code_gen_config["export_test_model_kwargs"],
             code_gen_config["components"],
             code_gen_config["default_components"],
             code_gen_config["skip_tests"],
             code_gen_config["is_precompiled"],
             code_gen_config["no_assets"],
             code_gen_config["global_requirements_incompatible"],
             code_gen_config["torchscript_opt"],
             code_gen_config["inference_metrics"],
-            code_gen_config["supports_ort"],
+            code_gen_config["additional_readme_section"],
+            code_gen_config["skip_export"],
         )
 
     # Schema for code-gen.yaml
     CODE_GEN_YAML_SCHEMA = Schema(
         And(
             {
                 OptionalSchema("has_components", default=""): str,
                 OptionalSchema("is_aimet", default=False): bool,
                 OptionalSchema("has_on_target_demo", default=False): bool,
                 OptionalSchema("qnn_export_failure_reason", default=""): str,
                 OptionalSchema("tflite_export_failure_reason", default=""): str,
+                OptionalSchema("ort_export_failure_reason", default=""): str,
                 OptionalSchema("has_demo", default=True): bool,
                 OptionalSchema("check_trace", default=True): bool,
                 OptionalSchema("channel_last_input", default=[]): list,
                 OptionalSchema("channel_last_output", default=[]): list,
                 OptionalSchema("outputs_to_skip_validation", default=[]): list,
                 OptionalSchema("export_test_model_kwargs", default={}): dict,
                 OptionalSchema("components", default={}): dict,
                 OptionalSchema("default_components", default=[]): list,
                 OptionalSchema("skip_tests", default=False): bool,
                 OptionalSchema("is_precompiled", default=False): bool,
                 OptionalSchema("no_assets", default=False): bool,
                 OptionalSchema("global_requirements_incompatible", default=False): bool,
                 OptionalSchema("torchscript_opt", default=[]): list,
                 OptionalSchema("inference_metrics", default="psnr"): str,
-                OptionalSchema("supports_ort", default=False): bool,
+                OptionalSchema("additional_readme_section", default=""): str,
+                OptionalSchema("skip_export", default=False): bool,
             }
         )
     )
 
     @staticmethod
     def load_code_gen_yaml(path: str | Path | None = None):
         if not path or not os.path.exists(path):
@@ -732,15 +745,15 @@
         if self.has_animated_banner:
             animated_banner_url = ASSET_CONFIG.get_web_asset_url(
                 self.id, QAIHM_WEB_ASSET.ANIMATED_MOV
             )
             if session.head(animated_banner_url).status_code != requests.codes.ok:
                 return False, f"Animated banner is missing at {animated_banner_url}"
 
-        expected_qaihm_repo = f"qai_hub_models/models/{self.id}"
+        expected_qaihm_repo = Path("qai_hub_models") / "models" / self.id
         if expected_qaihm_repo != ASSET_CONFIG.get_qaihm_repo(self.id):
             return False, "QAIHM repo not pointing to expected relative path"
 
         expected_example_use = f"qai_hub_models/models/{self.id}#example--usage"
         if expected_example_use != ASSET_CONFIG.get_example_use(self.id):
             return False, "Example-usage field not pointing to expected relative path"
```

## qai_hub_models/utils/draw.py

```diff
@@ -1,34 +1,34 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
-from typing import List, Optional, Tuple
+from typing import List, Optional, Tuple, Union
 
 import cv2
-import numpy
+import numpy as np
 import torch
 
 
 def draw_points(
-    frame: numpy.ndarray,
-    points: numpy.ndarray | torch.Tensor,
+    frame: np.ndarray,
+    points: np.ndarray | torch.Tensor,
     color: Tuple[int, int, int] = (0, 0, 0),
-    size: int = 3,
+    size: Union[int, List[int]] = 10,
 ):
     """
     Draw the given points on the frame.
 
     Parameters:
-        frame: numpy.ndarray
-            numpy array (H W C x uint8, BGR)
+        frame: np.ndarray
+            np array (H W C x uint8, BGR)
 
-        points: numpy.ndarray | torch.Tensor
+        points: np.ndarray | torch.Tensor
             array (N, 2) where layout is
                 [x1, y1] [x2, y2], ...
             or
             array (N * 2,) where layout is
                 x1, y1, x2, y2, ...
 
         color: Tuple[int, int, int]
@@ -36,82 +36,87 @@
 
         size: int
             Size of drawn points
 
     Returns:
         None; modifies frame in place.
     """
-    n2 = len(points.shape) == 2
-    for i in range(0, len(points) if n2 else len(points) // 2):
-        x, y = points[i] if n2 else (points[i * 2], points[i * 2 + 1])
-        cv2.circle(frame, (int(x), int(y)), size, color, thickness=size)
+    if len(points.shape) == 1:
+        points = points.reshape(-1, 2)
+    assert isinstance(size, int) or len(size) == len(points)
+    cv_keypoints = []
+    for i, (x, y) in enumerate(points):
+        curr_size = size if isinstance(size, int) else size[i]
+        cv_keypoints.append(cv2.KeyPoint(int(x), int(y), curr_size))
+
+    cv2.drawKeypoints(
+        frame,
+        cv_keypoints,
+        outImage=frame,
+        color=color,
+        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,
+    )
 
 
 def draw_connections(
-    frame: numpy.ndarray,
-    points: numpy.ndarray | torch.Tensor,
+    frame: np.ndarray,
+    points: np.ndarray | torch.Tensor,
     connections: List[Tuple[int, int]],
     color: Tuple[int, int, int] = (0, 0, 0),
-    size: int = 3,
+    size: int = 1,
 ):
     """
     Draw connecting lines between the given points on the frame.
 
     Parameters:
-        frame: numpy.ndarray
-            numpy array (H W C x uint8, BGR)
+        frame:
+            np array (H W C x uint8, BGR)
 
-        points: numpy.ndarray | torch.Tensor
+        points:
             array (N, 2) where layout is
                 [x1, y1] [x2, y2], ...
             or
             array (N * 2,) where layout is
                 x1, y1, x2, y2, ...
 
-        connections: List[Tuple[int, int]]
+        connections:
             List of points that should be connected by a line.
             Format is [(src point index, dst point index), ...]
 
-        color: Tuple[int, int, int]
+        color:
             Color of drawn points (RGB)
 
         size: int
             Size of drawn connection lines
 
     Returns:
         None; modifies frame in place.
     """
-    n2 = len(points.shape) == 2
-    for connection in connections:
-        x0, y0 = (
-            points[connection[0]]
-            if n2
-            else (points[connection[0] * 2], points[connection[0] * 2 + 1])
-        )
-        x1, y1 = (
-            points[connection[1]]
-            if n2
-            else (points[connection[1] * 2], points[connection[1] * 2 + 1])
-        )
-        x0, y0 = int(x0), int(y0)
-        x1, y1 = int(x1), int(y1)
-        cv2.line(frame, (x0, y0), (x1, y1), color, size)
+    if len(points.shape) == 1:
+        points = points.reshape(-1, 2)
+    point_pairs = [
+        ((int(points[i][0]), int(points[i][1])), (int(points[j][0]), int(points[j][1])))
+        for (i, j) in connections
+    ]
+    cv2.polylines(
+        frame, np.array(point_pairs), isClosed=False, color=color, thickness=size  # type: ignore
+    )
 
 
 def draw_box_from_corners(
-    frame: numpy.ndarray, corners: numpy.ndarray | torch.Tensor, color=(0, 0, 0), size=3
+    frame: np.ndarray, corners: np.ndarray | torch.Tensor, color=(0, 0, 0), size=3
 ):
     """
     Draw a box using the 4 points provided as boundaries.
 
     Parameters:
-        frame: numpy.ndarray
-            numpy array (H W C x uint8, BGR)
+        frame: np.ndarray
+            np array (H W C x uint8, BGR)
 
-        corners: numpy.ndarray | torch.Tensor
+        corners: np.ndarray | torch.Tensor
             array (4, 2) where layout is
                 [x1, y1] [x2, y2], ...
             or
             array (8) where layout is
                 x1, y1, x2, y2
 
         color: Tuple[int, int, int]
@@ -124,27 +129,27 @@
         None; modifies frame in place.
     """
     draw_points(frame, corners, color, size)
     draw_connections(frame, corners, [(0, 1), (0, 2), (1, 3), (2, 3)], color, size)
 
 
 def draw_box_from_xywh(
-    frame: numpy.ndarray,
-    box: numpy.ndarray | torch.Tensor,
+    frame: np.ndarray,
+    box: np.ndarray | torch.Tensor,
     color: Tuple[int, int, int] = (0, 0, 0),
     size: int = 3,
 ):
     """
     Draw a box using the provided data (center / height / width) to compute the box.
 
     Parameters:
-        frame: numpy.ndarray
-            numpy array (H W C x uint8, BGR)
+        frame: np.ndarray
+            np array (H W C x uint8, BGR)
 
-        box: numpy.ndarray | torch.Tensor
+        box: np.ndarray | torch.Tensor
             array (4), where layout is
                 [xcenter, ycenter, h, w]
 
         color: Tuple[int, int, int]
             Color of drawn points and connection lines (RGB)
 
         size: int
@@ -156,29 +161,29 @@
     xc, yc, h, w = box
     TL = [xc - w // 2, yc - h // 2]
     BR = [xc + w // 2, yc + h // 2]
     cv2.rectangle(frame, TL, BR, color, size)
 
 
 def draw_box_from_xyxy(
-    frame: numpy.ndarray,
-    top_left: numpy.ndarray | torch.Tensor | Tuple[int, int],
-    bottom_right: numpy.ndarray | torch.Tensor | Tuple[int, int],
+    frame: np.ndarray,
+    top_left: np.ndarray | torch.Tensor | Tuple[int, int],
+    bottom_right: np.ndarray | torch.Tensor | Tuple[int, int],
     color: Tuple[int, int, int] = (0, 0, 0),
     size: int = 3,
     text: Optional[str] = None,
 ):
     """
     Draw a box using the provided top left / bottom right points to compute the box.
 
     Parameters:
-        frame: numpy.ndarray
-            numpy array (H W C x uint8, BGR)
+        frame: np.ndarray
+            np array (H W C x uint8, BGR)
 
-        box: numpy.ndarray | torch.Tensor
+        box: np.ndarray | torch.Tensor
             array (4), where layout is
                 [xc, yc, h, w]
 
         color: Tuple[int, int, int]
             Color of drawn points and connection lines (RGB)
 
         size: int
@@ -213,11 +218,11 @@
 
     Inputs:
         num_classes: Number of colors to produce.
 
     Returns:
         A list of `num_classes` colors in RGB format.
     """
-    numpy.random.seed(42)  # For reproducible results
-    color_map = numpy.random.randint(0, 256, size=(num_classes, 3), dtype=numpy.uint8)
+    np.random.seed(42)  # For reproducible results
+    color_map = np.random.randint(0, 256, size=(num_classes, 3), dtype=np.uint8)
     color_map[0] = [0, 0, 0]  # Background class, usually black
     return color_map
```

## qai_hub_models/utils/huggingface.py

```diff
@@ -4,15 +4,17 @@
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import os
 from pathlib import Path
 from typing import List
 
-from huggingface_hub import HfFileSystem, hf_hub_download
+from huggingface_hub import HfApi, HfFileSystem, hf_hub_download
+from huggingface_hub.utils import GatedRepoError
+from packaging import version
 
 from qai_hub_models.utils.asset_loaders import ASSET_CONFIG, ModelZooAssetConfig
 from qai_hub_models.utils.base_model import TargetRuntime
 
 
 def fetch_huggingface_target_model(
     model_name: str,
@@ -41,7 +43,41 @@
     os.makedirs(dst_folder, exist_ok=True)
     paths = []
     for file in files:
         path = hf_hub_download(hf_path, file[len(hf_path) + 1 :], local_dir=dst_folder)
         paths.append(path)
 
     return paths
+
+
+def has_model_access(repo_name: str, repo_url: str):
+    # Huggingface returns GatedRepoError if model is not accessible to current User.
+    # ref: https://github.com/huggingface/huggingface_hub/blob/5ff2d150d121d04799b78bc08f2343c21b8f07a9/src/huggingface_hub/utils/_errors.py#L135
+
+    try:
+        hf_api = HfApi()
+        hf_api.model_info(repo_name)
+    except GatedRepoError:
+        no_access_error = (
+            f"Seems like you don't have access to {repo_name} yet.\nPlease follow the following steps:"
+            f"\n 1. Apply for access at {repo_url}"
+            f"\n 2. Setup Huggingface API token as described in https://huggingface.co/docs/huggingface_hub/en/quick-start#login-command"
+            f"\nOnce access request is approved, you should be able to export/load {repo_name} via AI-Hub."
+        )
+        raise RuntimeError(no_access_error)
+
+    # Model is accesible for current User.
+    return True
+
+
+def ensure_has_required_transformer(least_expected_version):
+    # import transformer as part of this function
+    # to avoid leaking installation globally on file import.
+    # NOTE: #10761 this function should not be required once AIMET (https://pypi.org/project/aimet-torch/)
+    # remove tight dependency on transformers.
+    import transformers
+
+    if version.parse(transformers.__version__) < version.parse(least_expected_version):
+        raise RuntimeError(
+            f"Installed transformers version not supported. Expected >= {least_expected_version}, got {str(transformers.__version__)}\n"
+            f"Please run `pip install transformers=={least_expected_version}`"
+        )
```

## qai_hub_models/utils/image_processing.py

```diff
@@ -1,25 +1,35 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import functools
+import math
 from typing import Callable, List, Tuple
 
 import cv2
 import numpy as np
 import torch
 import torchvision.transforms as transforms
 from PIL.Image import Image
 from PIL.Image import fromarray as ImageFromArray
 from torch.nn.functional import interpolate, pad
 from torchvision import transforms
 
+IMAGENET_DIM = 224
+IMAGENET_TRANSFORM = transforms.Compose(
+    [
+        transforms.Resize(256),
+        transforms.CenterCrop(IMAGENET_DIM),
+        transforms.ToTensor(),
+    ]
+)
+
 
 def app_to_net_image_inputs(
     pixel_values_or_image: torch.Tensor | np.ndarray | Image | List[Image],
 ) -> Tuple[List[np.ndarray], torch.Tensor]:
     """
     Convert the provided images to application inputs.
     ~~This does not change channel order. RGB stays RGB, BGR stays BGR, etc~~
@@ -171,20 +181,23 @@
     Based on https://github.com/zmurez/MediaPipePyTorch/blob/master/blazebase.py
     """
     height, width = image.shape[-2:]
     dst_frame_height, dst_frame_width = dst_size
 
     h_ratio = dst_frame_height / height
     w_ratio = dst_frame_width / width
-    if width * h_ratio > dst_frame_height:
-        scale = w_ratio
-    else:
+    scale = min(h_ratio, w_ratio)
+    if h_ratio < w_ratio:
         scale = h_ratio
-
-    import math
+        new_height = dst_frame_height
+        new_width = math.floor(width * scale)
+    else:
+        scale = w_ratio
+        new_height = math.floor(height * scale)
+        new_width = dst_frame_width
 
     new_height = math.floor(height * scale)
     new_width = math.floor(width * scale)
     pad_h = dst_frame_height - new_height
     pad_w = dst_frame_width - new_width
 
     pad_top = int(pad_h // 2)
```

## qai_hub_models/utils/inference.py

```diff
@@ -1,25 +1,24 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import os
-import tempfile
 from pathlib import Path
-from typing import List, Mapping, Tuple
+from typing import List, Mapping, Optional, Tuple
 
 import numpy as np
 import qai_hub as hub
 import torch
 from qai_hub.public_rest_api import DatasetEntries
 
 from qai_hub_models.models.protocols import ExecutableModelProtocol
-from qai_hub_models.utils.asset_loaders import ModelZooAssetConfig
+from qai_hub_models.utils.asset_loaders import ModelZooAssetConfig, qaihm_temp_dir
 from qai_hub_models.utils.base_model import BaseModel, SourceModelFormat, TargetRuntime
 from qai_hub_models.utils.input_spec import InputSpec
 from qai_hub_models.utils.qai_hub_helpers import (
     transpose_channel_first_to_last,
     transpose_channel_last_to_first,
 )
 from qai_hub_models.utils.qnn_helpers import is_qnn_hub_model
@@ -34,14 +33,16 @@
     model: BaseModel,
     source_model_format: SourceModelFormat,
     target_runtime: TargetRuntime,
     output_path: str | Path = "",
     input_spec: InputSpec | None = None,
     check_trace: bool = True,
     prepare_compile_options_only: bool = False,
+    external_onnx_weights: bool = False,
+    output_names: Optional[List[str]] = None,
 ) -> Tuple[str | None, str]:
     """
     Args:
 
     - (source_model_format, target_runtime):  One of the followings
 
         (1) (ONNX, QNN)
@@ -82,39 +83,48 @@
         model, AIMETQuantizableMixin
     )
 
     model_name = model.__class__.__name__
 
     compilation_options = model.get_hub_compile_options(target_runtime)
 
+    if output_names is None:
+        output_names = []
+
     if is_aimet:
         if source_model_format == SourceModelFormat.ONNX:
 
             def export_model_func():
+                print("Exporting model to ONNX and generating AIMET encodings")
                 return model.convert_to_onnx_and_aimet_encodings(
-                    output_path, model_name=model_name
+                    output_path,
+                    model_name=model_name,
+                    external_weights=external_onnx_weights,
+                    output_names=output_names,
                 )
 
         elif (
             source_model_format == SourceModelFormat.TORCHSCRIPT
             and target_runtime == TargetRuntime.TFLITE
         ):
 
             def export_model_func():
+                print("Converting model to Torchscript")
                 traced_model = model.convert_to_torchscript(
                     input_spec=input_spec, check_trace=check_trace
                 )
                 model_path = os.path.join(output_path, model_name + ".pt")
                 os.makedirs(output_path, exist_ok=True)
                 torch.jit.save(traced_model, model_path)
                 return model_path
 
         else:  # Torchscript and QNN
 
             def export_model_func():
+                print("Converting model to Torchscript and generating AIMET encodings")
                 exported_model = model.convert_to_torchscript_and_aimet_encodings(  # type: ignore
                     output_path,
                     model_name=model_name,
                     input_spec=input_spec,
                 )
                 return exported_model
 
@@ -157,15 +167,15 @@
     """
 
     if input_spec is None:
         input_spec = model.get_input_spec()
 
     model_name = model.__class__.__name__
 
-    with tempfile.TemporaryDirectory() as tmp_dir:
+    with qaihm_temp_dir() as tmp_dir:
         assert tmp_dir is not None
         source_model, compilation_options = prepare_compile_zoo_model_to_hub(
             model=model,
             source_model_format=source_model_format,
             target_runtime=target_runtime,
             output_path=tmp_dir,
             check_trace=check_trace,
@@ -214,19 +224,21 @@
 
     def __init__(
         self,
         model: hub.Model,
         input_names: List[str],
         device: hub.Device,
         inference_options: str = "",
+        output_names: Optional[List[str]] = None,
     ):
         self.model = model
         self.input_names = input_names
         self.device = device
         self.inference_options = inference_options
+        self.output_names = [] if output_names is None else output_names
 
     def __call__(
         self,
         *args: torch.Tensor
         | np.ndarray
         | List[torch.Tensor | np.ndarray]
         | hub.Dataset
@@ -305,17 +317,20 @@
         if channel_last_output:
             output_dataset = transpose_channel_last_to_first(
                 channel_last_output,
                 output_dataset,  # type: ignore
                 target_runtime,
             )  # type: ignore
 
+        outputs = output_dataset.values()  # type: ignore
+        if len(self.output_names) > 0:
+            outputs = [output_dataset[out_name] for out_name in self.output_names]  # type: ignore
+
         output_torch = [
-            torch.from_numpy(np.concatenate(outputs, axis=0))
-            for outputs in output_dataset.values()  # type: ignore
+            torch.from_numpy(np.concatenate(output, axis=0)) for output in outputs
         ]
 
         if len(output_torch) == 1:
             return output_torch[0]
         return tuple(output_torch)
 
 
@@ -330,27 +345,25 @@
     Caches pre-compiled model in default asset path to be used in sub-sequence demos.
     """
     asset_config = ModelZooAssetConfig.from_cfg()
     model_id_path = asset_config.get_local_store_model_path(
         model_name, model_version, f"{model_component}_model_id.cached"
     )
 
-    use_cached_model = not ignore_cached_model or os.path.exists(model_id_path)
     uploaded_model = None
-    if use_cached_model:
+    if not ignore_cached_model:
         try:
             with open(model_id_path, "r") as model_id_file:
                 model_id = model_id_file.readline().strip()
             print(f"Using previously uploaded model({model_id}) for {model_component}")
             uploaded_model = hub.get_model(model_id=model_id)
             if uploaded_model is not None:
                 return uploaded_model
 
         except Exception:
-            # Try uploading model instead
-            use_cached_model = False
+            pass
 
     # Upload model on hub
     uploaded_model = hub.upload_model(model_path)
     with open(model_id_path, "w") as model_id_file:
         model_id_file.writelines([f"{uploaded_model.model_id}"])
     return uploaded_model
```

## qai_hub_models/utils/measurement.py

```diff
@@ -1,22 +1,23 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import os
-import tempfile
 from pathlib import Path
 from typing import List, Union
 
 import numpy as np
 import qai_hub as hub
 from tflite import Model as TFModel  # type: ignore
 
+from qai_hub_models.utils.asset_loaders import qaihm_temp_dir
+
 
 def display_with_sig_figs(num: float, num_sig_figs: int = 3) -> str:
     """
     Displays the given number as a string with the appropriate number of
     significant figures. Example:
         display_with_sig_figs(1234.2, num_sig_figs=3) -> "1230"
     Parameters:
@@ -99,15 +100,15 @@
     return get_formatted_size(parameter_cnt, ["", "K", "M", "B", "T"], 1000.0)
 
 
 def get_model_size_mb(hub_model: hub.Model) -> float:
     """Return target model size in MB. This is a special case for ease of
     testing"""
     assert hub_model is not None
-    with tempfile.TemporaryDirectory() as tmp_dir:
+    with qaihm_temp_dir() as tmp_dir:
         download_path = Path(tmp_dir) / "model"
         # Download the model into the temporary directory
         hub_model.download(download_path)  # type: ignore
         size_mb = get_disk_size(download_path, unit="MB")
         return size_mb
```

## qai_hub_models/utils/model_adapters.py

```diff
@@ -12,15 +12,15 @@
 
 def flatten(obj):
     """Flatten nested list or tuple"""
     tgt_type = (list, tuple)  # targeted types
     flattened_list = []
     for item in obj:
         if isinstance(item, tgt_type):
-            flattened_list.extend(flatten(item, tgt_type))
+            flattened_list.extend(flatten(item))
         else:
             flattened_list.append(item)
     return flattened_list
 
 
 class TorchNumpyAdapter:
     def __init__(self, base_model: torch.jit.ScriptModule | torch.nn.Module):
```

## qai_hub_models/utils/printing.py

```diff
@@ -1,14 +1,14 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from collections import Counter
 from pathlib import Path
-from typing import Any, Dict, List, Optional
+from typing import Any, Dict, List, Optional, Union
 
 import numpy as np
 import qai_hub as hub
 from prettytable import PrettyTable
 from qai_hub.client import DatasetEntries, SourceModelType
 from tabulate import tabulate
 
@@ -16,25 +16,44 @@
 from qai_hub_models.utils.compare import METRICS_FUNCTIONS, generate_comparison_metrics
 from qai_hub_models.utils.config_loaders import QAIHMModelPerf
 from qai_hub_models.utils.qnn_helpers import is_qnn_hub_model
 
 _INFO_DASH = "-" * 60
 
 
+def print_with_box(data: List[str]) -> None:
+    """
+    Print input list with box around it as follows
+    +-----------------------------+
+    | list data 1                 |
+    | list data 2 that is longest |
+    | data                        |
+    +-----------------------------+
+    """
+    size = max(len(line) for line in data)
+    size += 2
+    print("+" + "-" * size + "+")
+    for line in data:
+        print("| {:<{}} |".format(line, size - 2))
+    print("+" + "-" * size + "+")
+
+
 def print_inference_metrics(
     inference_job: hub.InferenceJob,
     inference_result: DatasetEntries,
     torch_out: List[np.ndarray],
     outputs_to_skip: Optional[List[int]] = None,
+    output_names: Optional[List[str]] = None,
     metrics: str = "psnr",
 ) -> None:
+    if output_names is None:
+        output_names = list(inference_result.keys())
     inference_data = [
-        np.concatenate(outputs, axis=0) for outputs in inference_result.values()
+        np.concatenate(inference_result[out_name], axis=0) for out_name in output_names
     ]
-    output_names = list(inference_result.keys())
     df_eval = generate_comparison_metrics(
         torch_out, inference_data, names=output_names, metrics=metrics
     )
     for output_idx in outputs_to_skip or []:
         if output_idx < len(output_names):
             df_eval = df_eval.drop(output_names[output_idx])
 
@@ -74,15 +93,15 @@
     print(f"Performance results on-device for {profile_job.name.title()}.")
     print(_INFO_DASH)
 
     if profile_job.model.model_type == SourceModelType.TFLITE:
         runtime = TargetRuntime.TFLITE
     elif is_qnn_hub_model(profile_job.model):
         runtime = TargetRuntime.QNN
-    elif profile_job.model.model_type == SourceModelType.ORT:
+    elif profile_job.model.model_type in [SourceModelType.ORT, SourceModelType.ONNX]:
         runtime = TargetRuntime.ORT
     else:
         raise NotImplementedError()
 
     print_profile_metrics(
         QAIHMModelPerf.ModelRuntimePerformanceDetails(
             profile_job.model.name,
@@ -124,22 +143,34 @@
     table = PrettyTable(align="l", header=False, border=False, padding_width=0)
     for row in rows:
         table.add_row([row[0], f": {row[1]}"])
     print(table.get_string())
 
 
 def print_on_target_demo_cmd(
-    compile_job: hub.CompileJob, model_folder: Path, device: str
+    compile_job: Union[hub.CompileJob, List[hub.CompileJob]],
+    model_folder: Path,
+    device: str,
 ) -> None:
     """
     Outputs a command that will run a model's demo script via inference job.
     """
-    assert compile_job.wait().success
-    print("\nRun this model on a hosted device on sample data using:")
-    target_model = compile_job.get_target_model()
-    assert target_model is not None
+    if isinstance(compile_job, hub.CompileJob):
+        compile_job = [compile_job]
+
+    target_model_id = []
+    for job in compile_job:
+        assert job.wait().success
+        target_model = job.get_target_model()
+        assert target_model is not None
+        target_model_id.append(target_model.model_id)
+
+    target_model_id_str = ",".join(target_model_id)
+    print(
+        f"\nRun compiled model{'s' if len(target_model_id) > 1 else ''} on a hosted device on sample data using:"
+    )
     print(
         f"python {model_folder / 'demo.py'} "
         "--on-device "
-        f"--hub-model-id {target_model.model_id} "
+        f"--hub-model-id {target_model_id_str} "
         f'--device "{device}"\n'
     )
```

## qai_hub_models/utils/qai_hub_helpers.py

```diff
@@ -2,15 +2,15 @@
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
 from __future__ import annotations
 
 import os
 from pathlib import Path
-from typing import Dict, List
+from typing import Dict, List, Optional
 
 import numpy as np
 import qai_hub as hub
 from qai_hub.client import APIException, UserError
 
 from qai_hub_models.utils.asset_loaders import ASSET_CONFIG
 from qai_hub_models.utils.base_model import TargetRuntime
@@ -85,15 +85,15 @@
     skip_inferencing: bool,
     skip_downloading: bool,
     skip_summary: bool,
     output_path: str | Path,
     target_runtime: TargetRuntime,
     compile_options: str,
     profile_options: str,
-    components: List[str] | None = None,
+    components: Optional[List[str]] = None,
 ) -> List[str]:
     print(_WARNING_DASH)
     print(
         f"Unable to find a valid API token for {_AIHUB_NAME}. Using results from a previous job run on the same device.\n"
         f"To get access to the complete experience, please sign-up for access at {_AIHUB_URL}."
     )
     print(_WARNING_DASH)
```

## qai_hub_models/utils/quantization_aimet.py

```diff
@@ -27,54 +27,64 @@
         "AIMET must be installed to load quantized models. "
         "AIMET is only supported on Linux. "
         "Install AIMET via the instructions here: "
         "https://quic.github.io/aimet-pages/releases/latest/install/index.html"
     )
 
 import shutil
-import tempfile
 from pathlib import Path
 from typing import Any, Dict, List, Optional, Tuple
-from zipfile import ZipFile
+from zipfile import ZIP_DEFLATED, ZipFile
 
 import aimet_torch.elementwise_ops as aimet_ops
 import torch
 import torch.nn.modules as nn
-from qai_hub.client import DatasetEntries
+from onnx import load_model as load_onnx_model
+from onnx import save_model as save_onnx_model
+from qai_hub.client import DatasetEntries, Device
 
 from qai_hub_models.evaluators.base_evaluators import _DataLoader, _for_each_batch
 from qai_hub_models.models._shared.common import apply_module_function_recursively
 from qai_hub_models.models.common import SourceModelFormat, TargetRuntime
 from qai_hub_models.models.protocols import (
     PretrainedHubModelProtocol,
     QuantizableModelProtocol,
 )
+from qai_hub_models.utils.asset_loaders import qaihm_temp_dir
 from qai_hub_models.utils.input_spec import InputSpec, make_torch_inputs
 
 
 def _should_tie_observers(op: torch.nn.Module) -> bool:
     """
     Determine whether the input and output observers of this op should be tied.
     """
     if not hasattr(op, "_module_to_wrap"):
         return False
     wrapped_op = op._module_to_wrap
-    op_types_to_tie = [nn.MaxPool2d, nn.AvgPool2d, nn.Upsample, aimet_ops.Concat]
+    op_types_to_tie = [
+        nn.MaxPool2d,
+        nn.AvgPool2d,
+        nn.Upsample,
+        aimet_ops.Concat,
+        aimet_ops.Interpolate,
+    ]
     for op_type in op_types_to_tie:
         if isinstance(wrapped_op, op_type):
             return True
     return False
 
 
-def _get_observer_module_name(modules: Dict[str, Any], name: str) -> Optional[str]:
-    module = modules.get(name)
+def _get_observer_module_name(modules: Dict[str, Any], target: Any) -> Optional[str]:
+    if not isinstance(target, str):
+        return None
+    module = modules.get(target)
     if isinstance(module, QcQuantizeWrapper):
-        return name
+        return target
     elif isinstance(module, aimet_ops.CustomSiLU):
-        return name + ".mul"
+        return target + ".mul"
     return None
 
 
 def _tie_quantizer_deps(
     quantizer_deps: Dict[str, List[str]], modules: Dict[str, torch.nn.Module]
 ) -> None:
     """
@@ -136,15 +146,21 @@
             # Keep going up the tree until you find something with an observer.
             # If one of these nodes has multiple inputs, this may behave incorrectly.
             while (
                 observer_module_name := _get_observer_module_name(
                     modules, input_node.target
                 )
             ) is None:
+                if input_node.target == getattr:
+                    # If the input node is getting a tensor attribute (e.g. shape)
+                    # No observers need to be tied
+                    break
                 input_node = input_node.all_input_nodes[0]
+            if input_node.target == getattr or observer_module_name is None:
+                continue
             if observer_module_name not in quantizer_deps:
                 quantizer_deps[observer_module_name] = []
             quantizer_deps[observer_module_name].append(node.target)
             quantizer_deps[node.target].append(observer_module_name)
     _tie_quantizer_deps(quantizer_deps, modules)
 
 
@@ -311,15 +327,15 @@
         if not input_spec:
             input_spec = self.get_input_spec()
 
         os.makedirs(output_dir, exist_ok=True)
         zip_path = os.path.join(output_dir, f"{model_name}.aimet.zip")
         base_dir = Path(f"{model_name}.aimet")
 
-        with tempfile.TemporaryDirectory() as tmpdir:
+        with qaihm_temp_dir() as tmpdir:
             base_path = Path(tmpdir) / base_dir
             os.makedirs(base_path)
             self.quant_sim.export(
                 str(base_path),
                 model_name,
                 tuple(make_torch_inputs(input_spec)),
                 export_to_torchscript=True,
@@ -339,63 +355,91 @@
         return zip_path
 
     def convert_to_onnx_and_aimet_encodings(
         self,
         output_dir: str | Path,
         input_spec: InputSpec | None = None,
         model_name: str | None = None,
+        external_weights: bool = False,
+        output_names: Optional[List[str]] = None,
     ) -> str:
         """
         Converts the torch module to a zip file containing an
         unquantized ONNX model and an aimet quantization encodings file.
         """
         if model_name is None:
             model_name = self.__class__.__name__
         if not input_spec:
             input_spec = self.get_input_spec()
 
         os.makedirs(output_dir, exist_ok=True)
         zip_path = os.path.join(output_dir, f"{model_name}.aimet.zip")
         base_dir = Path(f"{model_name}.aimet")
 
-        with tempfile.TemporaryDirectory() as tmpdir:
+        with qaihm_temp_dir() as tmpdir:
             base_path = Path(tmpdir) / base_dir
             if base_path.exists():
                 shutil.rmtree(base_path)
             os.makedirs(base_path)
 
             onnx_utils.EXPORT_TO_ONNX_DIRECT = self.needs_onnx_direct_aimet_export
+
             self.quant_sim.export(
                 str(base_path),
                 model_name,
                 tuple(make_torch_inputs(input_spec)),
-                onnx_export_args=dict(input_names=[name for name in input_spec]),
+                onnx_export_args=dict(
+                    input_names=[name for name in input_spec], output_names=output_names
+                ),
             )
-
             onnx_file_name = f"{model_name}.onnx"
             encodings_file_name = f"{model_name}.encodings"
-            with ZipFile(zip_path, "w") as zip_object:
+            external_weights_file_name = f"{model_name}.data"
+
+            if external_weights:
+                # Torch exports to onnx with external weights scattered in a directory.
+                # Save ONNX model with weights to one file.
+                onnx_file_path = str(base_path / onnx_file_name)
+                onnx_model = load_onnx_model(onnx_file_path)
+                save_onnx_model(
+                    onnx_model,
+                    str(onnx_file_path),
+                    save_as_external_data=True,
+                    all_tensors_to_one_file=True,
+                    location=external_weights_file_name,
+                )
+
+            # compresslevel defines how fine compression should run
+            # higher the level, heavier algorithm is used leading to more time.
+            # For large models, higher compression takes longer time to compress.
+            with ZipFile(zip_path, "w", ZIP_DEFLATED, compresslevel=4) as zip_object:
                 zip_object.write(base_path, base_dir)
+
                 zip_object.write(
                     base_path / onnx_file_name, os.path.join(base_dir, onnx_file_name)
                 )
+                if external_weights:
+                    zip_object.write(
+                        base_path / external_weights_file_name,
+                        os.path.join(base_dir, external_weights_file_name),
+                    )
                 zip_object.write(
                     base_path / encodings_file_name,
                     os.path.join(base_dir, encodings_file_name),
                 )
 
         return zip_path
 
     def convert_to_torchscript(
         self, input_spec: InputSpec | None = None, check_trace: bool = True
     ) -> Any:
         if not input_spec:
             input_spec = self.get_input_spec()
 
-        with tempfile.TemporaryDirectory() as tempdir:
+        with qaihm_temp_dir() as tempdir:
             self.quant_sim.export(
                 tempdir,
                 "model",
                 tuple(make_torch_inputs(input_spec)),
                 export_to_torchscript=True,
                 use_embedded_encodings=True,
             )
@@ -411,20 +455,27 @@
         """
         if not input_spec:
             input_spec = self.get_input_spec()
         inputs = make_torch_inputs(input_spec)
         return {k: v.numpy() for k, v in zip(input_spec.keys(), inputs)}
 
     def get_hub_compile_options(
-        self, target_runtime: TargetRuntime, other_compile_options: str = ""
+        self,
+        target_runtime: TargetRuntime,
+        other_compile_options: str = "",
+        device: Optional[Device] = None,
     ) -> str:
         compile_options = super().get_hub_compile_options(  # type: ignore
-            target_runtime, other_compile_options
+            target_runtime, other_compile_options, device
         )
-        return compile_options + " --quantize_full_type int8 --quantize_io"
+        compile_options = compile_options + " --quantize_full_type int8"
+        if target_runtime != TargetRuntime.ORT:
+            # TODO(#10896): Restore quantize_io flag when targeting ORT
+            compile_options = compile_options + " --quantize_io"
+        return compile_options
 
     def preferred_hub_source_model_format(
         self, target_runtime: TargetRuntime
     ) -> SourceModelFormat:
         return SourceModelFormat.ONNX
```

## qai_hub_models/utils/scorecard/common.py

```diff
@@ -1,36 +1,263 @@
 # ---------------------------------------------------------------------
 # Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
 # SPDX-License-Identifier: BSD-3-Clause
 # ---------------------------------------------------------------------
+import os
+from enum import Enum
+from typing import Dict, List, Optional, Tuple
+
 import qai_hub as hub
 
-SCORECARD_DEVICE_NAME_TO_CHIPSET_NAME = {
-    "s23": "qualcomm-snapdragon-8gen2",
-    "s24": "qualcomm-snapdragon-8gen3",
-    "6490": "qualcomm-qcs6490",
-    "8250": "qualcomm-qcs8250",
-    "8550": "qualcomm-qcs8550",
-}
-
-
-SCORECARD_DEVICE_NAME_TO_CHIPSET = {
-    device: f"chipset:{chipset}"
-    for device, chipset in SCORECARD_DEVICE_NAME_TO_CHIPSET_NAME.items()
-}
+from qai_hub_models.models.common import TargetRuntime
+
+_DEVICE_CACHE: Dict[str, hub.Device] = {}
 
 
-def __get_device(device_name) -> hub.Device:
+def _get_cached_device(device_name: str) -> hub.Device:
     # Gets a device with attributes & OS. This only comes from hub.get_devices()
-    for device in hub.get_devices():
-        if device.name == device_name:
-            return device
-    raise ValueError(f"No device named {device_name}")
+    device = _DEVICE_CACHE.get(device_name, None)
+    if not device:
+        device = hub.get_devices(device_name)[0]
+        _DEVICE_CACHE[device_name] = device
+    return device
+
+
+class ScorecardDevice(Enum):
+    any = 0  # no specific device (usable only during compilation)
+
+    # cs == chipset
+    cs_8_gen_2 = 1
+    cs_8_gen_3 = 2
+    cs_6490 = 3
+    cs_8250 = 4
+    cs_8550 = 5
+    cs_x_elite = 6
+
+    def enabled(self) -> bool:
+        valid_test_devices = os.environ.get("WHITELISTED_PROFILE_TEST_DEVICES", "ALL")
+        return (
+            valid_test_devices == "ALL"
+            or self == ScorecardDevice.any
+            or self.name in valid_test_devices.split(",")
+        )
+
+    def all_enabled(self) -> List["ScorecardDevice"]:
+        return [x for x in ScorecardDevice if x.enabled()]
+
+    def get_reference_device(self) -> hub.Device:
+        if self in [ScorecardDevice.cs_8_gen_2, ScorecardDevice.any]:
+            return _get_cached_device("Samsung Galaxy S23")
+        if self == ScorecardDevice.cs_8_gen_3:
+            return _get_cached_device("Samsung Galaxy S24")
+        if self == ScorecardDevice.cs_6490:
+            return _get_cached_device("RB3 Gen 2 (Proxy)")
+        if self == ScorecardDevice.cs_8250:
+            return _get_cached_device("RB5 (Proxy)")
+        if self == ScorecardDevice.cs_8550:
+            return _get_cached_device("QCS8550 (Proxy)")
+        if self == ScorecardDevice.cs_x_elite:
+            return _get_cached_device("Snapdragon X Elite CRD")
+        raise NotImplementedError(f"No reference device for {self.name}")
+
+    def get_chipset(self) -> str:
+        if self in [ScorecardDevice.cs_8_gen_2, ScorecardDevice.any]:
+            return "qualcomm-snapdragon-8gen2"
+        if self == ScorecardDevice.cs_8_gen_3:
+            return "qualcomm-snapdragon-8gen3"
+        if self == ScorecardDevice.cs_6490:
+            return "qualcomm-qcs6490"
+        if self == ScorecardDevice.cs_8250:
+            return "qualcomm-qcs8250"
+        if self == ScorecardDevice.cs_8550:
+            return "qualcomm-qcs8550"
+        if self == ScorecardDevice.cs_x_elite:
+            return "qualcomm-snapdragon-x-elite"
+        raise NotImplementedError(f"No chipset for {self.name}")
+
+    def get_os(self) -> str:
+        for attr in self.get_reference_device().attributes:
+            if attr.startswith("os:"):
+                return attr[3:]
+        raise ValueError(f"OS Not found for device: {self.name}")
+
+
+class ScorecardCompilePath(Enum):
+    TFLITE = 0
+    QNN = 1
+    ORT = 2
+
+    def __str__(self):
+        return self.name.lower()
+
+    @property
+    def long_name(self):
+        return f"torchscript_onnx_{self.name.lower()}"
+
+    def enabled(self) -> bool:
+        valid_test_runtimes = os.environ.get("WHITELISTED_TEST_RUNTIMES", "ALL")
+        return valid_test_runtimes == "ALL" or (
+            self.get_runtime().name.lower()
+            in [x.lower() for x in valid_test_runtimes.split(",")]
+        )
+
+    @staticmethod
+    def all_enabled() -> List["ScorecardCompilePath"]:
+        return [x for x in ScorecardCompilePath if x.enabled()]
+
+    @staticmethod
+    def get_parameterized_test_config(
+        aimet_model=False,
+        only_enabled_paths=True,
+        only_enabled_devices=True,
+    ) -> List[Tuple["ScorecardCompilePath", ScorecardDevice]]:
+        path_list: List[ScorecardCompilePath] = ScorecardCompilePath.all_enabled() if only_enabled_paths else ScorecardCompilePath  # type: ignore
+        path_devices_dict = {
+            sc_path: sc_path.get_test_devices(aimet_model, only_enabled_devices)
+            for sc_path in path_list
+        }
+        return [
+            (key, dev) for key, devices in path_devices_dict.items() for dev in devices
+        ]
+
+    def get_runtime(self) -> TargetRuntime:
+        if self == ScorecardCompilePath.TFLITE:
+            return TargetRuntime.TFLITE
+        if self == ScorecardCompilePath.ORT:
+            return TargetRuntime.ORT
+        if self == ScorecardCompilePath.QNN:
+            return TargetRuntime.QNN
+        raise NotImplementedError()
+
+    def get_test_devices(
+        self, aimet_model=False, only_enabled=True
+    ) -> List[ScorecardDevice]:
+        if self == ScorecardCompilePath.QNN:
+            devices = [ScorecardDevice.any, ScorecardDevice.cs_x_elite]
+        else:
+            devices = [ScorecardDevice.any]
+
+        return [x for x in devices if x.enabled()] if only_enabled else devices
+
+    def get_compile_options(self, aimet_model=False) -> str:
+        if aimet_model and self.get_runtime() == TargetRuntime.ORT:
+            # TODO(#10896): Restore quantize_io flag to
+            # the default set of flags used to target ORT.
+            # This flag can be removed when that happens.
+            return "--quantize_io"
+        return ""
+
+    def get_job_cache_name(
+        self,
+        model: str,
+        device: ScorecardDevice = ScorecardDevice.any,
+        component: Optional[str] = None,
+    ):
+        if device not in self.get_test_devices():
+            device = ScorecardDevice.any  # default to the "generic" compilation path
+        return f"{model}_{self.name}{'-' + device.name if device != ScorecardDevice.any else ''}{'_' + component if component else ''}"
+
+
+class ScorecardProfilePath(Enum):
+    TFLITE = 0
+    QNN = 1
+    ORT = 2
+    ORT_DML_GPU = 3
+
+    def __str__(self):
+        return self.name.lower()
+
+    @property
+    def long_name(self):
+        return f"torchscript_onnx_{self.name.lower()}"
+
+    def enabled(self) -> bool:
+        valid_test_runtimes = os.environ.get("WHITELISTED_TEST_RUNTIMES", "ALL")
+        return valid_test_runtimes == "ALL" or (
+            self.get_runtime().name.lower()
+            in [x.lower() for x in valid_test_runtimes.split(",")]
+        )
+
+    @staticmethod
+    def all_enabled() -> List["ScorecardProfilePath"]:
+        return [x for x in ScorecardProfilePath if x.enabled()]
+
+    @staticmethod
+    def get_parameterized_test_config(
+        aimet_model=False,
+        only_enabled_paths=True,
+        only_enabled_devices=True,
+    ) -> List[Tuple["ScorecardProfilePath", ScorecardDevice]]:
+        path_list: List[ScorecardProfilePath] = ScorecardProfilePath.all_enabled() if only_enabled_paths else ScorecardProfilePath  # type: ignore
+        path_devices_dict = {
+            sc_path: sc_path.get_test_devices(aimet_model, only_enabled_devices)
+            for sc_path in path_list
+        }
+        return [
+            (key, dev) for key, devices in path_devices_dict.items() for dev in devices
+        ]
+
+    def get_runtime(self) -> TargetRuntime:
+        if self == ScorecardProfilePath.TFLITE:
+            return TargetRuntime.TFLITE
+        if self in [ScorecardProfilePath.ORT, ScorecardProfilePath.ORT_DML_GPU]:
+            return TargetRuntime.ORT
+        if self == ScorecardProfilePath.QNN:
+            return TargetRuntime.QNN
+        raise NotImplementedError()
+
+    def get_compile_path(self) -> ScorecardCompilePath:
+        if self == ScorecardProfilePath.TFLITE:
+            return ScorecardCompilePath.TFLITE
+        if self in [ScorecardProfilePath.ORT, ScorecardProfilePath.ORT_DML_GPU]:
+            return ScorecardCompilePath.ORT
+        if self == ScorecardProfilePath.QNN:
+            return ScorecardCompilePath.QNN
+        raise NotImplementedError()
+
+    def get_profile_options(self) -> str:
+        if self == ScorecardProfilePath.ORT_DML_GPU:
+            return "--compute_unit gpu"
+        return ""
+
+    def get_test_devices(
+        self, aimet_model=False, only_enabled=True
+    ) -> List[ScorecardDevice]:
+        if self == ScorecardProfilePath.TFLITE:
+            devices = [
+                ScorecardDevice.cs_8_gen_2,
+                ScorecardDevice.cs_8_gen_3,
+                ScorecardDevice.cs_8550,
+            ] + (
+                [ScorecardDevice.cs_6490, ScorecardDevice.cs_8250]
+                if aimet_model
+                else []
+            )
+        elif self == ScorecardProfilePath.ORT:
+            devices = [
+                ScorecardDevice.cs_8_gen_2,
+                ScorecardDevice.cs_8_gen_3,
+                ScorecardDevice.cs_x_elite,
+            ]
+        elif self == ScorecardProfilePath.QNN:
+            devices = [
+                ScorecardDevice.cs_8_gen_2,
+                ScorecardDevice.cs_8_gen_3,
+                ScorecardDevice.cs_x_elite,
+                ScorecardDevice.cs_8550,
+            ] + ([ScorecardDevice.cs_6490] if aimet_model else [])
+        elif self == ScorecardProfilePath.ORT_DML_GPU:
+            devices = [ScorecardDevice.cs_x_elite]
+        else:
+            raise NotImplementedError()
 
+        return [x for x in devices if x.enabled()] if only_enabled else devices
 
-REFERENCE_DEVICE_PER_SUPPORTED_CHIPSETS = {
-    "qualcomm-snapdragon-8gen2": __get_device("Samsung Galaxy S23"),
-    "qualcomm-snapdragon-8gen3": __get_device("Samsung Galaxy S24"),
-    "qualcomm-qcs6490": __get_device("RB3 Gen 2 (Proxy)"),
-    "qualcomm-qcs8250": __get_device("RB5 (Proxy)"),
-    "qualcomm-qcs8550": __get_device("QCS8550 (Proxy)"),
-}
+    def get_job_cache_name(
+        self,
+        model: str,
+        device: ScorecardDevice,
+        component: Optional[str] = None,
+    ):
+        return (
+            f"{model}_{self.name}-{device.name}{'_' + component if component else ''}"
+        )
```

## qai_hub_models/utils/scorecard/job_summary.py

```diff
@@ -4,31 +4,30 @@
 # ---------------------------------------------------------------------
 from dataclasses import dataclass
 from functools import cached_property
 from typing import Any, Dict, List, Optional, Type, Union, cast
 
 import qai_hub as hub
 
-from qai_hub_models.models.common import TargetRuntime
 from qai_hub_models.utils.config_loaders import QAIHMModelCodeGen, QAIHMModelInfo
 from qai_hub_models.utils.scorecard.common import (
-    REFERENCE_DEVICE_PER_SUPPORTED_CHIPSETS,
-    SCORECARD_DEVICE_NAME_TO_CHIPSET_NAME,
+    ScorecardCompilePath,
+    ScorecardDevice,
+    ScorecardProfilePath,
 )
 
 
 @dataclass
 class JobSummary:
     model_id: str
     job_id: Optional[str]
-    runtime: TargetRuntime
+    _device: ScorecardDevice
 
     def __post_init__(self):
         assert self.model_id
-        assert self.runtime
         # Verify Job Exists
         if self.job_id:
             assert self.job
 
     @classmethod
     def from_model_id(
         cls: Type["JobSummary"], model_id: str, job_ids: Dict[str, str]
@@ -96,14 +95,16 @@
             or self.model_id.endswith("Quantizable")
             else "No"
         )
 
 
 @dataclass
 class CompileJobSummary(JobSummary):
+    path: ScorecardCompilePath
+
     @classmethod
     def from_model_id(
         cls: Type["CompileJobSummary"], model_id: str, job_ids: Dict[str, str]
     ) -> List["CompileJobSummary"]:
         """
         Reads jobs for `model_id` from the dictionary and creates summaries for each. `job_ids` format:
         Either:
@@ -119,33 +120,33 @@
         components = []
 
         if model_code_gen.components:
             if model_code_gen.default_components:
                 components = model_code_gen.default_components
             else:
                 components = list(model_code_gen.components.keys())
+        else:
+            components.append(None)  # type: ignore
 
-        for runtime in TargetRuntime:
-            if not components:
-                model_runs.append(
-                    cls(
-                        model_id=model_info.name,
-                        job_id=job_ids.get(f"{model_id}_{runtime.name}", None),
-                        runtime=runtime,
-                    )
-                )
-            else:
-                for component in components:
+        path: ScorecardCompilePath
+        for path in ScorecardCompilePath.all_enabled():
+            for component in components:
+                for device in path.get_test_devices(model_code_gen.is_aimet):
                     model_runs.append(
                         cls(
-                            model_id=component,
+                            model_id=component or model_info.name,
                             job_id=job_ids.get(
-                                f"{model_id}_{runtime.name}_{component}", None
+                                path.get_job_cache_name(
+                                    model=model_id,
+                                    device=device,
+                                    component=component,
+                                )
                             ),
-                            runtime=runtime,
+                            path=path,
+                            _device=device,
                         )
                     )
 
         return model_runs
 
     def __post_init__(self):
         super().__post_init__()
@@ -158,15 +159,15 @@
         if self.job:
             return None
         return cast(hub.CompileJob, self.job)
 
 
 @dataclass
 class ProfileJobSummary(JobSummary):
-    _chipset: str
+    path: ScorecardProfilePath
 
     @classmethod
     def from_model_id(
         cls: Type["ProfileJobSummary"], model_id: str, job_ids: Dict[str, str]
     ) -> List["ProfileJobSummary"]:
         """
         Reads jobs for `model_id` from the dictionary and creates summaries for each. `job_ids` format:
@@ -183,75 +184,61 @@
         components = []
 
         if model_code_gen.components:
             if model_code_gen.default_components:
                 components = model_code_gen.default_components
             else:
                 components = list(model_code_gen.components.keys())
+        else:
+            components.append(None)  # type: ignore
 
-        for runtime in TargetRuntime:
-            for device, chipset in SCORECARD_DEVICE_NAME_TO_CHIPSET_NAME.items():
-                run_dev = f"{runtime.name}-{device}"
-                if not components:
-                    if (job_id := job_ids.get(f"{model_id}_{run_dev}", None)) is None:
-                        continue
+        path: ScorecardProfilePath
+        for path in ScorecardProfilePath.all_enabled():
+            for component in components:
+                for device in path.get_test_devices(model_code_gen.is_aimet):
                     model_runs.append(
                         cls(
-                            model_id=model_info.name,
-                            job_id=job_id,
-                            runtime=runtime,
-                            _chipset=chipset,
+                            model_id=component or model_info.name,
+                            job_id=job_ids.get(
+                                path.get_job_cache_name(
+                                    model=model_id,
+                                    device=device,
+                                    component=component,
+                                ),
+                                None,
+                            ),
+                            _device=device,
+                            path=path,
                         )
                     )
-                else:
-                    for component in components:
-                        if (
-                            job_id := job_ids.get(
-                                f"{model_id}_{run_dev}_{component}", None
-                            )
-                        ) is None:
-                            continue
-                        model_runs.append(
-                            cls(
-                                model_id=component,
-                                job_id=job_id,
-                                runtime=runtime,
-                                _chipset=chipset,
-                            )
-                        )
 
         return model_runs
 
     def __post_init__(self):
         super().__post_init__()
-        assert self.chipset in REFERENCE_DEVICE_PER_SUPPORTED_CHIPSETS
         if not self.skipped:
             assert isinstance(self.job, hub.ProfileJob)
             if self._job_status.success:
                 assert self.profile_results
 
     @cached_property
     def chipset(self) -> str:
         """Chipset the job was run on."""
         if not self.job:
-            return self._chipset
+            return self._device.get_chipset()
 
         hub_device = self.job.device
         for attr in hub_device.attributes:
             if attr.startswith("chipset:"):
                 return attr.split(":")[1]
         raise ValueError("No chipset found.")
 
     @cached_property
     def device(self) -> hub.Device:
-        return (
-            self.job.device
-            if self.job
-            else REFERENCE_DEVICE_PER_SUPPORTED_CHIPSETS[self.chipset]
-        )
+        return self.job.device if self.job else self._device.get_reference_device()
 
     @cached_property
     def profile_job(self) -> Optional[hub.ProfileJob]:
         """Get the hub.CompileJob object."""
         if not self.job:
             return None
         return cast(hub.ProfileJob, self.job)
```

## qai_hub_models/utils/scorecard/model_card.py

```diff
@@ -9,18 +9,19 @@
 import multiprocessing
 import pprint
 from dataclasses import dataclass
 from typing import Any, Dict, List, Set, Tuple, Union
 
 import qai_hub as hub
 
-from qai_hub_models.models.common import TargetRuntime
 from qai_hub_models.utils.config_loaders import MODEL_IDS
 from qai_hub_models.utils.scorecard.common import (
-    REFERENCE_DEVICE_PER_SUPPORTED_CHIPSETS,
+    ScorecardCompilePath,
+    ScorecardDevice,
+    ScorecardProfilePath,
 )
 from qai_hub_models.utils.scorecard.job_summary import (
     CompileJobSummary,
     ProfileJobSummary,
 )
 
 
@@ -123,17 +124,18 @@
 
 
 # Caching this information is helpful because it requires pulling data from hub.
 # Pulling data from hub is slow.
 __REFERENCE_DEVICE_INFO_PER_CHIPSET = {}
 
 
-def get_reference_device_info(chipset: str) -> Dict[str, str]:
+def get_reference_device_info(device: ScorecardDevice) -> Dict[str, str]:
+    chipset = device.get_chipset()
     if chipset not in __REFERENCE_DEVICE_INFO_PER_CHIPSET:
-        hub_device = REFERENCE_DEVICE_PER_SUPPORTED_CHIPSETS[chipset]
+        hub_device = device.get_reference_device()
         device_name = hub_device.name
         os_version = hub_device.os
         os_name, form_factor, manufacturer = "", "", ""
         for attr in hub_device.attributes:
             if attr.startswith("vendor"):
                 manufacturer = attr.split(":")[-1]
             if attr.startswith("format"):
@@ -149,69 +151,69 @@
             manufacturer=manufacturer.capitalize(),
             chipset=chipset,
         )
     return __REFERENCE_DEVICE_INFO_PER_CHIPSET[chipset]
 
 
 @dataclass
-class ChipsetPerfSummary:
-    chipset_name: str
-    run_per_runtime: Dict[TargetRuntime, ProfileJobSummary]  # Map<Runtime, Summary>
+class DevicePerfSummary:
+    device: ScorecardDevice
+    run_per_path: Dict[ScorecardProfilePath, ProfileJobSummary]  # Map<path, Summary>
 
     @staticmethod
-    def from_runs(chipset_name: str, runtime_runs: List[ProfileJobSummary]):
+    def from_runs(device: ScorecardDevice, path_runs: List[ProfileJobSummary]):
         # Figure out unique devices in various baselines
-        run_per_runtime: Dict[TargetRuntime, ProfileJobSummary] = {}
-        for run in runtime_runs:
-            assert run.chipset == chipset_name  # Chipset should match
-            run_per_runtime[run.runtime] = run
+        run_per_path: Dict[ScorecardProfilePath, ProfileJobSummary] = {}
+        for run in path_runs:
+            assert run._device == device  # Device should match
+            run_per_path[run.path] = run
 
-        return ChipsetPerfSummary(chipset_name, run_per_runtime)
+        return DevicePerfSummary(device, run_per_path)
 
     def get_perf_card(self) -> Dict[str, str | Dict[str, str]]:
         perf_card: Dict[str, str | Dict[str, str]] = {}
-        for runtime, run in self.run_per_runtime.items():
+        for path, run in self.run_per_path.items():
             if not run.skipped:  # Skipped runs are not included
-                perf_card[runtime.long_name] = run.performance_metrics
-        perf_card["reference_device_info"] = get_reference_device_info(
-            self.chipset_name
-        )
+                perf_card[path.long_name] = run.performance_metrics
+        perf_card["reference_device_info"] = get_reference_device_info(self.device)
         perf_card["timestamp"] = datetime.datetime.utcnow().isoformat() + "Z"
         return perf_card
 
     def __repr__(self) -> str:
         return pprint.pformat(self.get_perf_card())
 
 
 @dataclass
 class ModelPerfSummary:
     model_id: str
-    runs_per_chipset: Dict[str, ChipsetPerfSummary]  # Map<Device Name, Summary>
+    runs_per_device: Dict[
+        ScorecardDevice, DevicePerfSummary
+    ]  # Map<Device Name, Summary>
 
     @staticmethod
     def from_runs(model_id: str, device_runs: List[ProfileJobSummary]):
         # Figure out unique devices in various baselines
-        runs_per_chipset: Dict[str, List[ProfileJobSummary]] = {}
+        runs_per_device: Dict[ScorecardDevice, List[ProfileJobSummary]] = {}
         for run in device_runs:
             assert run.model_id == model_id  # All should have the same model ID
-            list = runs_per_chipset.get(run.chipset or "", [])
-            runs_per_chipset[run.chipset] = list
+            list = runs_per_device.get(run._device, [])
+            runs_per_device[run._device] = list
             list.append(run)
 
         return ModelPerfSummary(
             model_id,
             {
-                chipset_name: ChipsetPerfSummary.from_runs(chipset_name, runs)
-                for chipset_name, runs in runs_per_chipset.items()
+                device: DevicePerfSummary.from_runs(device, runs)
+                for device, runs in runs_per_device.items()
             },
         )
 
     def get_perf_card(self) -> List[Dict[str, Union[str, Dict[str, str]]]]:
         perf_card = []
-        for summary in self.runs_per_chipset.values():
+        for summary in self.runs_per_device.values():
             perf_card.append(summary.get_perf_card())
         return perf_card
 
     def __repr__(self):
         return pprint.pformat(self.get_perf_card())
 
 
@@ -222,16 +224,16 @@
     @staticmethod
     def from_model_ids(
         job_ids: Dict[str, str], model_ids=MODEL_IDS
     ) -> Dict[str, PerfSummary]:
         """
         Reads jobs for every `model_id` from the dictionary and creates summaries for each. `job_ids` format:
         Either:
-            <model_id>|<runtime>|<device>|<model_component_id> : job_id
-            <model_id>|<runtime>|<device> : job_id
+            <model_id>_<path>-<device>_<model_component_id> : job_id
+            <model_id>_<path>-<device> : job_id
 
         Returns models in this format:
             model_id: List[Summary]
         """
         print("Generating Performance Summary for Models")
         pool = multiprocessing.Pool(processes=15)
         model_summaries = pool.map(
@@ -244,16 +246,16 @@
     @staticmethod
     def from_model_id(
         model_id: str, job_ids: Dict[str, str]
     ) -> Tuple[str, PerfSummary]:
         """
         Reads jobs for every `model_id` from the dictionary and creates summaries for each. `job_ids` format:
         Either:
-            <model_id>|<runtime>|<device>|<model_component_id> : job_id
-            <model_id>|<runtime>|<device> : job_id
+            <model_id>_<path>-<device>_<model_component_id> : job_id
+            <model_id>_<path>-<device> : job_id
 
         Returns models in this format:
             model_id: List[Summary]
         """
         print(f"    {model_id} ")
         runs = ProfileJobSummary.from_model_id(model_id, job_ids)
         return model_id, PerfSummary.from_runs(runs)
@@ -273,15 +275,17 @@
                 for model_id, runs in runs_per_model.items()
             }
         )
 
     def get_chipsets(self) -> Set[str]:
         chips: Set[str] = set()
         for _, model_summary in self.runs_per_model.items():
-            chips.update(model_summary.runs_per_chipset.keys())
+            chips.update(
+                [x.get_chipset() for x in model_summary.runs_per_device.keys()]
+            )
         return chips
 
     def get_perf_card(self) -> Dict[str, str | List[Any] | Dict[str, Any]]:
         perf_card: Dict[str, str | List[Any] | Dict[str, Any]] = {}
 
         chips = self.get_chipsets()
         perf_card["aggregated"] = dict(
@@ -299,42 +303,68 @@
         return perf_card
 
     def __repr__(self):
         return pprint.pformat(self.get_perf_card())
 
 
 @dataclass
+class DeviceCompileSummary:
+    device: ScorecardDevice
+    run_per_path: Dict[ScorecardCompilePath, CompileJobSummary]  # Map<path, Summary>
+
+    @staticmethod
+    def from_runs(device: ScorecardDevice, path_runs: List[CompileJobSummary]):
+        # Figure out unique devices in various baselines
+        run_per_path: Dict[ScorecardCompilePath, CompileJobSummary] = {}
+        for run in path_runs:
+            assert run._device == device  # Device should match
+            run_per_path[run.path] = run
+
+        return DeviceCompileSummary(device, run_per_path)
+
+
+@dataclass
 class ModelCompileSummary:
     model_id: str
-    runs_per_runtime: Dict[
-        TargetRuntime, CompileJobSummary
+    runs_per_device: Dict[
+        ScorecardDevice, DeviceCompileSummary
     ]  # Map<Device Name, Summary>
 
     @staticmethod
-    def from_runs(model_id: str, runtime_runs: List[CompileJobSummary]):
-        run_per_runtime: Dict[TargetRuntime, CompileJobSummary] = {}
-        for run in runtime_runs:
+    def from_runs(model_id: str, path_runs: List[CompileJobSummary]):
+        runs_per_device: Dict[ScorecardDevice, List[CompileJobSummary]] = {}
+        for run in path_runs:
             assert run.model_id == model_id  # model id should match
-            run_per_runtime[run.runtime] = run
-        return ModelCompileSummary(model_id, run_per_runtime)
+            list = runs_per_device.get(run._device, [])
+            runs_per_device[run._device] = list
+            list.append(run)
+        return ModelCompileSummary(
+            model_id,
+            {
+                device: DeviceCompileSummary.from_runs(device, runs)
+                for device, runs in runs_per_device.items()
+            },
+        )
 
 
 @dataclass
 class CompileSummary:
     runs_per_model: Dict[str, ModelCompileSummary]  # Map<Model ID, Summary>
 
     @staticmethod
     def from_model_ids(
         job_ids: Dict[str, str], model_ids=MODEL_IDS
     ) -> Dict[str, CompileSummary]:
         """
         Reads jobs for every `model_id` from the dictionary and creates summaries for each. `job_ids` format:
         Either:
-            <model_id>|<runtime>|<device>|<model_component_id> : job_id
-            <model_id>|<runtime>|<device> : job_id
+            <model_id>_<runtime>-<device>_<model_component_id> : job_id
+            <model_id>_<runtime>-<device> : job_id
+            <model_id>_<runtime>_<model_component_id> : job_id
+            <model_id>_<runtime> : job_id
 
         Returns models in this format:
             model_id: List[Summary]
         """
         print("Generating Compilation Summary for Models")
         pool = multiprocessing.Pool(processes=15)
         model_summaries = pool.map(
@@ -347,16 +377,18 @@
     @staticmethod
     def from_model_id(
         model_id: str, job_ids: Dict[str, str]
     ) -> Tuple[str, CompileSummary]:
         """
         Reads jobs for every `model_id` from the dictionary and creates summaries for each. `job_ids` format:
         Either:
-            <model_id>|<runtime>|<device>|<model_component_id> : job_id
-            <model_id>|<runtime>|<device> : job_id
+            <model_id>_<runtime>-<device>_<model_component_id> : job_id
+            <model_id>_<runtime>-<device> : job_id
+            <model_id>_<runtime>_<model_component_id> : job_id
+            <model_id>_<runtime> : job_id
 
         Returns models in this format:
             model_id: List[Summary]
         """
         print(f"    {model_id} ")
         runs = CompileJobSummary.from_model_id(model_id, job_ids)
         return model_id, CompileSummary.from_runs(runs)
```

## Comparing `qai_hub_models/models/stable_diffusion_quantized/export.py` & `qai_hub_models/models/stable_diffusion_v1_5_quantized/export.py`

 * *Files 0% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 
 import warnings
 from pathlib import Path
 from typing import Any, Dict, List, Mapping, Optional, Tuple, cast
 
 import qai_hub as hub
 
-from qai_hub_models.models.stable_diffusion_quantized import Model
+from qai_hub_models.models.stable_diffusion_v1_5_quantized import Model
 from qai_hub_models.utils.args import export_parser
 from qai_hub_models.utils.base_model import BasePrecompiledModel, TargetRuntime
 from qai_hub_models.utils.printing import print_profile_metrics_from_job
 from qai_hub_models.utils.qai_hub_helpers import (
     can_access_qualcomm_ai_hub,
     export_without_hub_access,
 )
@@ -70,29 +70,29 @@
             `model_cls.from_precompiled`
 
     Returns:
         A Mapping from component_name to a 2-tuple of:
             * A ProfileJob containing metadata about the profile job (None if profiling skipped).
             * An InferenceJob containing metadata about the inference job (None if inferencing skipped).
     """
-    model_name = "stable_diffusion_quantized"
+    model_name = "stable_diffusion_v1_5_quantized"
     output_path = Path(output_dir or Path.cwd() / "build" / model_name)
     if chipset:
         hub_device = hub.Device(attributes=f"chipset:{chipset}")
     else:
         hub_device = hub.Device(name=device)
     component_arg = components
     components = components or DEFAULT_COMPONENTS
     for component_name in components:
         if component_name not in ALL_COMPONENTS:
             raise ValueError(f"Invalid component {component_name}.")
     if not can_access_qualcomm_ai_hub():
         return export_without_hub_access(
-            "stable_diffusion_quantized",
-            "Stable-Diffusion",
+            "stable_diffusion_v1_5_quantized",
+            "Stable-Diffusion-v1.5",
             device,
             skip_profiling,
             skip_inferencing,
             False,
             skip_summary,
             output_path,
             TargetRuntime.QNN,
```

## Comparing `qai_hub_models/models/stable_diffusion_quantized/info.yaml` & `qai_hub_models/models/stable_diffusion_v1_5_quantized/info.yaml`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-name: Stable-Diffusion
-id: stable_diffusion_quantized
+name: Stable-Diffusion-v1.5
+id: stable_diffusion_v1_5_quantized
 status: public
 headline: State-of-the-art generative AI model used to generate detailed images conditioned
   on text descriptions.
 domain: Generative AI
 description: Generates high resolution images from text prompts using a latent diffusion
   model. This model uses CLIP ViT-L/14 as text encoder, U-Net based latent denoising,
   and VAE based decoder to generate the final image.
@@ -14,24 +14,25 @@
 research_paper: https://arxiv.org/abs/2112.10752
 research_paper_title: High-Resolution Image Synthesis with Latent Diffusion Models
 license: https://github.com/CompVis/stable-diffusion/blob/main/LICENSE
 deploy_license: https://github.com/CompVis/stable-diffusion/blob/main/LICENSE
 source_repo: https://github.com/CompVis/stable-diffusion/tree/main
 technical_details:
   Input: Text prompt to generate image
-  QNN-SDK: '2.19'
+  QNN-SDK: '2.20'
   Text Encoder Number of parameters: 340M
   UNet Number of parameters: 865M
   VAE Decoder Number of parameters: 83M
   Model size: 1GB
 applicable_scenarios:
   - Image Generation
   - Image Editing
   - Content Creation
 related_models:
+  - stable_diffusion_v2_1_quantized
   - controlnet_quantized
 form_factors:
   - Phone
   - Tablet
 has_static_banner: yes
 has_animated_banner: yes
 license_type: creativeml-openrail-m
```

## Comparing `qai_hub_models/models/stable_diffusion_quantized/model.py` & `qai_hub_models/models/stable_diffusion_v1_5_quantized/model.py`

 * *Files 0% similar despite different names*

```diff
@@ -9,15 +9,15 @@
 from qai_hub_models.models.protocols import FromPrecompiledProtocol
 from qai_hub_models.utils.asset_loaders import CachedWebModelAsset
 from qai_hub_models.utils.base_model import BasePrecompiledModel, CollectionModel
 from qai_hub_models.utils.input_spec import InputSpec
 
 MODEL_ID = __name__.split(".")[-2]
 MODEL_ASSET_VERSION = 1
-QNN_SDK_PREFIX = "QNN219"
+QNN_SDK_PREFIX = "QNN220"
 TEXT_ENCODER = os.path.join(QNN_SDK_PREFIX, "text_encoder.serialized.bin")
 UNET_DIFFUSER = os.path.join(QNN_SDK_PREFIX, "unet.serialized.bin")
 VAE_DECODER = os.path.join(QNN_SDK_PREFIX, "vae_decoder.serialized.bin")
 
 
 class StableDiffusionQuantized(FromPrecompiledProtocol, CollectionModel):
     """
```

## Comparing `qai_hub_models/models/stable_diffusion_quantized/perf.yaml` & `qai_hub_models/models/stable_diffusion_v1_5_quantized/perf.yaml`

 * *Files identical despite different names*

## Comparing `qai_hub_models-0.5.1.dist-info/LICENSE` & `qai_hub_models-0.6.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `qai_hub_models-0.5.1.dist-info/METADATA` & `qai_hub_models-0.6.0.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: qai-hub-models
-Version: 0.5.1
+Version: 0.6.0
 Summary: Models optimized for export to run on device.
 Home-page: https://github.com/quic/ai-hub-models
 Author: Qualcomm Technologies, Inc.
 License: BSD-3
 Platform: UNKNOWN
 Requires-Python: >=3.8, <3.11
 Description-Content-Type: text/markdown
@@ -63,15 +63,15 @@
 Requires-Dist: timm ==0.9.11 ; extra == 'detr_resnet50'
 Provides-Extra: detr_resnet50_dc5
 Requires-Dist: transformers ==4.27.4 ; extra == 'detr_resnet50_dc5'
 Requires-Dist: timm ==0.9.11 ; extra == 'detr_resnet50_dc5'
 Provides-Extra: dev
 Requires-Dist: boto3 ==1.34.40 ; extra == 'dev'
 Requires-Dist: botocore ==1.34.40 ; extra == 'dev'
-Requires-Dist: coverage ==6.5.0 ; extra == 'dev'
+Requires-Dist: coverage ==5.3.1 ; extra == 'dev'
 Requires-Dist: imageio[ffmpeg] ==2.31.5 ; extra == 'dev'
 Requires-Dist: jinja2 ==3.0.3 ; extra == 'dev'
 Requires-Dist: mypy ==0.991 ; extra == 'dev'
 Requires-Dist: pre-commit ==3.5.0 ; extra == 'dev'
 Requires-Dist: pytest-cov ==4.1.0 ; extra == 'dev'
 Requires-Dist: pytest-xdist ==3.3.1 ; extra == 'dev'
 Requires-Dist: ruamel-yaml ==0.18.6 ; extra == 'dev'
@@ -208,25 +208,37 @@
 Requires-Dist: scipy ==1.8.1 ; extra == 'real_esrgan_general_x4v3'
 Requires-Dist: seaborn ==0.11.0 ; extra == 'real_esrgan_general_x4v3'
 Requires-Dist: basicsr ==1.4.2 ; extra == 'real_esrgan_general_x4v3'
 Provides-Extra: real_esrgan_x4plus
 Requires-Dist: scipy ==1.8.1 ; extra == 'real_esrgan_x4plus'
 Requires-Dist: seaborn ==0.11.0 ; extra == 'real_esrgan_x4plus'
 Requires-Dist: basicsr ==1.4.2 ; extra == 'real_esrgan_x4plus'
+Provides-Extra: riffusion-quantized
+Requires-Dist: transformers ==4.27.4 ; extra == 'riffusion-quantized'
+Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'riffusion-quantized'
+Provides-Extra: riffusion_quantized
+Requires-Dist: transformers ==4.27.4 ; extra == 'riffusion_quantized'
+Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'riffusion_quantized'
 Provides-Extra: sam
 Requires-Dist: matplotlib ==3.7.4 ; extra == 'sam'
 Requires-Dist: pycocotools ==2.0.7 ; extra == 'sam'
-Provides-Extra: stable-diffusion-quantized
-Requires-Dist: transformers ==4.27.4 ; extra == 'stable-diffusion-quantized'
-Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'stable-diffusion-quantized'
-Provides-Extra: stable_diffusion_quantized
-Requires-Dist: transformers ==4.27.4 ; extra == 'stable_diffusion_quantized'
-Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'stable_diffusion_quantized'
+Provides-Extra: stable-diffusion-v1-5-quantized
+Requires-Dist: transformers ==4.27.4 ; extra == 'stable-diffusion-v1-5-quantized'
+Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'stable-diffusion-v1-5-quantized'
+Provides-Extra: stable-diffusion-v2-1-quantized
+Requires-Dist: transformers ==4.27.4 ; extra == 'stable-diffusion-v2-1-quantized'
+Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'stable-diffusion-v2-1-quantized'
+Provides-Extra: stable_diffusion_v1_5_quantized
+Requires-Dist: transformers ==4.27.4 ; extra == 'stable_diffusion_v1_5_quantized'
+Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'stable_diffusion_v1_5_quantized'
+Provides-Extra: stable_diffusion_v2_1_quantized
+Requires-Dist: transformers ==4.27.4 ; extra == 'stable_diffusion_v2_1_quantized'
+Requires-Dist: diffusers[torch] ==0.21.4 ; extra == 'stable_diffusion_v2_1_quantized'
 Provides-Extra: stylegan2
-Requires-Dist: click ==8.0 ; extra == 'stylegan2'
+Requires-Dist: click ==8.1.7 ; extra == 'stylegan2'
 Provides-Extra: trocr
 Requires-Dist: transformers ==4.27.4 ; extra == 'trocr'
 Requires-Dist: sentencepiece ==0.2.0 ; extra == 'trocr'
 Provides-Extra: whisper-base-en
 Requires-Dist: openai-whisper ==20230314 ; extra == 'whisper-base-en'
 Requires-Dist: scipy ; extra == 'whisper-base-en'
 Provides-Extra: whisper-small-en
@@ -240,53 +252,90 @@
 Requires-Dist: scipy ; extra == 'whisper_base_en'
 Provides-Extra: whisper_small_en
 Requires-Dist: openai-whisper ==20230314 ; extra == 'whisper_small_en'
 Requires-Dist: scipy ==1.8.1 ; extra == 'whisper_small_en'
 Provides-Extra: whisper_tiny_en
 Requires-Dist: openai-whisper ==20230314 ; extra == 'whisper_tiny_en'
 Requires-Dist: scipy ; extra == 'whisper_tiny_en'
+Provides-Extra: yolonas
+Requires-Dist: object-detection-metrics ==0.4.post1 ; extra == 'yolonas'
+Requires-Dist: stringcase ==1.2.0 ; extra == 'yolonas'
+Requires-Dist: rapidfuzz ==3.8.1 ; extra == 'yolonas'
+Requires-Dist: treelib ==1.6.1 ; extra == 'yolonas'
+Requires-Dist: imagesize ==1.4.1 ; extra == 'yolonas'
+Requires-Dist: einops ==0.3.2 ; extra == 'yolonas'
+Requires-Dist: Deprecated ==1.2.11 ; extra == 'yolonas'
+Requires-Dist: data-gradients ==0.3.1 ; extra == 'yolonas'
+Requires-Dist: shapely ==2.0.3 ; extra == 'yolonas'
+Provides-Extra: yolonas-quantized
+Requires-Dist: object-detection-metrics ==0.4.post1 ; extra == 'yolonas-quantized'
+Requires-Dist: stringcase ==1.2.0 ; extra == 'yolonas-quantized'
+Requires-Dist: rapidfuzz ==3.8.1 ; extra == 'yolonas-quantized'
+Requires-Dist: treelib ==1.6.1 ; extra == 'yolonas-quantized'
+Requires-Dist: imagesize ==1.4.1 ; extra == 'yolonas-quantized'
+Requires-Dist: einops ==0.3.2 ; extra == 'yolonas-quantized'
+Requires-Dist: Deprecated ==1.2.11 ; extra == 'yolonas-quantized'
+Requires-Dist: data-gradients ==0.3.1 ; extra == 'yolonas-quantized'
+Requires-Dist: shapely ==2.0.3 ; extra == 'yolonas-quantized'
+Provides-Extra: yolonas_quantized
+Requires-Dist: object-detection-metrics ==0.4.post1 ; extra == 'yolonas_quantized'
+Requires-Dist: stringcase ==1.2.0 ; extra == 'yolonas_quantized'
+Requires-Dist: rapidfuzz ==3.8.1 ; extra == 'yolonas_quantized'
+Requires-Dist: treelib ==1.6.1 ; extra == 'yolonas_quantized'
+Requires-Dist: imagesize ==1.4.1 ; extra == 'yolonas_quantized'
+Requires-Dist: einops ==0.3.2 ; extra == 'yolonas_quantized'
+Requires-Dist: Deprecated ==1.2.11 ; extra == 'yolonas_quantized'
+Requires-Dist: data-gradients ==0.3.1 ; extra == 'yolonas_quantized'
+Requires-Dist: shapely ==2.0.3 ; extra == 'yolonas_quantized'
 Provides-Extra: yolov7
 Requires-Dist: matplotlib ==3.7.4 ; extra == 'yolov7'
 Requires-Dist: object-detection-metrics ==0.4.post1 ; extra == 'yolov7'
 Requires-Dist: scipy ==1.8.1 ; extra == 'yolov7'
 Requires-Dist: seaborn ==0.11.0 ; extra == 'yolov7'
+Requires-Dist: shapely ==2.0.3 ; extra == 'yolov7'
 Provides-Extra: yolov7-quantized
 Requires-Dist: matplotlib ==3.7.4 ; extra == 'yolov7-quantized'
 Requires-Dist: object-detection-metrics ==0.4.post1 ; extra == 'yolov7-quantized'
 Requires-Dist: scipy ==1.8.1 ; extra == 'yolov7-quantized'
 Requires-Dist: seaborn ==0.11.0 ; extra == 'yolov7-quantized'
+Requires-Dist: shapely ==2.0.3 ; extra == 'yolov7-quantized'
 Provides-Extra: yolov7_quantized
 Requires-Dist: matplotlib ==3.7.4 ; extra == 'yolov7_quantized'
 Requires-Dist: object-detection-metrics ==0.4.post1 ; extra == 'yolov7_quantized'
 Requires-Dist: scipy ==1.8.1 ; extra == 'yolov7_quantized'
 Requires-Dist: seaborn ==0.11.0 ; extra == 'yolov7_quantized'
+Requires-Dist: shapely ==2.0.3 ; extra == 'yolov7_quantized'
 Provides-Extra: yolov8-det
 Requires-Dist: object-detection-metrics ==0.4.post1 ; extra == 'yolov8-det'
 Requires-Dist: seaborn ==0.11.0 ; extra == 'yolov8-det'
 Requires-Dist: thop ==0.1.1.post2209072238 ; extra == 'yolov8-det'
 Requires-Dist: ultralytics ==8.0.193 ; extra == 'yolov8-det'
+Requires-Dist: shapely ==2.0.3 ; extra == 'yolov8-det'
 Provides-Extra: yolov8-det-quantized
 Requires-Dist: object-detection-metrics ==0.4.post1 ; extra == 'yolov8-det-quantized'
 Requires-Dist: seaborn ==0.11.0 ; extra == 'yolov8-det-quantized'
 Requires-Dist: thop ==0.1.1.post2209072238 ; extra == 'yolov8-det-quantized'
 Requires-Dist: ultralytics ==8.0.193 ; extra == 'yolov8-det-quantized'
+Requires-Dist: shapely ==2.0.3 ; extra == 'yolov8-det-quantized'
 Provides-Extra: yolov8-seg
 Requires-Dist: seaborn ==0.11.0 ; extra == 'yolov8-seg'
 Requires-Dist: thop ==0.1.1.post2209072238 ; extra == 'yolov8-seg'
 Requires-Dist: ultralytics ==8.0.193 ; extra == 'yolov8-seg'
 Provides-Extra: yolov8_det
 Requires-Dist: object-detection-metrics ==0.4.post1 ; extra == 'yolov8_det'
 Requires-Dist: seaborn ==0.11.0 ; extra == 'yolov8_det'
 Requires-Dist: thop ==0.1.1.post2209072238 ; extra == 'yolov8_det'
 Requires-Dist: ultralytics ==8.0.193 ; extra == 'yolov8_det'
+Requires-Dist: shapely ==2.0.3 ; extra == 'yolov8_det'
 Provides-Extra: yolov8_det_quantized
 Requires-Dist: object-detection-metrics ==0.4.post1 ; extra == 'yolov8_det_quantized'
 Requires-Dist: seaborn ==0.11.0 ; extra == 'yolov8_det_quantized'
 Requires-Dist: thop ==0.1.1.post2209072238 ; extra == 'yolov8_det_quantized'
 Requires-Dist: ultralytics ==8.0.193 ; extra == 'yolov8_det_quantized'
+Requires-Dist: shapely ==2.0.3 ; extra == 'yolov8_det_quantized'
 Provides-Extra: yolov8_seg
 Requires-Dist: seaborn ==0.11.0 ; extra == 'yolov8_seg'
 Requires-Dist: thop ==0.1.1.post2209072238 ; extra == 'yolov8_seg'
 Requires-Dist: ultralytics ==8.0.193 ; extra == 'yolov8_seg'
 
 [![Qualcomm AI Hub Models](https://qaihub-public-assets.s3.us-west-2.amazonaws.com/qai-hub-models/quic-logo.jpg)](https://aihub.qualcomm.com)
 
@@ -298,36 +347,46 @@
 
 * Explore models optimized for on-device deployment of vision, speech, text, and genenrative AI.
 * View open-source recipes to quantize, optimize, and deploy these models on-device.
 * Browse through [performance metrics](https://aihub.qualcomm.com/models) captured for these models on several devices.
 * Access the models through [Hugging Face](https://huggingface.co/qualcomm).
 * [Sign up](https://myaccount.qualcomm.com/signup) to run these models on hosted Qualcomm devices.
 
+Supported **python package host machine** Operating Systems:
+- Linux (x86, ARM)
+- Windows (x86)
+- Windows (ARM-- ONLY via x86 Python, not ARM Python)
+- MacOS (x86, ARM)
+
 Supported runtimes
 * [TensorFlow Lite](https://www.tensorflow.org/lite)
 * [Qualcomm AI Engine Direct](https://www.qualcomm.com/developer/artificial-intelligence#overview)
+* [ONNX](https://onnxruntime.ai/docs/execution-providers/QNN-ExecutionProvider.html)
 
-Supported operating systems:
-* Android 11+
+Models can be deployed on:
+* Android
+* Windows
+* Linux
 
 Supported compute units
 * CPU, GPU, NPU (includes [Hexagon DSP](https://developer.qualcomm.com/software/hexagon-dsp-sdk/dsp-processor), [HTP](https://developer.qualcomm.com/hardware/qualcomm-innovators-development-kit/ai-resources-overview/ai-hardware-cores-accelerators))
 
 Supported precision
 * Floating Points: FP16
 * Integer: INT8 (8-bit weight and activation on select models), INT4 (4-bit weight, 16-bit activation on select models)
 
 Supported chipsets
 * [Snapdragon 845](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-845-mobile-platform), [Snapdragon 855/855+](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-855-mobile-platform), [Snapdragon 865/865+](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-865-plus-5g-mobile-platform), [Snapdragon 888/888+](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-888-5g-mobile-platform)
-* [Snapdragon 8 Gen 1](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-1-mobile-platform), [Snapdragon 8 Gen 2](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-2-mobile-platform), [Snapdragon 8 Gen 3](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-3-mobile-platform)
+* [Snapdragon 8 Gen 1](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-1-mobile-platform), [Snapdragon 8 Gen 2](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-2-mobile-platform), [Snapdragon 8 Gen 3](https://www.qualcomm.com/products/mobile/snapdragon/smartphones/snapdragon-8-series-mobile-platforms/snapdragon-8-gen-3-mobile-platform), [Snapdragon X Elite](https://www.qualcomm.com/products/mobile/snapdragon/pcs-and-tablets/snapdragon-x-elite)
 
 Select supported devices
 * Samsung Galaxy S21 Series, Galaxy S22 Series, Galaxy S23 Series, Galaxy S24 Series
 * Xiaomi 12, 13
 * Google Pixel 3, 4, 5
+* Snapdragon X Elite CRD (Compute Reference Device)
 
 and many more.
 
 ## Installation
 
 We currently support **Python >=3.8 and <= 3.10.** We recommend using a Python
 virtual environment
@@ -547,14 +606,16 @@
 ### Computer Vision
 
 | Model | README | Torch App | Device Export | CLI Demo
 | -- | -- | -- | -- | --
 | | | | |
 | **Image Classification**
 | [ConvNext-Tiny](https://aihub.qualcomm.com/models/convnext_tiny) | [qai_hub_models.models.convnext_tiny](qai_hub_models/models/convnext_tiny/README.md) |  |  | 
+| [ConvNext-Tiny-w8a16-Quantized](qai_hub_models/models/convnext_tiny_w8a16_quantized/README.md) | [qai_hub_models.models.convnext_tiny_w8a16_quantized](qai_hub_models/models/convnext_tiny_w8a16_quantized/README.md) |  |  | 
+| [ConvNext-Tiny-w8a8-Quantized](qai_hub_models/models/convnext_tiny_w8a8_quantized/README.md) | [qai_hub_models.models.convnext_tiny_w8a8_quantized](qai_hub_models/models/convnext_tiny_w8a8_quantized/README.md) |  |  | 
 | [DenseNet-121](https://aihub.qualcomm.com/models/densenet121) | [qai_hub_models.models.densenet121](qai_hub_models/models/densenet121/README.md) |  |  | 
 | [EfficientNet-B0](https://aihub.qualcomm.com/models/efficientnet_b0) | [qai_hub_models.models.efficientnet_b0](qai_hub_models/models/efficientnet_b0/README.md) |  |  | 
 | [GoogLeNet](https://aihub.qualcomm.com/models/googlenet) | [qai_hub_models.models.googlenet](qai_hub_models/models/googlenet/README.md) |  |  | 
 | [GoogLeNetQuantized](https://aihub.qualcomm.com/models/googlenet_quantized) | [qai_hub_models.models.googlenet_quantized](qai_hub_models/models/googlenet_quantized/README.md) |  |  | 
 | [Inception-v3](https://aihub.qualcomm.com/models/inception_v3) | [qai_hub_models.models.inception_v3](qai_hub_models/models/inception_v3/README.md) |  |  | 
 | [Inception-v3-Quantized](https://aihub.qualcomm.com/models/inception_v3_quantized) | [qai_hub_models.models.inception_v3_quantized](qai_hub_models/models/inception_v3_quantized/README.md) |  |  | 
 | [MNASNet05](https://aihub.qualcomm.com/models/mnasnet05) | [qai_hub_models.models.mnasnet05](qai_hub_models/models/mnasnet05/README.md) |  |  | 
@@ -607,15 +668,16 @@
 | [XLSR-Quantized](https://aihub.qualcomm.com/models/xlsr_quantized) | [qai_hub_models.models.xlsr_quantized](qai_hub_models/models/xlsr_quantized/README.md) |  |  | 
 | | | | |
 | **Semantic Segmentation**
 | [DDRNet23-Slim](https://aihub.qualcomm.com/models/ddrnet23_slim) | [qai_hub_models.models.ddrnet23_slim](qai_hub_models/models/ddrnet23_slim/README.md) |  |  | 
 | [DeepLabV3-Plus-MobileNet](https://aihub.qualcomm.com/models/deeplabv3_plus_mobilenet) | [qai_hub_models.models.deeplabv3_plus_mobilenet](qai_hub_models/models/deeplabv3_plus_mobilenet/README.md) |  |  | 
 | [DeepLabV3-Plus-MobileNet-Quantized](https://aihub.qualcomm.com/models/deeplabv3_plus_mobilenet_quantized) | [qai_hub_models.models.deeplabv3_plus_mobilenet_quantized](qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/README.md) |  |  | 
 | [DeepLabV3-ResNet50](https://aihub.qualcomm.com/models/deeplabv3_resnet50) | [qai_hub_models.models.deeplabv3_resnet50](qai_hub_models/models/deeplabv3_resnet50/README.md) |  |  | 
-| [FCN_ResNet50](https://aihub.qualcomm.com/models/fcn_resnet50) | [qai_hub_models.models.fcn_resnet50](qai_hub_models/models/fcn_resnet50/README.md) |  |  | 
+| [FCN-ResNet50](https://aihub.qualcomm.com/models/fcn_resnet50) | [qai_hub_models.models.fcn_resnet50](qai_hub_models/models/fcn_resnet50/README.md) |  |  | 
+| [FCN-ResNet50-Quantized](https://aihub.qualcomm.com/models/fcn_resnet50_quantized) | [qai_hub_models.models.fcn_resnet50_quantized](qai_hub_models/models/fcn_resnet50_quantized/README.md) |  |  | 
 | [FFNet-122NS-LowRes](https://aihub.qualcomm.com/models/ffnet_122ns_lowres) | [qai_hub_models.models.ffnet_122ns_lowres](qai_hub_models/models/ffnet_122ns_lowres/README.md) |  |  | 
 | [FFNet-40S](https://aihub.qualcomm.com/models/ffnet_40s) | [qai_hub_models.models.ffnet_40s](qai_hub_models/models/ffnet_40s/README.md) |  |  | 
 | [FFNet-40S-Quantized](https://aihub.qualcomm.com/models/ffnet_40s_quantized) | [qai_hub_models.models.ffnet_40s_quantized](qai_hub_models/models/ffnet_40s_quantized/README.md) |  |  | 
 | [FFNet-54S](https://aihub.qualcomm.com/models/ffnet_54s) | [qai_hub_models.models.ffnet_54s](qai_hub_models/models/ffnet_54s/README.md) |  |  | 
 | [FFNet-54S-Quantized](https://aihub.qualcomm.com/models/ffnet_54s_quantized) | [qai_hub_models.models.ffnet_54s_quantized](qai_hub_models/models/ffnet_54s_quantized/README.md) |  |  | 
 | [FFNet-78S](https://aihub.qualcomm.com/models/ffnet_78s) | [qai_hub_models.models.ffnet_78s](qai_hub_models/models/ffnet_78s/README.md) |  |  | 
 | [FFNet-78S-LowRes](https://aihub.qualcomm.com/models/ffnet_78s_lowres) | [qai_hub_models.models.ffnet_78s_lowres](qai_hub_models/models/ffnet_78s_lowres/README.md) |  |  | 
@@ -633,23 +695,29 @@
 | [DETR-ResNet101-DC5](https://aihub.qualcomm.com/models/detr_resnet101_dc5) | [qai_hub_models.models.detr_resnet101_dc5](qai_hub_models/models/detr_resnet101_dc5/README.md) |  |  | 
 | [DETR-ResNet50](https://aihub.qualcomm.com/models/detr_resnet50) | [qai_hub_models.models.detr_resnet50](qai_hub_models/models/detr_resnet50/README.md) |  |  | 
 | [DETR-ResNet50-DC5](https://aihub.qualcomm.com/models/detr_resnet50_dc5) | [qai_hub_models.models.detr_resnet50_dc5](qai_hub_models/models/detr_resnet50_dc5/README.md) |  |  | 
 | [MediaPipe-Face-Detection](https://aihub.qualcomm.com/models/mediapipe_face) | [qai_hub_models.models.mediapipe_face](qai_hub_models/models/mediapipe_face/README.md) |  |  | 
 | [MediaPipe-Hand-Detection](https://aihub.qualcomm.com/models/mediapipe_hand) | [qai_hub_models.models.mediapipe_hand](qai_hub_models/models/mediapipe_hand/README.md) |  |  | 
 | [YOLOv8-Detection](https://aihub.qualcomm.com/models/yolov8_det) | [qai_hub_models.models.yolov8_det](qai_hub_models/models/yolov8_det/README.md) |  |  | 
 | [YOLOv8-Detection-Quantized](https://aihub.qualcomm.com/models/yolov8_det_quantized) | [qai_hub_models.models.yolov8_det_quantized](qai_hub_models/models/yolov8_det_quantized/README.md) |  |  | 
+| [Yolo-NAS](https://aihub.qualcomm.com/models/yolonas) | [qai_hub_models.models.yolonas](qai_hub_models/models/yolonas/README.md) |  |  | 
+| [Yolo-NAS-Quantized](https://aihub.qualcomm.com/models/yolonas_quantized) | [qai_hub_models.models.yolonas_quantized](qai_hub_models/models/yolonas_quantized/README.md) |  |  | 
 | [Yolo-v6](https://aihub.qualcomm.com/models/yolov6) | [qai_hub_models.models.yolov6](qai_hub_models/models/yolov6/README.md) |  |  | 
 | [Yolo-v7](https://aihub.qualcomm.com/models/yolov7) | [qai_hub_models.models.yolov7](qai_hub_models/models/yolov7/README.md) |  |  | 
 | [Yolo-v7-Quantized](https://aihub.qualcomm.com/models/yolov7_quantized) | [qai_hub_models.models.yolov7_quantized](qai_hub_models/models/yolov7_quantized/README.md) |  |  | 
 | | | | |
 | **Pose Estimation**
 | [HRNetPose](https://aihub.qualcomm.com/models/hrnet_pose) | [qai_hub_models.models.hrnet_pose](qai_hub_models/models/hrnet_pose/README.md) |  |  | 
 | [LiteHRNet](https://aihub.qualcomm.com/models/litehrnet) | [qai_hub_models.models.litehrnet](qai_hub_models/models/litehrnet/README.md) |  |  | 
 | [MediaPipe-Pose-Estimation](https://aihub.qualcomm.com/models/mediapipe_pose) | [qai_hub_models.models.mediapipe_pose](qai_hub_models/models/mediapipe_pose/README.md) |  |  | 
 | [OpenPose](https://aihub.qualcomm.com/models/openpose) | [qai_hub_models.models.openpose](qai_hub_models/models/openpose/README.md) |  |  | 
+| [Posenet-Mobilenet](qai_hub_models/models/posenet_mobilenet/README.md) | [qai_hub_models.models.posenet_mobilenet](qai_hub_models/models/posenet_mobilenet/README.md) |  |  | 
+| | | | |
+| **Depth Estimation**
+| [Midas-V2](qai_hub_models/models/midas/README.md) | [qai_hub_models.models.midas](qai_hub_models/models/midas/README.md) |  |  | 
 
 ### Audio
 
 | Model | README | Torch App | Device Export | CLI Demo
 | -- | -- | -- | -- | --
 | | | | |
 | **Speech Recognition**
@@ -672,14 +740,16 @@
 ### Generative Ai
 
 | Model | README | Torch App | Device Export | CLI Demo
 | -- | -- | -- | -- | --
 | | | | |
 | **Image Generation**
 | [ControlNet](https://aihub.qualcomm.com/models/controlnet_quantized) | [qai_hub_models.models.controlnet_quantized](qai_hub_models/models/controlnet_quantized/README.md) |  |  | 
-| [Stable-Diffusion](https://aihub.qualcomm.com/models/stable_diffusion_quantized) | [qai_hub_models.models.stable_diffusion_quantized](qai_hub_models/models/stable_diffusion_quantized/README.md) |  |  | 
+| [Riffusion](qai_hub_models/models/riffusion_quantized/README.md) | [qai_hub_models.models.riffusion_quantized](qai_hub_models/models/riffusion_quantized/README.md) |  |  | 
+| [Stable-Diffusion-v1.5](https://aihub.qualcomm.com/models/stable_diffusion_v1_5_quantized) | [qai_hub_models.models.stable_diffusion_v1_5_quantized](qai_hub_models/models/stable_diffusion_v1_5_quantized/README.md) |  |  | 
+| [Stable-Diffusion-v2.1](qai_hub_models/models/stable_diffusion_v2_1_quantized/README.md) | [qai_hub_models.models.stable_diffusion_v2_1_quantized](qai_hub_models/models/stable_diffusion_v2_1_quantized/README.md) |  |  | 
 | | | | |
 | **Text Generation**
 | [Baichuan-7B](https://aihub.qualcomm.com/models/baichuan_7b_quantized) | [qai_hub_models.models.baichuan_7b_quantized](qai_hub_models/models/baichuan_7b_quantized/README.md) |  |  | 
 | [Llama-v2-7B-Chat](https://aihub.qualcomm.com/models/llama_v2_7b_chat_quantized) | [qai_hub_models.models.llama_v2_7b_chat_quantized](qai_hub_models/models/llama_v2_7b_chat_quantized/README.md) |  |  | 
```

## Comparing `qai_hub_models-0.5.1.dist-info/RECORD` & `qai_hub_models-0.6.0.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,64 +1,66 @@
 qai_hub_models/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/_version.py,sha256=re3Skq9KHSFnKkF5HvubG0knuLXdhMdZAY5wfbcr3yQ,281
+qai_hub_models/_version.py,sha256=LNCk3thpW_o6xnoFg3Kg_rU5927dhKjoxOywUzx5X7c,281
 qai_hub_models/asset_bases.yaml,sha256=53T7xh4bysMolqjwBOIxm4tm23szHhVQPgOKo7JzCvo,620
 qai_hub_models/conftest.py,sha256=STs4po9vrxUPOpDmX0XbVCVSIHtsbKou_Y11coTBAbU,734
-qai_hub_models/global_requirements.txt,sha256=y7Pp0d7YLb--E7J-8DQAPOG2Irq3K8oSPJqU0VhwR4M,913
-qai_hub_models/requirements-dev.txt,sha256=z81nhpASF1QUiIvaj3ZEXr75ifaWkSstpB-Y-3jOJuo,395
+qai_hub_models/global_requirements.txt,sha256=PZaNVWcL0qy3k3g6_doW1WSacswuwXcMC0maGDKXDqQ,1052
+qai_hub_models/requirements-dev.txt,sha256=3ZlCC6BE12jsmuam7XD4Ivbl0clFOyfEfGqb0iAVDtY,395
 qai_hub_models/requirements.txt,sha256=qu_-OPNrU_0-iG2lejY1zNn-4t0WbTOxw--z6IH7WQk,427
 qai_hub_models/datasets/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/datasets/bsd300.py,sha256=ndX_-l3KFRQeOPxsq8AjyIwDb0ozGYQpP7N84lpQwtk,4757
+qai_hub_models/datasets/bsd300.py,sha256=KDTQD3MK9r2AvuohHSKUKgByKA7kInKteAYVm53U_os,4635
 qai_hub_models/datasets/coco.py,sha256=p1tVNIqajR6P1B3b7yNpztiVq_uo3KKBOuDzlCg0WNw,4109
-qai_hub_models/datasets/common.py,sha256=_MxFQMrogLDazscDyxu3NNid4j3IWK9jTl5kefWfgNA,1614
-qai_hub_models/datasets/imagenette.py,sha256=Iwu0UIRgnP6UhWGzHcoyq-qZLyIU0AQvcX9wKskPEEQ,3173
-qai_hub_models/datasets/pascal_voc.py,sha256=Z4au_liYBDNjTzh3_S9GkdLPulYe_vnRa8SL6emKIGg,2616
+qai_hub_models/datasets/common.py,sha256=8WBsk_5iNgiuUQDpafKEcIu79OlJHOo16cOo6jDPfZc,1632
+qai_hub_models/datasets/imagenet.py,sha256=rDgHkStd389Nk38qvQltWFW3LC5g5TXOKJyrlcG_ubw,3312
+qai_hub_models/datasets/imagenette.py,sha256=p8k5WgblcaZhi7cYr9yS81STAC6cYdmjxDLLYWct0ok,3029
+qai_hub_models/datasets/pascal_voc.py,sha256=4bTFu32fMhpzqU2uSr4dj0sKX8YLS59XQNwjLrlSCBA,2585
 qai_hub_models/evaluators/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/evaluators/base_evaluators.py,sha256=vFG_4HpZkx3bHBluZq17GvP7iBCWkGphy07__pkvaww,6212
 qai_hub_models/evaluators/classification_evaluator.py,sha256=ccjbu9vcVTn8UBB0iUM_fRxvTlbarwfFWlXIR90gFAU,1326
 qai_hub_models/evaluators/detection_evaluator.py,sha256=qQl-1WApD95Os0tYTVLpMsVNNT7ViqS4nvcNW7rAViw,3699
-qai_hub_models/evaluators/image_evaluator.py,sha256=nPMHIeyo7WreD6LEa-eqdHdvIZQA1NbPhYoVrOQw3DY,2434
+qai_hub_models/evaluators/segmentation_evaluator.py,sha256=Q602JWro4CKKuvq9u0nirJ36VZ7-CZYkUsoJQC3d5kE,2587
 qai_hub_models/evaluators/superres_evaluator.py,sha256=7NwBcPFJCjCbnmVpjB0cctRPNc2Am6PCL7z8vYzcDUY,2181
 qai_hub_models/models/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/common.py,sha256=Pgd1o517c7YvY8GWVX8ksH2Lu2lc7Ueagi4OKGYyN_E,666
-qai_hub_models/models/protocols.py,sha256=4CntVWA7YWOYsnIrj6VgEUDQFk4Hswhl3UJHZGQrB0U,7882
+qai_hub_models/models/protocols.py,sha256=x66A7xk2nNK6ssXvKnIQjNj5Act4ro2n2pvQx-KLLh0,8042
 qai_hub_models/models/_shared/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/common.py,sha256=LHJiXmHRCP2A6GynzbwoLabojxZZlZOXwMtu4FMZcbg,1655
 qai_hub_models/models/_shared/cityscapes_segmentation/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/cityscapes_segmentation/app.py,sha256=2n2tPq3bmKmAudJaQGoewvOZAX_RybjjLJnBmvfWKEM,4346
 qai_hub_models/models/_shared/cityscapes_segmentation/demo.py,sha256=Hzjw4Bo2hKUbPkHQTk3-extZweVbAaL7bd7LAHeVy5w,2904
-qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py,sha256=25G3q93XoJhT9-1sKh6p0cstO1vhAI13hOR91IucKmE,890
+qai_hub_models/models/_shared/cityscapes_segmentation/evaluator.py,sha256=KiwOPyLdxw5SdHRUYlNtZTkYw6-B7jvHAdU1hQ7zlw0,747
 qai_hub_models/models/_shared/cityscapes_segmentation/model.py,sha256=z-F-WrxlqJDdv2TNXwJW73pKgx-S0fSR2gQK0BqOoBU,2888
 qai_hub_models/models/_shared/cityscapes_segmentation/patches/move_datasets.diff,sha256=y_7LNVu_y66oJzmjUps6PFe-1XRj0J5cf68-DJNeBcU,8565
+qai_hub_models/models/_shared/convnext_tiny_quantized/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
+qai_hub_models/models/_shared/convnext_tiny_quantized/model.py,sha256=fE6B7uOHgHNLlB5gl7AbRdMMeAlwGaMbnQrtm-ePOQ8,4270
 qai_hub_models/models/_shared/deeplab/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/deeplab/app.py,sha256=KbIYMuVoiYDEYwuhCeOY-ZnxQFu6VGNqb4bFOP4fD8M,2449
 qai_hub_models/models/_shared/deeplab/demo.py,sha256=qcH492vccd3MSJj4KSDkN-8_Z0unWUAvKYJBU5YrMH0,2256
-qai_hub_models/models/_shared/deeplab/evaluator.py,sha256=z_lorsF_6cW4G5bNBxG89HdWmeG53PyRk9fOuswxB0M,915
-qai_hub_models/models/_shared/deeplab/model.py,sha256=_ZFBMSGMHFDlBXD9NCztIsdlqhUObU2-QrjgNoHWqEc,2015
+qai_hub_models/models/_shared/deeplab/model.py,sha256=ukobg_49Qd_wzzzy741Qj7ccprITg4_kdQRiO9DHmyw,2034
 qai_hub_models/models/_shared/detr/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/detr/app.py,sha256=EKqvk7pHd8_JVhc0BbESwLyZiVu_7poSf1Qgad2pbSM,4604
 qai_hub_models/models/_shared/detr/coco_label_map.py,sha256=QOEkaqmyW6mS8gXLxfUHJfCyWtCQ1lNzzLEap8zlW1c,3565
 qai_hub_models/models/_shared/detr/demo.py,sha256=TytpiefnNUXvlwezD9QkAe5Xt5aRzswJu9SWca4AwR8,2091
 qai_hub_models/models/_shared/detr/model.py,sha256=G0nk4_EnJDLINqr1jrbdW54EzH4-QoEKhCRqFvVViYs,2050
 qai_hub_models/models/_shared/fastsam/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/fastsam/app.py,sha256=VcGV3iloO_1t3g694bkV02PIPXIbnBaAoL6emPK-eRA,4883
-qai_hub_models/models/_shared/fastsam/demo.py,sha256=m9HSdl3LenmUHRF8lh8Gux66CLF6CyoHjXzP9SsZ4Q0,2022
+qai_hub_models/models/_shared/fastsam/demo.py,sha256=aZdQwMAC1a74KK2mMyZtzusSN_xT5GyAo7YvnB2Jp30,2026
 qai_hub_models/models/_shared/fastsam/model.py,sha256=Vr1iTFJ-UU0jxpwD2LCQ_mU-5fPCpw0NnPsbpGoOYzc,1935
 qai_hub_models/models/_shared/ffnet/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/ffnet/model.py,sha256=7fO0THAtD3QTphtGxRxIZbqnRAa88niDBN8xyX2xVYM,4548
 qai_hub_models/models/_shared/ffnet/test_utils.py,sha256=y-8cmFjaP_mCZIeSs4if8gKYMv-DW9mXKS_hIB_engc,1569
 qai_hub_models/models/_shared/ffnet_quantized/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/ffnet_quantized/aimet_config.json,sha256=RwkXlIJFptwSaAqRENlmu1spoW2oyFlhv2X20GGvIWM,1165
 qai_hub_models/models/_shared/ffnet_quantized/model.py,sha256=kFSAROGzDLIuSFPum3ZJ7_YTGcqs_BUYhAKpTa2uIRo,2475
 qai_hub_models/models/_shared/imagenet_classifier/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/models/_shared/imagenet_classifier/app.py,sha256=PnznFwwswxLZy-Kzr9tw82hyF633a2EuB84s2SQDZ7E,3041
+qai_hub_models/models/_shared/imagenet_classifier/app.py,sha256=8dlBBZKFHPoQmw0s1l5Gu8p2tS8HOwka0xObeosdxOs,2794
 qai_hub_models/models/_shared/imagenet_classifier/demo.py,sha256=8o3ad2SJUbkVzg5jecHin4N2J_YxMNQsQ-ioImN3ufU,2432
-qai_hub_models/models/_shared/imagenet_classifier/model.py,sha256=pvW_t6Tqb9ZFUpeYWGzi0Vr3TVxkqeV0dluRiIaApb0,4661
+qai_hub_models/models/_shared/imagenet_classifier/model.py,sha256=pmReXqJIbC-kPge4YlSWzqvwzWURHMR7M6GCVoLQYew,4669
 qai_hub_models/models/_shared/imagenet_classifier/test_utils.py,sha256=MnxFDMDKULF87n4p4N1eYrUJ854zECi-nPqPLqakFZA,3781
 qai_hub_models/models/_shared/mediapipe/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/models/_shared/mediapipe/app.py,sha256=Sj7U03ljI1MHXkigQvvhSOf7taPFrBrpKKiLa-KLBcc,30293
+qai_hub_models/models/_shared/mediapipe/app.py,sha256=82eBLcoSUxX8g9vCR-qg32ob3PvRXFQUd1r8K-9lL1g,30302
 qai_hub_models/models/_shared/mediapipe/utils.py,sha256=Q1J3HW3u_dIvnjmt9AzBDMqv9USjWuDeBzmG765YET0,4394
 qai_hub_models/models/_shared/quicksrnet/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/quicksrnet/common.py,sha256=TDl166uGYRvm6Y8XxGN0diS_yfNCrPbTAY0iNBGkta8,985
 qai_hub_models/models/_shared/repaint/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/repaint/app.py,sha256=FDjlRjBC-Zoq49nfmgjh9UuxFI7np6RsME9nUutEw9E,3366
 qai_hub_models/models/_shared/repaint/demo.py,sha256=E-rgU7-jd0gdWfSh3Q9gJS8kJGY4zf2rfh6hkiZhDwI,2229
 qai_hub_models/models/_shared/sesr/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
@@ -66,877 +68,949 @@
 qai_hub_models/models/_shared/super_resolution/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/super_resolution/app.py,sha256=PGVTn0KGgyvCP6f2yCyCvObk8v-a96CNie4T7p5bxRs,2143
 qai_hub_models/models/_shared/super_resolution/demo.py,sha256=jO9eshr15diMYgkEGbsSjYs4Lmuqhm8v48v9Eaoi204,2775
 qai_hub_models/models/_shared/swin/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/swin/swin_transformer.py,sha256=E0EHaEh4k_FaVreqfNaj6iYu2HJ6qGjNhIiP6f1O1cs,9175
 qai_hub_models/models/_shared/video_classifier/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/video_classifier/app.py,sha256=DAfACj-oPdmLkJ5cohQbdz5JwmWrttKXOt2TiiuQExk,11183
-qai_hub_models/models/_shared/video_classifier/demo.py,sha256=tO2sX2PIFsT8-Lps2ZGHiW1YaCELejYmwbAs7tFC5sk,1581
+qai_hub_models/models/_shared/video_classifier/demo.py,sha256=SbVdcrP-TxN43QiPdGlIbXJzeucIDdmKsjihulxFAsE,1568
 qai_hub_models/models/_shared/video_classifier/model.py,sha256=obQoYEt1AGWxrfm3tD12j1gmj6WlOGeBbATiXoEFTds,1969
 qai_hub_models/models/_shared/whisper/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/models/_shared/whisper/app.py,sha256=FG9-3wysMYdfh7tcH6miAO5xf3EayCqVbCzLsSP05XQ,10188
+qai_hub_models/models/_shared/whisper/app.py,sha256=mvQsiCeKb5fGw_gXXKit6rUufbBAnKgHPh3ZScZgBYc,10427
 qai_hub_models/models/_shared/whisper/demo.py,sha256=7zi12g3CskAHOndf2nZIl_QVFLYp0n-P3VtO-ylEECU,1302
-qai_hub_models/models/_shared/whisper/model.py,sha256=8NfDTyLWl2Hp2t8lqc2fbNos65RsnwHLWxVXcelUT_8,14119
-qai_hub_models/models/_shared/whisper/test_utils.py,sha256=MVNzlRySllNxozJt9_biaPIii2hLNJHNga7weTv1mdU,2848
+qai_hub_models/models/_shared/whisper/model.py,sha256=ngvh0LeRg0ZDHW15uXzykFUWWl9EgUcH7uvVxDXnTD0,18911
+qai_hub_models/models/_shared/whisper/test_utils.py,sha256=dFYLSgwYGml90vI5fWjXDjCWcmsG4c1JmYJlmE-Oqak,3123
 qai_hub_models/models/_shared/yolo/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/models/_shared/yolo/app.py,sha256=SfL-yVTn7RWHPzRiX0dgfNGk84BKBcNOnIjIZ5b3XzI,7413
-qai_hub_models/models/_shared/yolo/demo.py,sha256=XBrIzWUujUBj6r4qPhra5mbUnZF5FZSs0MzWTIEeqz8,2432
+qai_hub_models/models/_shared/yolo/demo.py,sha256=Nvs2nbTThw__u9yjJwkxGFdhai_YQMMwdWbTg5vGs94,2494
 qai_hub_models/models/_shared/yolo/utils.py,sha256=EZWCEI-Z2oot3NyL9h3J_B_2GRSlShyqARfXEenPoHQ,6081
 qai_hub_models/models/aotgan/__init__.py,sha256=BaaLR3zQEAxIdLMsE_bj1ifMQKJs6_HBVeTvC601C8Q,450
 qai_hub_models/models/aotgan/conftest.py,sha256=2KW4tZi0QOWX-_NVefoyLT0Mmcq8hsFa0md4R6eVMbc,1405
 qai_hub_models/models/aotgan/demo.py,sha256=pNMiZ4xPMXki0cep2i7KrSCYpbkFjVA7AjWJK3WLROw,597
-qai_hub_models/models/aotgan/export.py,sha256=sX-ECB4WXo1aCQquwBjKjlllRTUx_vWiz59S8bVQyQE,8437
+qai_hub_models/models/aotgan/export.py,sha256=A_HoTASyxpEZH7_GcWn5-DxkTACvaClDsHjThkYl7Us,8895
 qai_hub_models/models/aotgan/info.yaml,sha256=n2LVDkDfhrNh60R-XxQiZexVaNz3DHlmtJ7yuJ3MJ5U,1023
 qai_hub_models/models/aotgan/model.py,sha256=mj9Zn0MkmSh1Frsw9ESCB-pL1LjJoE_EHhNVPhfbqyo,4838
-qai_hub_models/models/aotgan/perf.yaml,sha256=f8-Qu-mep_LzR0jOBXPWVDpYe-0JxvxKb9ioI3vdYHU,4545
+qai_hub_models/models/aotgan/perf.yaml,sha256=PlKmj2dXhmSw_Hbw5y7ybNG_lakOJZbPchnjqP59CVA,5978
 qai_hub_models/models/aotgan/test.py,sha256=sPojPTH2xmn5YoLMpTt8rurhiF1BDS1PMgRhXK01Fio,2006
 qai_hub_models/models/aotgan/patches/layer_norm.diff,sha256=NbhINLgR1N0wfrI3x89m7JheBbxT2MTNkEm9dpedBeY,532
 qai_hub_models/models/baichuan_7b_quantized/info.yaml,sha256=dW4nlITktQn6wzRj8G5g8F3jzgWLtf9Azn8Tg5rcgcA,1987
 qai_hub_models/models/baichuan_7b_quantized/perf.yaml,sha256=CJPN5oLM6lBtEdScWIlo9aKCxb2sxYxH5OYC4zVGRzs,2038
 qai_hub_models/models/controlnet_quantized/__init__.py,sha256=aCoQX1Qz9SrPHhuSJr4vD7f9nJtkhCzAa9__HBvcg4g,559
 qai_hub_models/models/controlnet_quantized/app.py,sha256=5wCKVlYNyeZ_m1PfqCOQlIq80bToaYsTdxw2Fk4l05U,9705
 qai_hub_models/models/controlnet_quantized/demo.py,sha256=rPSPdSGbFMUJ9jdMj5nLEypRqsbmrMaitON-j6zgzds,6397
 qai_hub_models/models/controlnet_quantized/export.py,sha256=rZ4tiyoA0qzpYkObTNmRMbw7-U67XDCqDCce6PGcF2c,7894
-qai_hub_models/models/controlnet_quantized/info.yaml,sha256=ivhi6oY9IC0gGtw9REooBoV7aJQqUIvHw-g9YfE-qVw,1329
+qai_hub_models/models/controlnet_quantized/info.yaml,sha256=CbDuLUIJrHpm4Lkmk4VtNwJBEGkaDbO6sYdCBGjMpFg,1334
 qai_hub_models/models/controlnet_quantized/model.py,sha256=8BO-diXqlxH6wuNRs7VeYUlSxhDGWYCemAT_XgEzVt8,5426
 qai_hub_models/models/controlnet_quantized/perf.yaml,sha256=a0C641UKT6i75a3Tu0x_zsyVtB07oWBzsM3P2PSftAc,8427
 qai_hub_models/models/controlnet_quantized/requirements.txt,sha256=HWz6kAx8f-AYWf5IWvv-q1IOznXS94gXTQrtKGY4L70,46
-qai_hub_models/models/controlnet_quantized/test.py,sha256=ilelb6gq5P7_1RveAB09vfppdhzrVbWpHYiB-twI55w,1556
+qai_hub_models/models/controlnet_quantized/test.py,sha256=amPpiFha1MDrDAVHldxAgnXg_Sf8XlMTUh2JpSXzcbw,1588
 qai_hub_models/models/convnext_tiny/__init__.py,sha256=HCfwohVDc0VtmT7yLoa43BTiBAEkKXtFE9-jfRy3XrM,475
 qai_hub_models/models/convnext_tiny/conftest.py,sha256=ujfvC8Pi3rMwWoBaR_PHFxG--AQmLkiLr6EBNwGwuww,1318
 qai_hub_models/models/convnext_tiny/demo.py,sha256=22oIp3py29Uzbsh0ob2ak4ic4ZdetMiqwgyo3I2fqEY,543
-qai_hub_models/models/convnext_tiny/export.py,sha256=oRwJYk21MkDOPCkeiQhDj931pHiyQNlIXRbF6fpoWkc,8196
+qai_hub_models/models/convnext_tiny/export.py,sha256=NuKS9jTMfywZFoGi9Wvp9M1TqNQHoi8iGCXI4YVbZmA,8514
 qai_hub_models/models/convnext_tiny/info.yaml,sha256=C017TbQv8k22-OVMYW8RnesPhOAtRXZVqKANe2fAP7s,1287
 qai_hub_models/models/convnext_tiny/model.py,sha256=TohaPxU5EWTTBKMEjZusg238O3MEaA9MNKH36v6SoFw,708
-qai_hub_models/models/convnext_tiny/perf.yaml,sha256=Fg5TvSc1qlKNVDJEGh8C2aABL4YgG955krRLgO5yg7M,2707
+qai_hub_models/models/convnext_tiny/perf.yaml,sha256=-u7lp1d52yaCfSwoNSf3qxfC1PHIxqtScEc5dS0izmw,6053
 qai_hub_models/models/convnext_tiny/test.py,sha256=cijDPmavZsw6N6okJvEeQFhiC8zoWymxjT1QvbPUNhg,857
+qai_hub_models/models/convnext_tiny_w8a16_quantized/__init__.py,sha256=kHvsYnEKnyjnsxURFaNhGnGCYtluZGuCccWIbEYvAEI,491
+qai_hub_models/models/convnext_tiny_w8a16_quantized/conftest.py,sha256=BxiKypaK1_TnvToiiaRmi50J5gzE3jrQpokr8aJUbIk,1428
+qai_hub_models/models/convnext_tiny_w8a16_quantized/demo.py,sha256=G-QY--qNwbowRzyg1Cd663LAcz4iZFG-Nnnj5MDHTUc,604
+qai_hub_models/models/convnext_tiny_w8a16_quantized/export.py,sha256=9oaSj3G96V4FIVJWc2VzMhcY3QmP7LTwiL67IO9a3EQ,9022
+qai_hub_models/models/convnext_tiny_w8a16_quantized/info.yaml,sha256=8kn-l40DKyHardlfzGzfAKeQvCt2AWVyCgbnP2ofnAc,1384
+qai_hub_models/models/convnext_tiny_w8a16_quantized/model.py,sha256=GrNEjEyXOehcoGWYx2VAmxgaqAHKDC7_5u4QNRFALwo,1180
+qai_hub_models/models/convnext_tiny_w8a16_quantized/test.py,sha256=NdsRlbGvsYkkKUNj-0JMUFBgye8tBlkIRCpSSnOYkUM,1028
+qai_hub_models/models/convnext_tiny_w8a8_quantized/__init__.py,sha256=bHEZT-L7lwokPv7Hd3eH1uJW7v3nTwsu7-KKdVnz0pA,490
+qai_hub_models/models/convnext_tiny_w8a8_quantized/conftest.py,sha256=8orR7kVTzsaPldQ3EmI9v3sqDPgPZKS4XaiEOcX8Ivg,1427
+qai_hub_models/models/convnext_tiny_w8a8_quantized/demo.py,sha256=3PiiQabqP3vyY4L8W0c5UH57l72ndUitKk2yntwyv8U,601
+qai_hub_models/models/convnext_tiny_w8a8_quantized/export.py,sha256=3t9yu77EkPKXvFewhDbv8eO7GVQyd6699etVRkAMiGw,9018
+qai_hub_models/models/convnext_tiny_w8a8_quantized/info.yaml,sha256=TCWXjS6K35D7ID8jfNMF40M9tMy6__LXT9B3mJyDY-M,1380
+qai_hub_models/models/convnext_tiny_w8a8_quantized/model.py,sha256=w60fQDSfPTVRcEVlrvaDWCrrueng_Jm_d2Wvvh4RD38,1177
+qai_hub_models/models/convnext_tiny_w8a8_quantized/test.py,sha256=Z0h_hW0A5Quba9U9JQbWmiKqnTta99Ue97FhiqIu2Gs,1024
 qai_hub_models/models/ddrnet23_slim/__init__.py,sha256=fz62JiNfyn7NSLR8F95JKyRtjB6CQFJSxx8Vt2YQwyc,398
 qai_hub_models/models/ddrnet23_slim/app.py,sha256=bUGwG2F79praX6iTM5vHKxUlVlPyrUVl4FzmX3U9uWk,3750
 qai_hub_models/models/ddrnet23_slim/conftest.py,sha256=A1oi389WhdhvCaYKOZbMRpACx6hY6LhOD7YPfeDtazM,1412
 qai_hub_models/models/ddrnet23_slim/demo.py,sha256=DtX3jP0RGrtig0qNjNSrhuhYErtopHTmsVcArT_HDrg,2128
-qai_hub_models/models/ddrnet23_slim/export.py,sha256=aqXz3HeEggHYqgDScvJSwb1RSiipDrIK-tjL45WV2lI,8474
+qai_hub_models/models/ddrnet23_slim/export.py,sha256=k_8hXjo1RW2WMoyTm7j51mNB6owc6sSlG-APlBia0-g,8874
 qai_hub_models/models/ddrnet23_slim/info.yaml,sha256=J8BrgX-ROS7IN6jqb1Co6mC7mJxBO3VFVlCGHQyqIfc,1334
 qai_hub_models/models/ddrnet23_slim/model.py,sha256=o19xSxPaoEKCcYRotMWrqZyqPHI3iwigwU7OLXUKQ4M,3831
-qai_hub_models/models/ddrnet23_slim/perf.yaml,sha256=sQJiUkGIDMJ6GgqrFdxOZEDmkyQwOfpX-ZDUqEpUqHw,3358
+qai_hub_models/models/ddrnet23_slim/perf.yaml,sha256=dhvvP8X8-WqrKW9DAsLI6s_YKYtehrZG1pDbPWeosO0,4493
 qai_hub_models/models/ddrnet23_slim/test.py,sha256=Km5AirOt-YDL_qfBSKMuco1rFWR5-4MpK-o1GRBqPBY,1790
 qai_hub_models/models/deeplabv3_plus_mobilenet/__init__.py,sha256=58gIL-e--wJG349dRqjibNq1oNRWsJXgEvxdFXmshwc,455
 qai_hub_models/models/deeplabv3_plus_mobilenet/conftest.py,sha256=bBqQVhtUYgRY9UQqbX5bYZ2bMThL5NN741N7iPSVUdE,1423
 qai_hub_models/models/deeplabv3_plus_mobilenet/demo.py,sha256=VGfLkT14sw3VZa1h7zO8pCGeDvUz7H2zNQzGrmmRFc8,1091
-qai_hub_models/models/deeplabv3_plus_mobilenet/export.py,sha256=zaN3RtLVHt7EaBK9HR1-mYKGgGpQTYj36XGgapFfaPM,8498
+qai_hub_models/models/deeplabv3_plus_mobilenet/export.py,sha256=iK6tDpIuiYkGqA9-p7DXoOp5ddpDC8kK4GWIr5U1jow,8898
 qai_hub_models/models/deeplabv3_plus_mobilenet/info.yaml,sha256=tOIqh_pZfUo_BSpIgVlLIo_cMJVmH6W2wM7rlrsZ-58,1266
 qai_hub_models/models/deeplabv3_plus_mobilenet/model.py,sha256=dKYL4xLKUadUbe7Lo2ehMUatVAm0-d0eowxSqSgnCrM,2098
-qai_hub_models/models/deeplabv3_plus_mobilenet/perf.yaml,sha256=qfnMaqfJDygw4qcMJXU_PBrnNzShz8NXE4IuTgc5w1o,3812
+qai_hub_models/models/deeplabv3_plus_mobilenet/perf.yaml,sha256=sFXxghxrw3chroax4KcmDSqYIN8vOBFnGE-v_49AULY,6082
 qai_hub_models/models/deeplabv3_plus_mobilenet/test.py,sha256=FBAfk-9Og-uhCceKzyEE2mAHXiviSSORSr8OBGyr8uA,1862
 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/__init__.py,sha256=6gI73njrwOCedV126NWRglDo8htjzBkCv-wafEOcm2s,466
 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/conftest.py,sha256=9bwTSPjEbFS_ehNB1R9JrV4V-3SA15_6zO0ack3bm58,1433
 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/demo.py,sha256=xNiFm6o-ExYHcIWhaKyykyoqGZCCbfMvSVOK0x1MKpA,1156
-qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/export.py,sha256=BDMPIbt6zl0SFExisffmuPUPZQvm5MnnGYLdgDG0-OU,8914
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/export.py,sha256=a1DxXyUbyXmmNctQrLmj1NO4TbQKRBASV76kfKx-5RY,9314
 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/info.yaml,sha256=jzYZAMV9CZj3dzyi1FBgJdrzgeJht67WCYljOe5G_A8,1306
 qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/model.py,sha256=WExi05L2u2fQ1sJ1ifacq8yGeuGEsT9P1uPdM3NMX6s,2861
-qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/perf.yaml,sha256=mZlwe3z3zo9yhKsF77dXYBA0zZe3pJaSjo65j5UON28,5887
-qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/test.py,sha256=y_jG5Nc56h9W5HCWbVSV3xWG4HUi7iig2LqXxFNmkgs,2192
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/perf.yaml,sha256=_V6OOnIDaHnbBBjPdpqa8MIccYjkhhsN3tvXzwm2530,7788
+qai_hub_models/models/deeplabv3_plus_mobilenet_quantized/test.py,sha256=eXAfIE3k0Lcei5AV-4D0ErZsBwxFIq0g3jo3J7lfMRo,2183
 qai_hub_models/models/deeplabv3_resnet50/__init__.py,sha256=RgC9JWAyPRQugvGjLMpeeWh68mvBeK7KxBnIvMtA_P8,451
 qai_hub_models/models/deeplabv3_resnet50/conftest.py,sha256=ZTzIgleUyd3NmjF3jHcTk9_augG4gmjytPgkzHeS3lo,1417
 qai_hub_models/models/deeplabv3_resnet50/demo.py,sha256=lMuWqSJPGTmg-nop-u9DtDlGAQl7H3-JIaPsKcQ2rzE,1077
-qai_hub_models/models/deeplabv3_resnet50/export.py,sha256=t5xm1C592lkYdYH22v0ZyvsgAPfJbBQTtQrvjTZ7eBk,8492
+qai_hub_models/models/deeplabv3_resnet50/export.py,sha256=mDYDPf2jtCL9xu9XdOWQzqsQzIVjfV2Wxd4Bs71K5wY,8940
 qai_hub_models/models/deeplabv3_resnet50/info.yaml,sha256=f1lisHPDiRoqpQpWreOc6R4_p5v7MIYoKLpWmTf7ecY,1285
-qai_hub_models/models/deeplabv3_resnet50/model.py,sha256=_rRFyPK9tCpuY-LfIty6wreSQi0zKZkNxTqCclU8lKY,1581
-qai_hub_models/models/deeplabv3_resnet50/perf.yaml,sha256=Sy0BVOL9ExUmcWFiPsWwm_UbGAosHvd38kQqPhkia0w,3781
+qai_hub_models/models/deeplabv3_resnet50/model.py,sha256=qpEAYUtcl58euJy1hv2YSk3eFP0HnWpTxrGYtpXlOSk,2094
+qai_hub_models/models/deeplabv3_resnet50/perf.yaml,sha256=JmIUGm3pmhuC6j2zdgKm3aRnyVygItKKQ1un1C0Jnm0,4764
 qai_hub_models/models/deeplabv3_resnet50/test.py,sha256=e8d2z4KXbtOXYEHW-Qxp1v1Mvn9rE-VvwA-X4LZByjM,1766
 qai_hub_models/models/densenet121/__init__.py,sha256=TxCV8tdZZ-D4d15naxmzu-0l5CHW_N3BkJiIV5HRzbY,471
 qai_hub_models/models/densenet121/conftest.py,sha256=CXbss3wdhiBSf9-KJeBwORzGCZTZuPPIjwP35npouvc,1316
 qai_hub_models/models/densenet121/demo.py,sha256=kmSL8S4QnCR2AUdz-josBJzF3ehyQ5fOgHcsbgaIN2g,533
-qai_hub_models/models/densenet121/export.py,sha256=gRRjsjzagElAkfC_2VyUJGl8MPXttmtXvf6cWwlZhOs,8169
+qai_hub_models/models/densenet121/export.py,sha256=3k1_eOWamEvYQOjUZsvtdH5qW1iFVTXVP6OmqtimJe4,8507
 qai_hub_models/models/densenet121/info.yaml,sha256=RRS9Pk3XMI-kwohy4GZbzLePqaFD_03Iky3W4adFvX8,1310
 qai_hub_models/models/densenet121/model.py,sha256=Lp5noeTPApNrJL6z_VmsZfEHDuKCZNl_WUY-Lr7xatM,698
-qai_hub_models/models/densenet121/perf.yaml,sha256=yWrL0L1imY6MlzAnTRSocrd8VRw18-SKxY4B4oElLHM,4525
+qai_hub_models/models/densenet121/perf.yaml,sha256=w1M9Twb9cDByLFj1GYlWO646wNHeo3_ZyklwBc5MbQA,6039
 qai_hub_models/models/densenet121/test.py,sha256=iCxswqgBQ_GXA2vfr-ShiukFZS2Gj38OV9zY6Jm3Gvk,841
 qai_hub_models/models/detr_resnet101/__init__.py,sha256=X6U_xy5Tp6diGFQfn7Na0XebaI6Ojc0wFv6xSoh_sKY,483
 qai_hub_models/models/detr_resnet101/conftest.py,sha256=lbkM1UMAWwunLYYfxspPN3aPMAUrX0Mrre3ymvNAJRs,1319
 qai_hub_models/models/detr_resnet101/demo.py,sha256=9o_p8fmwrZycE5JQAsWCfskglZNfyrJOAVV49KJwChg,896
-qai_hub_models/models/detr_resnet101/export.py,sha256=la23kz4e0y0e0YURydoYT33bwXft1nK30AxZxhP3ORI,8186
+qai_hub_models/models/detr_resnet101/export.py,sha256=qI5q0LgPy-hS960T3UTbL3xLbp1yBYmuYvpzwrlUBpc,8494
 qai_hub_models/models/detr_resnet101/info.yaml,sha256=YP4G-_ATugwYJnXGz9uJ0ghrH38exu_7SQfOq-2_OLE,1188
 qai_hub_models/models/detr_resnet101/model.py,sha256=f6SJr1m53RdUirPG1GOVeLDluBBxLIsOV-z2hSgF20E,661
-qai_hub_models/models/detr_resnet101/perf.yaml,sha256=v7u9nCgEGMPn4JAgTbBfcGaMsvm8QdOezdsAyXS7NX0,3401
+qai_hub_models/models/detr_resnet101/perf.yaml,sha256=zbR4gs5ZPtsmxenLIswhIGtJGPXBxnA_-nOs3j-zX90,6087
 qai_hub_models/models/detr_resnet101/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet101/test.py,sha256=gDbI8asjHE5ONZLujGFXIcO_h-EC0OtEkg1t4SATMd8,1316
 qai_hub_models/models/detr_resnet101_dc5/__init__.py,sha256=u76QZ7QpcYASS-4FZptRsGd5jROIardr7dRIEktZ0Ds,490
 qai_hub_models/models/detr_resnet101_dc5/conftest.py,sha256=ljzCSez1M8ssW6cI8D8VKwpe2aQ6iNYIlcBBBq2E5Gk,1323
 qai_hub_models/models/detr_resnet101_dc5/demo.py,sha256=ie0VdX_40vFCnnIWRCnjxH9SRsxlGpGIJpvoteK6rNI,906
-qai_hub_models/models/detr_resnet101_dc5/export.py,sha256=RKaJQ4JkhhWxDQrcmJNX7C3Bt1_zW7qUdT-UVgEKZes,8202
+qai_hub_models/models/detr_resnet101_dc5/export.py,sha256=dUkHdvGFn0CFxp2YdgkYM0sspjl4T_F1V4y2gqVwrr0,8510
 qai_hub_models/models/detr_resnet101_dc5/info.yaml,sha256=otqz2oAdErfBmbBgnQNQgRXlBMcw_nI-p_UTJr4nYsw,1215
 qai_hub_models/models/detr_resnet101_dc5/model.py,sha256=A4OU7F9N_JMElzRdKUH7VKkXmqlmvBxlZ71B6kN1Yyo,668
-qai_hub_models/models/detr_resnet101_dc5/perf.yaml,sha256=yniqcl9xkno9ks4jOZTz9yHuw5bWVDB1E6XsqqtRNxc,3420
+qai_hub_models/models/detr_resnet101_dc5/perf.yaml,sha256=ZaiSFbYji2dpVkFD7lQP1iu7TiB-PQEYCWR6PNt3EMM,6106
 qai_hub_models/models/detr_resnet101_dc5/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet101_dc5/test.py,sha256=nvO-ikZrOs6mMSkoDxxFwOSuwA64diJrxslDgMVpXpw,1373
 qai_hub_models/models/detr_resnet50/__init__.py,sha256=atV57qR8yJou6gYUi56f7UTiBsbDVyQAvbwgHFuXI0c,481
 qai_hub_models/models/detr_resnet50/conftest.py,sha256=c4IiCFPKy2a1FUt9aGI1z5FUxhgjs8eI59SMGpJ97ig,1318
 qai_hub_models/models/detr_resnet50/demo.py,sha256=X9w6CWF9U1rYaN3Xo1RIDLgxk4l-CP8Kdrbp-wYaWBQ,893
-qai_hub_models/models/detr_resnet50/export.py,sha256=aFW5Y40A7oZlOcj2EEXoGcVx0ba_x9AsV_YcfSsBjsA,8182
+qai_hub_models/models/detr_resnet50/export.py,sha256=TI6oCwpwtQdegUva_Tod9OiO5cqXc0G1QCj1B-axqWw,8490
 qai_hub_models/models/detr_resnet50/info.yaml,sha256=Zgk7l86YchB6yOV1EHo072l0o8pOroZ79-VHvZbQhdQ,1185
 qai_hub_models/models/detr_resnet50/model.py,sha256=f7rWSFyK5_LybX4KkfnxZVlHpCCVk6hRfbx6CyEs7dg,659
-qai_hub_models/models/detr_resnet50/perf.yaml,sha256=q0rwAKFeJSULU8tEfGfNBBvjyQbtuxkWVhQ6GWjCboo,3406
+qai_hub_models/models/detr_resnet50/perf.yaml,sha256=RMr4N9Wd6eJHBGsjpzj58zjYjzoMN2qsEZKrYSatVXk,6067
 qai_hub_models/models/detr_resnet50/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet50/test.py,sha256=O0P_osmIHBpoaSgCSsyPgMrS0hEhyWYiQiGfhVZzdMg,1636
 qai_hub_models/models/detr_resnet50_dc5/__init__.py,sha256=EOUQu8Eybv_S6dv2MxT06c5fP8fb-zbNd-MGM6Yo4DI,488
 qai_hub_models/models/detr_resnet50_dc5/conftest.py,sha256=7V30k07eDrT8aRJX47OFbfbXxhUiMHFgcAK-f27MOTE,1322
 qai_hub_models/models/detr_resnet50_dc5/demo.py,sha256=BFw09UhIreglAFeM1lS9xnKYumTh4fnS3hgqp0io7Es,903
-qai_hub_models/models/detr_resnet50_dc5/export.py,sha256=iSAne6D4IGvR2JkCXgOe-TFuFFfDWBYyDlj40G5sTQY,8198
+qai_hub_models/models/detr_resnet50_dc5/export.py,sha256=71fz-136tn9iKY6rxE92N1y88DYdN-9c-PeX5rb4YRA,8506
 qai_hub_models/models/detr_resnet50_dc5/info.yaml,sha256=QrFUKwhno5ciqd0fpkPTis2SvXi2Nmqg9DkZTk6PaUE,1212
 qai_hub_models/models/detr_resnet50_dc5/model.py,sha256=8bZ0js6wEEb64RPwCHfwJtt9OrgTyk2KaRaS_9QxIAo,666
-qai_hub_models/models/detr_resnet50_dc5/perf.yaml,sha256=vjLSOax81ighjLerrlkXCmHgtsgSppU3o4f_ppdo5XE,3413
+qai_hub_models/models/detr_resnet50_dc5/perf.yaml,sha256=egSDxk6bjPlo26bCd0iFz2jQXbsSDuVwJFOi0F3vqQQ,6057
 qai_hub_models/models/detr_resnet50_dc5/requirements.txt,sha256=7JPG1_GVGtM8EzOggEHT_-_rO9H8b9NsMK5aLI1nImM,34
 qai_hub_models/models/detr_resnet50_dc5/test.py,sha256=xt60UK2gLEMyw460kHLE14y2zgzHhhaM3peD7PfIb7Y,1344
 qai_hub_models/models/efficientnet_b0/__init__.py,sha256=7iMYCbJmA6I13XjCyUEWsizqKWjp0PMumYfobTRAhww,477
 qai_hub_models/models/efficientnet_b0/conftest.py,sha256=mmy-wvh_GtqMw-oXw2zUJhvVMpb1t52oYGYVKC4PEu0,1320
 qai_hub_models/models/efficientnet_b0/demo.py,sha256=aZVZDgd4XTbaL3_3NKFMSHP15gKWYwe2qX6DEwJRPuM,549
-qai_hub_models/models/efficientnet_b0/export.py,sha256=QTZaeK00ekGRu9GG8FGqd2-q7ifctRotVjmUwy3DRsk,8184
+qai_hub_models/models/efficientnet_b0/export.py,sha256=FA9M0p9jVRS24XcJ0m0CbL2jW-WS2VeCxq_OeCES_Jc,8522
 qai_hub_models/models/efficientnet_b0/info.yaml,sha256=qHfAm4KiPtXcdb3GmVePXiZZmsOE07Gbzl8c7Hg268c,1361
 qai_hub_models/models/efficientnet_b0/model.py,sha256=3miNRC3gRr5pfkWrwYdLFmx7Wjw0eI1CIKEOHLlB7jQ,714
-qai_hub_models/models/efficientnet_b0/perf.yaml,sha256=xxJ7lRgM5BsXhM70_ah-BBTByiYJxnJqQVqf7pqmo74,4559
+qai_hub_models/models/efficientnet_b0/perf.yaml,sha256=0haHYMQrl-k1WqmlRSKh1AartaAw57zoCDa1Ab_RoZg,6039
 qai_hub_models/models/efficientnet_b0/test.py,sha256=RyrRTanG4fCwWk8d_EaFn9j5GVqLdDcubsEzKBM16mI,867
 qai_hub_models/models/esrgan/__init__.py,sha256=BYOPJKlrdJCd_dkZlkI9_rvh_Ip3TtrD70sT4hk6mWM,463
 qai_hub_models/models/esrgan/conftest.py,sha256=nAb4KPJgxCpWfgWGOakDl35g6NO1NyqrfFjhsfoPCBU,1405
 qai_hub_models/models/esrgan/demo.py,sha256=QL3qaDz7O56O938cS7fMOPl_bb1gvcfJhP8onfAcMf4,939
-qai_hub_models/models/esrgan/export.py,sha256=ZrIX3XdVuPrkugdkP753vKT192OpJsXt2JnaEbr6gSI,8283
+qai_hub_models/models/esrgan/export.py,sha256=zATQCMeY3VCrpU-ffYE-vnT8AjOAKjfOh70x18txIC8,8683
 qai_hub_models/models/esrgan/info.yaml,sha256=uO-MC4Ex0o9haQh7iX2SBYKmtjEHkeko_xZmkGRAlPs,1117
 qai_hub_models/models/esrgan/model.py,sha256=zitbXArjzUEyP9_u5Z4ePgLFGYxG4DFKiWBRPdGy9Ow,3473
-qai_hub_models/models/esrgan/perf.yaml,sha256=qPS8jr6-1auln7_o4msEQaXXigEWHS31rP9WA1Pw2ZM,4597
+qai_hub_models/models/esrgan/perf.yaml,sha256=FjoVsrZyNUuudJ_ZgCVh8JKKi8f1DhqAEcxIxSi9_Ow,6085
 qai_hub_models/models/esrgan/test.py,sha256=b4pQrPN0z66v__gKsgAcXqIfEViysY9yhstuOaiNXi0,1831
 qai_hub_models/models/facebook_denoiser/__init__.py,sha256=2p_IKIllP1Me0a2Ga3OfzoAWS-sarWBUoedNgVhknoY,418
 qai_hub_models/models/facebook_denoiser/app.py,sha256=FNIaoA5NYrjXxrOoL3kiDS7TI_MpuxKPldE3ru_tA_w,3207
 qai_hub_models/models/facebook_denoiser/conftest.py,sha256=I80EDdxb0xRVFjMoYWFb_mqBn7EuADI9KB6KugtSi1I,1416
-qai_hub_models/models/facebook_denoiser/demo.py,sha256=pwZ7sKCYgl2YcRWBD6L4WChECUCwu_OrUmMWcfoqQOc,3172
-qai_hub_models/models/facebook_denoiser/export.py,sha256=TzP4EYf26f2Dg9533nH-hcYTmmP_YDY8XGQMPGdw0t4,7951
+qai_hub_models/models/facebook_denoiser/demo.py,sha256=lFuuVXVjiBwW71cwb37NtdDq8FV_LV9nGgW2QO7oxNs,3176
+qai_hub_models/models/facebook_denoiser/export.py,sha256=72V18sjC7-YDkbfQtBPqcROvkfm6cv9Ba32S0jhqhOA,7943
 qai_hub_models/models/facebook_denoiser/info.yaml,sha256=uBJBGRUnVPzhU6dK-LACvms0FCVSzJh2qYiLtv3JoGo,1070
 qai_hub_models/models/facebook_denoiser/model.py,sha256=_mrUIQsgrhGH_5HH103yFgEfpe1GEgU_FZe1qfbVTQo,2414
-qai_hub_models/models/facebook_denoiser/perf.yaml,sha256=dFSqSfm1uLyGsIMT_gf3-Mu3VEhORyX-zP674JsSB3A,3431
+qai_hub_models/models/facebook_denoiser/perf.yaml,sha256=xJiY-HQIwFB4MsS9zhOW-xe97V8ym6PuHn7E5i4Ier4,4531
 qai_hub_models/models/facebook_denoiser/requirements.txt,sha256=o34BIQCgYdBbS6Yp3eup9VorcrIfdNiUoGtrK3AoiPg,74
 qai_hub_models/models/facebook_denoiser/test.py,sha256=kmnG_zoQOVGd45TqH-pJcRTDrHq_Y0TYymSPg2r7PdY,2492
 qai_hub_models/models/fastsam_s/__init__.py,sha256=t-LgQ3KECa94clPW6e9afDjg0_iE3_AT5wwGh9u0ViE,440
 qai_hub_models/models/fastsam_s/conftest.py,sha256=_oEpAzGrHUH0toAcao-aw-DbKtK7HKL0_DXsTI8a_DM,1314
 qai_hub_models/models/fastsam_s/demo.py,sha256=Ja3h5Trq_ZHYvEpQLp6AVki87esrcrrymRv0RpNp35g,762
-qai_hub_models/models/fastsam_s/export.py,sha256=UW1GJlZZeFGnWFMqJeL7pQz9WBDvIeeUeMLGlS7BSrc,8545
+qai_hub_models/models/fastsam_s/export.py,sha256=6-nuYKGpZb_GW7wEqCO1dF-UVbCEMae_PkS1vBSxxSI,8953
 qai_hub_models/models/fastsam_s/info.yaml,sha256=BGCkK4ixZsZXeXRq_6IUUBn-cc3mV-1Zcq73p3TjTlQ,1301
 qai_hub_models/models/fastsam_s/model.py,sha256=-ynkCngyRJIeb85n8GIsAmAWmAt_fiRcZ85pz1c0nPw,683
-qai_hub_models/models/fastsam_s/perf.yaml,sha256=dQRFG2C71pbE11NYtxZezWIuEjNcA8QBTQrcYqhJ7pk,3402
+qai_hub_models/models/fastsam_s/perf.yaml,sha256=DdohmztKv2l-7dt9MGAU3lfBOqhodbgpUzUX5onx4Pk,6071
 qai_hub_models/models/fastsam_s/requirements.txt,sha256=bzLR7n9PMXzABwKJJUEb6gOX23t_wZuYJkbyRPJjjQI,64
 qai_hub_models/models/fastsam_s/test.py,sha256=KJD_DBh8sto_x8A9VmEN-z2AyLBhTZyykZzGTv1KsMA,1332
 qai_hub_models/models/fastsam_x/__init__.py,sha256=1jZ4uim6OseYmBo5ppIM32GcilG-cEFb0CDmJAFvSPU,440
 qai_hub_models/models/fastsam_x/conftest.py,sha256=dsLBHKCGWnrbsTmDiP6YkPqWZHMjiVrHFpLSR4pxklE,1314
 qai_hub_models/models/fastsam_x/demo.py,sha256=7jJK0ZIIhiMh_XS_HOUmpek_XcKaeZ0TxQudsXzUBRc,762
-qai_hub_models/models/fastsam_x/export.py,sha256=hyI6MXzsWuvS8-34RRUDXMi36pKXPrgemgetWJPrY-c,8545
+qai_hub_models/models/fastsam_x/export.py,sha256=-COvWVMqTu5d9W7bHA2DybxngdM3SDs5vcIiDDPqdug,8953
 qai_hub_models/models/fastsam_x/info.yaml,sha256=Z3zYIUu7OAKfoOkoFd9j5jH-ZJII930iPflu_wFp12I,1300
 qai_hub_models/models/fastsam_x/model.py,sha256=15O2L72SANNVLxoWjNdugNV4hgUb0oZzdoxnNddQYM4,683
-qai_hub_models/models/fastsam_x/perf.yaml,sha256=V52hDrG37i-mOgxVjBmUamGKoH_QoU1Fi6fDwXzR_9M,3409
+qai_hub_models/models/fastsam_x/perf.yaml,sha256=XbqapXDeK8H9Iyboz9Vp_MeCofuygT3t0DELwJPVocA,6081
 qai_hub_models/models/fastsam_x/requirements.txt,sha256=bzLR7n9PMXzABwKJJUEb6gOX23t_wZuYJkbyRPJjjQI,64
 qai_hub_models/models/fastsam_x/test.py,sha256=QUaUSFuPrqESZSMF8R6c2zH2rGrvD7FdDXrcRjue8wk,1332
 qai_hub_models/models/fcn_resnet50/__init__.py,sha256=PIkLfZIDzf16OqQ6kptH6YVx_2MdAwWt3zLJJLtlbDc,410
-qai_hub_models/models/fcn_resnet50/app.py,sha256=U_aLzk8it6_-tC66l4Dz17-LgBJAfyIz9hPjKZGhO3M,2683
+qai_hub_models/models/fcn_resnet50/app.py,sha256=Y0Sppajf7njKCO3tSZO2HqxJHfejZmNPLZR00T7qwrU,2481
 qai_hub_models/models/fcn_resnet50/conftest.py,sha256=Q7Dn2LQRlW19oWeM8xkEwJ9Ak43uYj8xlrIW_K_xNgM,1411
-qai_hub_models/models/fcn_resnet50/demo.py,sha256=aHhtDDnA4ESHE4JkkOs_7EJtsTKYAZxW8Btz9Wxhcbg,2315
-qai_hub_models/models/fcn_resnet50/export.py,sha256=wUdRw9tcYYggjLIWyUuzhPVXwKBR5t1ry86Y32R6fV8,8450
-qai_hub_models/models/fcn_resnet50/info.yaml,sha256=ny0XW7G2TGqi6uH9OAY4Gh8qzgd-QXNPqe8hSnayDLk,1241
-qai_hub_models/models/fcn_resnet50/model.py,sha256=LFsVZFks3DkPqsp62A5d6-ABQdVrIKlvBD6Sx2wrHpg,1989
-qai_hub_models/models/fcn_resnet50/perf.yaml,sha256=zU9cHU_-bFye58uZbUDH2bt1aR3dCtmTuzMRGYmTSSg,4576
+qai_hub_models/models/fcn_resnet50/demo.py,sha256=IagHmoY7ArmYyv6c20I8oP_k62eotMt0W_IOUuR_bac,2470
+qai_hub_models/models/fcn_resnet50/export.py,sha256=E5CNw2w9yxEo2zJNcH8Hzi-hCZaEVocPXvL1vNVGcOg,8850
+qai_hub_models/models/fcn_resnet50/info.yaml,sha256=QgisMIQ7K3zjvC_yI5LgX4RUgVYwTgbOZq1eC_u76Tg,1254
+qai_hub_models/models/fcn_resnet50/model.py,sha256=W6HJvdV9bYzBkTaU7yISn10AsJLTX6Zc6lcoFyTSLvY,2362
+qai_hub_models/models/fcn_resnet50/perf.yaml,sha256=fvQI1GpBSujqBwcuiIkZw99g4dZH0qkf6IYogEthYMk,6084
 qai_hub_models/models/fcn_resnet50/test.py,sha256=mIM8NwhGPlSf9KBX1QIWek3rTLoYC_bBvqMbZ0mrK70,1637
+qai_hub_models/models/fcn_resnet50_quantized/__init__.py,sha256=HR-zYWE-BKLtXHqkJCX-tDho5vTdGRPkeQXvfkU6g4Q,456
+qai_hub_models/models/fcn_resnet50_quantized/conftest.py,sha256=uVH6NVCukRzOzTrkuawRFCktuCPrciUFcyPhXEejqik,1421
+qai_hub_models/models/fcn_resnet50_quantized/demo.py,sha256=KrxzX4cSgXnBkINhM7AslMB38hHEAhCQOSVyCdy32Ew,547
+qai_hub_models/models/fcn_resnet50_quantized/export.py,sha256=knXXdABAVrW605AWJUdmbR1XOcTnxyidQv_6-llFAJ4,9266
+qai_hub_models/models/fcn_resnet50_quantized/info.yaml,sha256=mj8aXe7pB6eCoNYJf45HYWuVPOVhjakU9GehFEtt1uo,1308
+qai_hub_models/models/fcn_resnet50_quantized/model.py,sha256=3mO4Ygv00B_xW7Bvv8DCFQM-VcNSrUK7mIGfqovDzl4,2829
+qai_hub_models/models/fcn_resnet50_quantized/perf.yaml,sha256=Pijo61erQ3lpw0TgR4S6wr2l7US89oGWrNm7qqwCwoI,7744
+qai_hub_models/models/fcn_resnet50_quantized/test.py,sha256=dbMW8iMXJTts6x45oR4jQCiSzi59Twmvli-LCw77UQ8,1414
 qai_hub_models/models/ffnet_122ns_lowres/__init__.py,sha256=UoCIQtMaEcM3Dg5PhPZzDxbNdV4d3k_Rw0im4Jak6pY,487
 qai_hub_models/models/ffnet_122ns_lowres/conftest.py,sha256=ic0oXtut6oAfpsCALVx7KLlVIwWIxTIkUvOaw0Ef6A4,1417
 qai_hub_models/models/ffnet_122ns_lowres/demo.py,sha256=t2c7R8wXMYU_1XHw0y7W08uSWZFlG0ExvHW6RCDdaXg,607
-qai_hub_models/models/ffnet_122ns_lowres/export.py,sha256=4ykCr0rSf-4k9klS5ZHv9iyE5Inu6-2WhpWsdMx3hcg,8331
+qai_hub_models/models/ffnet_122ns_lowres/export.py,sha256=V92GABoBsU4k23ituMj5zudBXiDWbIt1XeTj8SYxv7U,8731
 qai_hub_models/models/ffnet_122ns_lowres/info.yaml,sha256=V2mZJGqPO-kJJwTVNFzLJ_xmB661g--bhcRMwsGULE8,1322
 qai_hub_models/models/ffnet_122ns_lowres/model.py,sha256=Jxz7d6RB3rbuya27MvlQgDrjkovo-gejpCBhpQda8wc,648
-qai_hub_models/models/ffnet_122ns_lowres/perf.yaml,sha256=Yw18bbcdye-lAsmTNcWrEM3ntFXYTmoSptgVmpaWY8I,4585
+qai_hub_models/models/ffnet_122ns_lowres/perf.yaml,sha256=7qfn1vkdDAU8ez0bFZQVStCjSR_8jiiQ_J14e_kZ1HM,6072
 qai_hub_models/models/ffnet_122ns_lowres/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_122ns_lowres/test.py,sha256=izaltx2uBp4Sa9cxpa27oo8koqI9wMs4KykgoOYxxBc,804
 qai_hub_models/models/ffnet_40s/__init__.py,sha256=cur1hXK3ukU_DL_ujlWpJykMMaxCXzpfT6yqAoZX4w4,479
 qai_hub_models/models/ffnet_40s/conftest.py,sha256=P4JxddjLlJCqqtwXq0Zla6_XedTzdjud5ZTdVj4q-fg,1408
 qai_hub_models/models/ffnet_40s/demo.py,sha256=e5examY5jBnCcFrA73SOXV4sdY0gkOF_YheXwgLQ7GA,582
-qai_hub_models/models/ffnet_40s/export.py,sha256=iMjRCHvT90aqK44GvuNnVZ6PIuetym65gC8Pmfms_0g,8295
+qai_hub_models/models/ffnet_40s/export.py,sha256=m3HOgN8r9dsgTxwerPnUOarN9P3v8aMDN8PduKPuLr8,8695
 qai_hub_models/models/ffnet_40s/info.yaml,sha256=DbHnioJHDD2KHUyYryhPGiss8VnBKHvKUaeM1C6QgzQ,1298
 qai_hub_models/models/ffnet_40s/model.py,sha256=vMqOf4XTdSWVlmM_72tBvjNtks874DkBM-aizeN5QEM,580
-qai_hub_models/models/ffnet_40s/perf.yaml,sha256=BPqjO4-jtVZdeDdfQgoHsK_fPRjJXA5ikD_RDh0ifsA,4571
+qai_hub_models/models/ffnet_40s/perf.yaml,sha256=QxzCigCcTplI8XxAMGh8-glKgMByXjigNlLRtynodOI,6073
 qai_hub_models/models/ffnet_40s/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_40s/test.py,sha256=Z-7FI6ko8Dw9B54eKzGZKYjPqJRT9iUumz3VaMMyzoE,746
 qai_hub_models/models/ffnet_40s_quantized/__init__.py,sha256=D9fLMk2pWGuTgkIU7jGbxtkoiWLnA7pajSv441yVE7w,490
 qai_hub_models/models/ffnet_40s_quantized/conftest.py,sha256=Xmad9xsA67ko118-nMnXSilR_VWkbo1ofbpJVxiGeDI,1418
 qai_hub_models/models/ffnet_40s_quantized/demo.py,sha256=a2SWiqFS6qv59AOAJgHvbBpRMMiFXGVI1v26YqxKZtU,627
-qai_hub_models/models/ffnet_40s_quantized/export.py,sha256=Thd1hkMANXVT-iWb90LWofYbLZedLbB1i_h9nNzwQto,8731
+qai_hub_models/models/ffnet_40s_quantized/export.py,sha256=_dK2-Q9HXOjXPSRE-e8VNKdQ11BiLx7Vd04iusj8oNk,9131
 qai_hub_models/models/ffnet_40s_quantized/info.yaml,sha256=ZRi81cKXx7ih9hPKERPXwN6hIrpI4k0Q3ci8KfXsRJQ,1347
 qai_hub_models/models/ffnet_40s_quantized/model.py,sha256=_QVWRQb90OhW_0UCzzjy4-Hgc4V9DKRPrkvzhynHL9M,1169
-qai_hub_models/models/ffnet_40s_quantized/perf.yaml,sha256=k_7IWVvg17PVOd4nOu82KsicRyoKBRKR6eo96Bvu8yo,5112
+qai_hub_models/models/ffnet_40s_quantized/perf.yaml,sha256=AMtT6T66TqQjdGhkj0YH2iu77Q9Fg_kSKHH605v8g-M,7760
 qai_hub_models/models/ffnet_40s_quantized/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_40s_quantized/test.py,sha256=11tNLKpWi4Wz1D99W0NAW531ENyWDNYpfxi6sMfy-UI,840
 qai_hub_models/models/ffnet_54s/__init__.py,sha256=Zhyq437g7aiixQVmV1yF2iZSE_0L_2Hb53mZab_fH78,479
 qai_hub_models/models/ffnet_54s/conftest.py,sha256=wW6j8OlrEqTkSi66o7fpnDGk3zhC4uzaa6bO5WiVqk0,1408
 qai_hub_models/models/ffnet_54s/demo.py,sha256=6oplfV5nQdodhEYRApOupdZMZ6Jxk-yxUwJS-j1zYyo,582
-qai_hub_models/models/ffnet_54s/export.py,sha256=PasZRSw4EtgKl74GwGf8iAl5y0YOpLrzmIt-IsALVo4,8295
+qai_hub_models/models/ffnet_54s/export.py,sha256=z_Zhxs9_5qZTJ2ILOUK0Rmr91JVDHWYem-Xrtk6Z9YA,8695
 qai_hub_models/models/ffnet_54s/info.yaml,sha256=ipbP8LhRKP6hky2-CdAHgusN0zu4RcJLNkSj8UziC7M,1275
 qai_hub_models/models/ffnet_54s/model.py,sha256=uOu3W3LoWHUB4cUcHQDIRcn7d2INjhVn7nju2WxG7sc,580
-qai_hub_models/models/ffnet_54s/perf.yaml,sha256=f1fxwCDU_pZmMdoNCshXv1z2XSxIK__HaEAFsgO7Jqk,4591
+qai_hub_models/models/ffnet_54s/perf.yaml,sha256=XZqorLQKH8PC0kE_-ptqEbXe0BKrKkoRfPGo4P959ig,6082
 qai_hub_models/models/ffnet_54s/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_54s/test.py,sha256=wxVmLWpT7VdTph6c4ktD7yYNyCeB-ms2aYQyZnWNXII,746
 qai_hub_models/models/ffnet_54s_quantized/__init__.py,sha256=exZ2v9rO0y7x3f0-V2qTvVLoeOjbmqdSMpHDMI9-cfE,490
 qai_hub_models/models/ffnet_54s_quantized/conftest.py,sha256=jA4965h2ygKxkMtNN__VJpB32QsEmcLaQvWO_cbjxH0,1418
 qai_hub_models/models/ffnet_54s_quantized/demo.py,sha256=QGw6h_1O-Ecp_yQJgAWqOgaGHPRQ2SvIrSrNf-_RdSY,627
-qai_hub_models/models/ffnet_54s_quantized/export.py,sha256=O7gHcR75MdUFzgJsSxe21BsWHQk7Niw6YeJXJW5_sQA,8731
+qai_hub_models/models/ffnet_54s_quantized/export.py,sha256=zXGsvuRau0IKRqMtvVtEehKdn2T11DczNSCq75lj3Eo,9131
 qai_hub_models/models/ffnet_54s_quantized/info.yaml,sha256=whLq0baglC4ajzuhqqU7qvBh7cJBiK8yn61hUBalMOg,1347
 qai_hub_models/models/ffnet_54s_quantized/model.py,sha256=itN2MVSiS5LEUk9CcQfKFNM5JeCNrGQRwjeukp_7k4U,1156
-qai_hub_models/models/ffnet_54s_quantized/perf.yaml,sha256=bNKS2nI5AVG3h1dOKFrIqP2ngAHihkBcNKCMfE2FzY4,5127
+qai_hub_models/models/ffnet_54s_quantized/perf.yaml,sha256=vjp4I-DQUSyK3db72yFgXiQFLTCbgYabUcu5HlJkcY0,7787
 qai_hub_models/models/ffnet_54s_quantized/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_54s_quantized/test.py,sha256=QLlA7Q-IcJJZFYgXrFRyw_RyhULYRZkCcyXEpSeG1_M,840
 qai_hub_models/models/ffnet_78s/__init__.py,sha256=EZpm0M4BbQeAEyxW-qlt-qCFp0idi3Kbyo3qkhmtoIc,479
 qai_hub_models/models/ffnet_78s/conftest.py,sha256=-mp4Facal0kDqXF-BUr0qGcSfZyb26wKyxaiCwDgnG8,1408
 qai_hub_models/models/ffnet_78s/demo.py,sha256=4b0QiRJKM-uCuJbmhV5MlO9k8k9Y8nYHs3khMb8-IGs,582
-qai_hub_models/models/ffnet_78s/export.py,sha256=2fNLZu77hDuUJyrKJi4WD-KXDi_ArXL2YbJHfVJ572g,8295
+qai_hub_models/models/ffnet_78s/export.py,sha256=KILwIt9LSHmAzbsu_xoTy6STq789v66DP7VkVm3WuiY,8695
 qai_hub_models/models/ffnet_78s/info.yaml,sha256=eTVW7aCWkN36QgKXhmSHmuKgJE907R_6ZaFBHLLzY_w,1279
 qai_hub_models/models/ffnet_78s/model.py,sha256=zxXU_ZSi81vLKD92yok1BS3vgLgLkS8p2sfGJKZT0Os,580
-qai_hub_models/models/ffnet_78s/perf.yaml,sha256=QhypgLe6AsSuZyzD74SGpZd541mvOWfdd5PLR9_KnBE,4585
+qai_hub_models/models/ffnet_78s/perf.yaml,sha256=5-zg38sorqTI8f2MzJ9VKNGgNUbFiM9kvtX1hfgYmt4,6086
 qai_hub_models/models/ffnet_78s/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_78s/test.py,sha256=rgdUjMRz67XdsnmHkyNH0ba3uvbXYyVfe5PfVnbekEs,746
 qai_hub_models/models/ffnet_78s_lowres/__init__.py,sha256=dGFCFlJOiHyZHYM-RC3aU-dT6dU1kGGAXxtoFtI1GPY,485
 qai_hub_models/models/ffnet_78s_lowres/conftest.py,sha256=eAzaEG2nMd34C7UpVOPQw5QjraxZnEwdZDjIPi3Ea0E,1415
 qai_hub_models/models/ffnet_78s_lowres/demo.py,sha256=DW_YMV_N9oqo3jn2VLiN6QjKDy2n70Y6UzTxABv7XAM,601
-qai_hub_models/models/ffnet_78s_lowres/export.py,sha256=NcYvFOX_XUTWTyuHvGnPqWSHGgDJucHseTNUL5hrRFQ,8323
+qai_hub_models/models/ffnet_78s_lowres/export.py,sha256=wBQeRcuoLy83EmdWDDViOzL-a_kCIdlk2i-oSf3PVjg,8723
 qai_hub_models/models/ffnet_78s_lowres/info.yaml,sha256=Mgrc9Bf84QxaywT0AIXKUx2zorycN8lQ2bCBl_VLN0g,1327
 qai_hub_models/models/ffnet_78s_lowres/model.py,sha256=ckAt5bunYsfv5pqH9jrwREUfZQJwaKtcgBCjOz-Vw3I,640
-qai_hub_models/models/ffnet_78s_lowres/perf.yaml,sha256=Heg9ftF_lC8KNY0LC1VOxc9vdzoy9OAXL2A1zl-_lN4,4582
+qai_hub_models/models/ffnet_78s_lowres/perf.yaml,sha256=V2wNpXF4tmNGhk6awA-zA4NzpUW_USQY9Er6xhxq4H0,6073
 qai_hub_models/models/ffnet_78s_lowres/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_78s_lowres/test.py,sha256=XuLMLq71pxQcI6SDVEqtwyl86MaLtxa9mtlLDfBgSSk,794
 qai_hub_models/models/ffnet_78s_quantized/__init__.py,sha256=byB0kzkpkVxI7x6LjOGtge9X58UkVzOkp_VjfUdJrVw,490
 qai_hub_models/models/ffnet_78s_quantized/conftest.py,sha256=8k14iOWqxpaGBYG4hUjBG36cdfgNwyiPUouglSjTAg8,1418
 qai_hub_models/models/ffnet_78s_quantized/demo.py,sha256=3mI25tVr9FGDR2y5_2PuYvKJ5vHCzE7Ogweps8oziAs,627
-qai_hub_models/models/ffnet_78s_quantized/export.py,sha256=cO0rzDxZDBpoglUl6jNErVSwXg9uEgRoqeAZwilkYkQ,8731
+qai_hub_models/models/ffnet_78s_quantized/export.py,sha256=fFXfzP9kRDVEi_pWFGMqNQHuoFOD3Kv569OA06ywBrU,9131
 qai_hub_models/models/ffnet_78s_quantized/info.yaml,sha256=WdgiQxWS3hXRs4A3xeP-Mis44omsqUNaQPhIHFYJ2r0,1347
 qai_hub_models/models/ffnet_78s_quantized/model.py,sha256=gilgE2OHPv4mCaKEr4AAcFyFzAdUGq92_RIvhbcqNTA,1156
-qai_hub_models/models/ffnet_78s_quantized/perf.yaml,sha256=GT8Tmamo9KO069jdvpjMOvY9ljeKn0PVvngIr_KOoFc,5123
+qai_hub_models/models/ffnet_78s_quantized/perf.yaml,sha256=PT24IqPNBZ-XOy-PNTdAgRYGdEYEZBF06LyPZKROujg,7789
 qai_hub_models/models/ffnet_78s_quantized/requirements.txt,sha256=cVS0KTDz2ekpU0ckkwEtt7cSABHx0HhXfXNS6RFHX5A,21
 qai_hub_models/models/ffnet_78s_quantized/test.py,sha256=f63BCpe22Yh5AtLy3HmZ6M37zu3G7kr18SmevBwh-VI,840
 qai_hub_models/models/googlenet/__init__.py,sha256=yMXfFedJcmupf20Fk8xXuArMIPqOdIvxB2S_AJ5j1Pk,472
 qai_hub_models/models/googlenet/conftest.py,sha256=KG0q37N6PuXolmfutSCwyqkepcjCMYtTgssfBCFJ90Y,1314
 qai_hub_models/models/googlenet/demo.py,sha256=Owxr4VAkRm-resJoBLK_1ZcAEnI2IwjHZ6Hgqs7zYXU,533
-qai_hub_models/models/googlenet/export.py,sha256=CVvEHTccEr0lCTr2ICPHp2tCHbttIr-q_64MbhL6nhY,8160
+qai_hub_models/models/googlenet/export.py,sha256=JTEXJ9jsTCtZlX9q8P_RiRkJnS4_oat8pVP-x3dlc60,8498
 qai_hub_models/models/googlenet/info.yaml,sha256=6Pb1-Auz6VGYb2FEYhCoKR6a7sJxkm-6eJ9KPoGz1_c,1295
 qai_hub_models/models/googlenet/model.py,sha256=7tVmhQtiQZeWGgeYTPVvwKr4ustB0tHoYafJyXZvVKQ,743
-qai_hub_models/models/googlenet/perf.yaml,sha256=UfJjBv4W_ilVFz1n0dWbCnXNL3J8QmejS3uH4nr1aow,4547
+qai_hub_models/models/googlenet/perf.yaml,sha256=uqYHYmxdOo1mthHQVOXqeOSszishU3fc0iF1VACMWiM,6029
 qai_hub_models/models/googlenet/test.py,sha256=gJcMSpz8KmfhCwa9E0d6iTnlOB40dHwsr8Qp9B4QdAQ,840
 qai_hub_models/models/googlenet_quantized/__init__.py,sha256=96ENoBCt6lWCfOBrvFQJIE9xh-sA3IDtfUXmdIWVhqg,573
 qai_hub_models/models/googlenet_quantized/conftest.py,sha256=8bXw-7g42_83uQXzGwuYHomFIudS1YRmWbCfg6OEKpU,1324
 qai_hub_models/models/googlenet_quantized/demo.py,sha256=FT-qPTxt9WnT7-wPR0jitsw0LnhvkbHlRCfJE_jiB-Y,578
-qai_hub_models/models/googlenet_quantized/export.py,sha256=8QJIrEd2YxeDxlKbGaKRC9zrJ1bCcGxJ-zp8sEdwZio,8623
+qai_hub_models/models/googlenet_quantized/export.py,sha256=lT2Hfu15ic5tTUC92rGRYBuOQW_9t4jd-YUrxTTI9rY,8961
 qai_hub_models/models/googlenet_quantized/info.yaml,sha256=VmjYexBXTnmukRqI8u0IEJYGZxMbKxze2l-Q3ROPyC0,1325
 qai_hub_models/models/googlenet_quantized/model.py,sha256=vGyWrxc-hmzDhR2HSVudaBPosPrPdodjZ6SqUUa7qIk,4298
-qai_hub_models/models/googlenet_quantized/perf.yaml,sha256=qHTkMbg7k1vpKMyFsQua_tCmu2YsHV0KB8TD4ZPssXQ,6623
+qai_hub_models/models/googlenet_quantized/perf.yaml,sha256=8fMOEOpbyUtfzOPDZrMJY-fxB8CRUpkX5EC2JwpyWZw,7700
 qai_hub_models/models/googlenet_quantized/test.py,sha256=ox8Mz4egNJmOQzfD-gNNXu9k5zHs7ZHKn1rV3xlNNd0,885
 qai_hub_models/models/hrnet_pose/__init__.py,sha256=_r8rdSD0i5yJRlSct5_EQpCV-hEExVZYmI_mHV5VkWQ,430
-qai_hub_models/models/hrnet_pose/app.py,sha256=3Ui8V3bt6Fu5dfB0KPdo9fAVLqsltG8ZUqS9p15PXOs,8458
+qai_hub_models/models/hrnet_pose/app.py,sha256=1Aq-ei8rISyu1zdAjHf0H5NKY3P7CNnwMSQ6EfTdAck,8458
 qai_hub_models/models/hrnet_pose/conftest.py,sha256=5ksauTz7qnrrGVO3Tu2c4Kp3Z7Xyn07nSNwRmoUC-ZI,1409
 qai_hub_models/models/hrnet_pose/demo.py,sha256=-XbTGmCKHJXe-qoj3Abrm81VZr1FGiJQ-Za6FBVFBww,1739
-qai_hub_models/models/hrnet_pose/export.py,sha256=x5WN5qZ47yI9CdqkR7Rqm7wMe4Bk378Uvg9u2FSY2mU,8441
-qai_hub_models/models/hrnet_pose/info.yaml,sha256=QdjXu2cv2b6EpWTS92p4iSu8-D2YmcRv59Zdxae7-yA,1195
-qai_hub_models/models/hrnet_pose/model.py,sha256=eYdiilsjMO8kuvH-FGuompMoMqHg2Ad-dT1Go75OLuc,3360
-qai_hub_models/models/hrnet_pose/perf.yaml,sha256=ODYLJANa5ue8ZWYSz6AakPq8LNRhygePZwJR3tHOWdo,4557
+qai_hub_models/models/hrnet_pose/export.py,sha256=zh1bnKnIW0jYaB6dtOdSL4cqgJPjel5q8-eS_kDpzXw,8841
+qai_hub_models/models/hrnet_pose/info.yaml,sha256=qNtmOErzX69g2FowQi2TIjJPQR54MbMQT4CfslJH9zM,1196
+qai_hub_models/models/hrnet_pose/model.py,sha256=OzYNKeWjMcDhtF9-7jrToa6wxeToW8ES8zYYbv_TqX8,3360
+qai_hub_models/models/hrnet_pose/perf.yaml,sha256=eeQJENYFWBnze1RLqp0JJAl2NOaoNtIeqJXMyNr95ak,6037
 qai_hub_models/models/hrnet_pose/requirements.txt,sha256=2U8K4vxuQ1x6tB35TVOiLW4UA0gN77o0ZvKJ5baKAgo,51
 qai_hub_models/models/hrnet_pose/test.py,sha256=LHAh3hxZfE1aeQt5Cwvv4SF3w6X3Dxz4tly_UxZpr84,1420
 qai_hub_models/models/huggingface_wavlm_base_plus/__init__.py,sha256=muVZdjzxn0rzXTLE50i1-birAkTSWucNt1VWyCoUwJ8,434
 qai_hub_models/models/huggingface_wavlm_base_plus/app.py,sha256=QlmoRNNkpzNGI-EzErNXsR8S-ygiC09QdAiS7ggnQs8,2133
 qai_hub_models/models/huggingface_wavlm_base_plus/conftest.py,sha256=h1bUo9zjWU7pN-OKUKPU_rQYSivMqTKWyhNK-yA_ovM,1426
 qai_hub_models/models/huggingface_wavlm_base_plus/demo.py,sha256=NtjLWFe0oiJgcm10f5ws6a4SIx-7RNbRLL-Q3pB3csY,1517
-qai_hub_models/models/huggingface_wavlm_base_plus/export.py,sha256=Vu0wDt9lhCrUBA6KH78Sa7LDW8m2E0nx8-dNVMknO9k,7848
+qai_hub_models/models/huggingface_wavlm_base_plus/export.py,sha256=IPFJybunrVrvIzeQVcjn0CRjTO_2Gq7xhxR2vwE_euU,7860
 qai_hub_models/models/huggingface_wavlm_base_plus/info.yaml,sha256=fJzxA3WwfNBQnWZq65pR90IGEHzlm5hxoJHvzEhVN1E,1248
-qai_hub_models/models/huggingface_wavlm_base_plus/model.py,sha256=BjWZCqVFXNJaINoT1San9dY2aZOAFbZjJ_WKQLV-CGg,7587
-qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml,sha256=ESPCK_Shr8yhKwJG_IQE47GkmhNYGqOQ0NjxgZbPAgc,3446
+qai_hub_models/models/huggingface_wavlm_base_plus/model.py,sha256=Ahcbrm5v6GOFZYtu5NeYCX0URGDtvhIUEWkFVell5Nw,7764
+qai_hub_models/models/huggingface_wavlm_base_plus/perf.yaml,sha256=1roXP0O8fgQ0Z-vXTkM7hdy5CnEiBqs3w3djxcDnrog,5540
 qai_hub_models/models/huggingface_wavlm_base_plus/requirements.txt,sha256=L9-RRcRpUwd13vGIthoV8IQaEALYng-Hd7RhR1hdpSE,72
 qai_hub_models/models/huggingface_wavlm_base_plus/test.py,sha256=irHDKl6YChuYRBq8zcNwe5p7CLd9ghY7yicSfHEtBZw,2560
 qai_hub_models/models/inception_v3/__init__.py,sha256=7VMcF8m79rmZ-UZk265C_tIMIHJev0qZFBz5cNzSd98,477
 qai_hub_models/models/inception_v3/conftest.py,sha256=qX9wf2lRJjAxyJTWsg-DpbRCnH1dCaD5rAP4Sppbj_E,1317
 qai_hub_models/models/inception_v3/demo.py,sha256=f9m6H1zPOHEGK1fi4PLDUYPb8TYoa8a4MqbjRWZBX1o,546
-qai_hub_models/models/inception_v3/export.py,sha256=Acz_grkLrrrcU5gjrBQSxzW99Frz9Topdn2udEzOsI8,8172
+qai_hub_models/models/inception_v3/export.py,sha256=4MV8zfigDcErfLmTy2ZaGSDz8m61HpD7QAKM3JLQTls,8510
 qai_hub_models/models/inception_v3/info.yaml,sha256=XoUgkOsDw_vFkVOOMARDWbBGFdcsRt2EcGqIqJZC_BU,1359
 qai_hub_models/models/inception_v3/model.py,sha256=PKOe7hJigPXwymvjHC8Q_mwVFj5LTp96cIPA6hoaDto,756
-qai_hub_models/models/inception_v3/perf.yaml,sha256=CEDhNficaLvvMUkoodHNcXhpHxRmYFKYsL4nmf4cVJ0,4565
+qai_hub_models/models/inception_v3/perf.yaml,sha256=881LowWQiQoeWOXFIwyIiY5YP0vfRpL6LbhRkzPRr9M,6043
 qai_hub_models/models/inception_v3/test.py,sha256=H4y4OTCinQAYkJvaceCOenfWzwjPqpWb2kk_hvSm2WM,861
 qai_hub_models/models/inception_v3_quantized/__init__.py,sha256=LYwSg6XsJc3GL5Bvm8AjHw5I2aCmG_kmpyQQRxxYlsM,584
 qai_hub_models/models/inception_v3_quantized/conftest.py,sha256=R8oCGdmloUxRZ8TmFvJYpqW7Q4-sLSChRl5c1udk_jQ,1327
 qai_hub_models/models/inception_v3_quantized/demo.py,sha256=HT8ebEriXX7NZpj9rdy6CKBWSdpM10pxniEvhwVFe10,591
-qai_hub_models/models/inception_v3_quantized/export.py,sha256=6lG_w1N3fx3CJoVKHftKVi6e_WC045ZqZpgXHSvtkko,8636
-qai_hub_models/models/inception_v3_quantized/info.yaml,sha256=FsSC1SKNT3quSNQaELEGwfybCmlNi1L2xiRI2oV08CQ,1556
+qai_hub_models/models/inception_v3_quantized/export.py,sha256=0iOeijRvNzS2jZWeMNfBp-NVQESmBNAdlrThCZ5rhng,8974
+qai_hub_models/models/inception_v3_quantized/info.yaml,sha256=vd-4pSKQF0L3GrIUTeaq8Dlm6u9-XzY8_S0fKvEUZ00,1496
 qai_hub_models/models/inception_v3_quantized/model.py,sha256=3eASb8PKybBcQ67prsdQYAddsmwdsYDIiJmWGKOuoT0,7762
-qai_hub_models/models/inception_v3_quantized/perf.yaml,sha256=ynfTPM08cL93a4WWfyyV72MbdbRPUNo4m5gm9HcvR_w,5106
+qai_hub_models/models/inception_v3_quantized/perf.yaml,sha256=W36dAkQc-yxmIbdj2clJJoXm_Q31Grz6b8vvlu1Nx9w,7750
 qai_hub_models/models/inception_v3_quantized/test.py,sha256=Xf0YrWDwy01ulM-o4IuAOQotc6hOU0Dn2IaV7h8SrA4,901
 qai_hub_models/models/lama_dilated/__init__.py,sha256=XfDJqXXsCsai11UnjLNnECFN6WquE9I55knTyDl-R7E,455
 qai_hub_models/models/lama_dilated/conftest.py,sha256=6Rizw6XyqvqEZrgrfy7xnOgKVOhDsdKOhjgtMxVkqVU,1411
 qai_hub_models/models/lama_dilated/demo.py,sha256=J8E3pR2GQTpmpqFfqPiQOfKUcywmDMPAzyUqIj2wMZY,911
-qai_hub_models/models/lama_dilated/export.py,sha256=HGxcqO52GxFK7sf1P8fWoweZOXrNdxjidP1Kc-LNhSY,8460
+qai_hub_models/models/lama_dilated/export.py,sha256=iPiMZRmhZGDI9y92nGbNRchw3KTHVTHUOY8bLl92Drk,8918
 qai_hub_models/models/lama_dilated/info.yaml,sha256=whgpaBOBqJnbIc6ErqRS4PTlaeBIkoJBEtrfscPeC9U,1082
 qai_hub_models/models/lama_dilated/model.py,sha256=uJp3NNmPQuzBXZ_gcgk1pK4DDkG7JmO_R-KlpbfpLy0,4977
-qai_hub_models/models/lama_dilated/perf.yaml,sha256=GoUbF8Rg10nX67oU03ibSWVY--uTJkY8QDXZwQuCYc0,4545
+qai_hub_models/models/lama_dilated/perf.yaml,sha256=C9miul2UI5jqVRwk0U2z14eMi7YsCpuGatMtOoFJerA,6013
 qai_hub_models/models/lama_dilated/requirements.txt,sha256=897JpKdjy0fMli83RtBp6iSBaWYRHXNDK0oMaO5aMBo,171
 qai_hub_models/models/lama_dilated/test.py,sha256=g1YxEMQjYA65848I8YM0lxylfy4f0LGSJ38pJXrPOYM,2074
 qai_hub_models/models/litehrnet/__init__.py,sha256=lboiEAPcBxZN_ao_VVtBVJl-B9EvrrwXcxK02PiZbLY,404
-qai_hub_models/models/litehrnet/app.py,sha256=FEwh94V2uJrp6bofDJTP_p53fR85Ir-nCiacakJ90BI,3986
+qai_hub_models/models/litehrnet/app.py,sha256=Exg8SPfWW5Ljzw2PRYASX5AjrK6W6ACmAuLzIKs3Qg8,3986
 qai_hub_models/models/litehrnet/conftest.py,sha256=4YLcFq1IKhh66JyVpAwE7o1rsjIkW0JZJVVb7HGPW7U,1314
 qai_hub_models/models/litehrnet/demo.py,sha256=N3NDwdOwfdjFANzufoZq2mBE0yk4vTZZFQCl_fgk9Ds,2072
-qai_hub_models/models/litehrnet/export.py,sha256=QQ721z16MzMP8DfZdNaL8BWEGfbMY08GyGujziS35X4,7919
-qai_hub_models/models/litehrnet/info.yaml,sha256=xfMHffAMgyP3Z0UT9aBd5_C5yF4QYifTOvXd4-yGEDs,1112
+qai_hub_models/models/litehrnet/export.py,sha256=BcmgX_6GgmItxayFY6PRR6UF5B3diHft-gP9DLjadgI,7931
+qai_hub_models/models/litehrnet/info.yaml,sha256=lfLHbAo-wHc8wdCzM24_Uyfotu3ftb0fHR-AV_dz4yE,1113
 qai_hub_models/models/litehrnet/model.py,sha256=XELmt0HE-7-ZHQ7VOZAJNdAWBqR2Cg-zK_0ifR4UxGc,3788
-qai_hub_models/models/litehrnet/perf.yaml,sha256=ITkeSH4dj8dF9EHb_pVidiAOWhWdqyNloao3qQm2wLY,3366
+qai_hub_models/models/litehrnet/perf.yaml,sha256=f7_VK678w0uXsdr13XCs_De6NgonP9-X2SGwiz_mm_0,4428
 qai_hub_models/models/litehrnet/requirements.txt,sha256=CFGaEErf8SbXCYCCjg58TvJZIExg-yQiluC2q78zZoY,39
 qai_hub_models/models/litehrnet/test.py,sha256=FStsyYV21iqYE_DTsLCDyFBISas7lTXpvHrP86Xxc3M,1714
-qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml,sha256=F6lfh0zDgvKfB_1WU1uIKJcq8B22zPHtgbt6cNBnpdU,1997
+qai_hub_models/models/llama_v2_7b_chat_quantized/info.yaml,sha256=A-2cTxZMqe09jGIGlsxu_oOShcIaEZ_sshf5zMTFMWU,1954
 qai_hub_models/models/llama_v2_7b_chat_quantized/perf.yaml,sha256=c2QHcYkaKgtRAYZzmxq3dU_AzAA2UiAaj948gNGZVRk,2039
 qai_hub_models/models/mediapipe_face/__init__.py,sha256=24U-bkhm4gXXnVLowI7uytJShrDYSq5KbKWGxkK-xxM,412
 qai_hub_models/models/mediapipe_face/app.py,sha256=eug0kpc1nRgnnhO94P242qu8x-P15nmK9YDCRVi_4Ug,2099
 qai_hub_models/models/mediapipe_face/conftest.py,sha256=_mXXKud8SCZkaVm_5kok-6u4Jya-s_0eH2vhQUYKBEE,1413
 qai_hub_models/models/mediapipe_face/demo.py,sha256=k43EC7_YMXA4Wk41JwLaCtUc_1HSQFEyZ5UY317fWG8,2862
-qai_hub_models/models/mediapipe_face/export.py,sha256=jQBrhnGnhLybLzd_19MrGqJ4XvRLXMxd6E2oaLHydeE,10089
+qai_hub_models/models/mediapipe_face/export.py,sha256=vZejgCP9aPQYN3seKfbeo88orDyWy3-L06vpfY0f5ZQ,10022
 qai_hub_models/models/mediapipe_face/info.yaml,sha256=QX3h5VsxsrDt0X6b0QpMelIhilsHJr6MatZewTwF2N4,1469
-qai_hub_models/models/mediapipe_face/model.py,sha256=WurHj_hb5bf8eOyryuBUAKzOC_mZkRgeM7fPpvERoss,7669
-qai_hub_models/models/mediapipe_face/perf.yaml,sha256=pZvWZNNxET6iCXpFU6YzbZfnEqUB46CfGOUDddF6VCs,8427
+qai_hub_models/models/mediapipe_face/model.py,sha256=uMC5zrTbvxEI6K30GJUuZWBpe6EwNJay6G3VAGAjKus,7669
+qai_hub_models/models/mediapipe_face/perf.yaml,sha256=RdE-3mhAY1qeB7tT1H1OqFpDcFkeJbiGZIrNbo0zHUU,11312
 qai_hub_models/models/mediapipe_face/test.py,sha256=eAgJ0ZjbMNL1bRBF6Ma-mVQ0hBMTQvbUEWO6PH8Gr4I,1441
 qai_hub_models/models/mediapipe_hand/__init__.py,sha256=XWXPWZ12S5pgPBmT3fwgql_JF_6Y_dY699q8nntteVQ,412
 qai_hub_models/models/mediapipe_hand/app.py,sha256=4-UayauaiZDVytVsclGrBnqvGjVQNf0PPCUwYVk-KLQ,9819
 qai_hub_models/models/mediapipe_hand/conftest.py,sha256=sroHsUr1p6RTvvcxk45k6rP03ylslgLjVQHlh0SdigE,1413
 qai_hub_models/models/mediapipe_hand/demo.py,sha256=u8ej7boLAgN-0O--m_lXFTVXb-4WW3qU4WKR5sijEJo,2832
-qai_hub_models/models/mediapipe_hand/export.py,sha256=YUgyHcmmSKF7-VPIZgRF0GUBps2F9N-rgYUdDHHL2AU,10044
+qai_hub_models/models/mediapipe_hand/export.py,sha256=h-uQn5F-DaVQrY6cna2t6B7f03o3TmVcpPpvINjqmTo,10022
 qai_hub_models/models/mediapipe_hand/info.yaml,sha256=oSKmBY6rTjq_Jj8ToPWD7eIu6cxkVpB1RGuQ23rb7WU,1374
-qai_hub_models/models/mediapipe_hand/model.py,sha256=Mjz_ttgFHlNUsWIKbcIGEU3j6TMiOLHZvmnF0eY1df4,6017
-qai_hub_models/models/mediapipe_hand/perf.yaml,sha256=Rn6sFtHD12hzLzjnYI9UC3z-fO22iclDwW3RAMWhG1A,8437
+qai_hub_models/models/mediapipe_hand/model.py,sha256=Mhc0fPWVc_He7eUuLSxgEGy_AcnVXrXVhXbj9l6JnAg,6017
+qai_hub_models/models/mediapipe_hand/perf.yaml,sha256=ZIqZnRKUncGvuHqiv6gd0LkrtSNYQIkwk6mtMoW3OYQ,11335
 qai_hub_models/models/mediapipe_hand/test.py,sha256=y-8RhCNppYbhLY_znVZKYVNwElIaJPdSkXEfBoyfGcM,1442
 qai_hub_models/models/mediapipe_pose/__init__.py,sha256=PE4aVOSnDXWvU9YVcKjBKUyO8926RMaeRXyD1chVXpw,412
 qai_hub_models/models/mediapipe_pose/app.py,sha256=djV3gcRkcvw2qYHBXqOjxY_2DxNahekk4BZ4Iax6uW4,4430
 qai_hub_models/models/mediapipe_pose/conftest.py,sha256=-8fIHIDPA5Q-p0t45CuiVNtzL_uZH6XzxAeqLg9Dl6g,1413
 qai_hub_models/models/mediapipe_pose/demo.py,sha256=hvy2P0bX0dZ9cU0NrxSN8MjJqfat2M4GHlXjKcvixO4,2889
-qai_hub_models/models/mediapipe_pose/export.py,sha256=tuAmOy84LiAWQogoLm-wvCcU5I-btwGdHtqSnFNj8HU,10045
+qai_hub_models/models/mediapipe_pose/export.py,sha256=xDNBJmW5Gx9Nj9ijFVaQkUf2hpl1L4yq4p1GeWhujt0,10023
 qai_hub_models/models/mediapipe_pose/info.yaml,sha256=D2BVSmo_qZbGJIbONvBpj2JJcJFGtQA78VBMyInXkIA,1387
-qai_hub_models/models/mediapipe_pose/model.py,sha256=oPrNq_v0zKYU0OL84GwAKt9yUsdAaMsH4lDNsPB0k1Y,5801
-qai_hub_models/models/mediapipe_pose/perf.yaml,sha256=PM8KocYcEbCQKaWaVkUeApZlOVk5XPeLo7P5AA0GL7E,8435
+qai_hub_models/models/mediapipe_pose/model.py,sha256=S9M4Ei6Ohkv9N_psB7w3P-FbrVOjcjIgYSKL1MUPdkw,5801
+qai_hub_models/models/mediapipe_pose/perf.yaml,sha256=yYjcsVzdRgKqlVFnAf5urGonBWGQfuvggmTndH2zS2c,11330
 qai_hub_models/models/mediapipe_pose/test.py,sha256=rsTIYChYPqBL1lI9cl4y_QR6jCmRhb4FgAjcwhLSGA4,1443
 qai_hub_models/models/mediapipe_selfie/__init__.py,sha256=TrPNyvFmctjh5bMloBxJAIFZTgojNj16AOnPBH6T8R4,362
 qai_hub_models/models/mediapipe_selfie/app.py,sha256=nWD63c4A6_M0gwC8P9n0dTlzsXL45V9Ep8I6hy3Ytd8,1411
 qai_hub_models/models/mediapipe_selfie/conftest.py,sha256=wTMdTYYTOWWEK0OA96C0WeDhQZ47Pv6VE8krG2UAPNA,1321
 qai_hub_models/models/mediapipe_selfie/demo.py,sha256=DULze0M58GkHmBIpIEoB6So-EsgYoqHBC3CQspRSR4Y,2674
-qai_hub_models/models/mediapipe_selfie/export.py,sha256=1JM1ciGZUPsBEZqWZiYPlDJ1I9FtPdIpAiBU-J3mKy8,8479
+qai_hub_models/models/mediapipe_selfie/export.py,sha256=0yE-ZZotBYxx501BJzr9NbnEWgu4gZVlubq1ZK5ejwQ,8879
 qai_hub_models/models/mediapipe_selfie/info.yaml,sha256=gEUPVKMaB0_qlPbEo1zM8uQ3dqAPK6YKQWHUNNOFdMo,1455
 qai_hub_models/models/mediapipe_selfie/model.py,sha256=COQHgx4VM0auz9h_etmnh1l3nQGU2BzmhP4qoqdIR_M,12352
-qai_hub_models/models/mediapipe_selfie/perf.yaml,sha256=G2MkBZukovkgjWP8CNQxuLoUw-zPTlQf_OgzrI_rXMw,4583
+qai_hub_models/models/mediapipe_selfie/perf.yaml,sha256=Lnln70OR6U_GL0SMQ5AyZnpJ2e3-cPBCBu7DI2wE1Co,6059
 qai_hub_models/models/mediapipe_selfie/requirements.txt,sha256=jjmBqpB3WUtMLBjVgPXNNN5yslq_EXOWdM9WMOES_a4,15
 qai_hub_models/models/mediapipe_selfie/test.py,sha256=5VWzDmnNS9jz90tZhqTPIIFaBUeHQ8GYGJRnmpvIavs,1396
 qai_hub_models/models/mediapipe_selfie/utils.py,sha256=-Fh52J_5r4Ty0YZiKtwSnP4yi93K3kpBc3-Y9jPCEUk,2492
+qai_hub_models/models/midas/__init__.py,sha256=-Ipqym4MBHzqk-bH4f1ot6eC4ajTOber8_txmBah2fg,396
+qai_hub_models/models/midas/app.py,sha256=Ap6iXlyVa4Zi-kK2wd5RN1hw_vsI0z1I9SGtp81pgXQ,2254
+qai_hub_models/models/midas/conftest.py,sha256=n1MTj0zkbXdp65i5qHi0ATlGZDNlBbunKEgQNO8d9Sk,1404
+qai_hub_models/models/midas/demo.py,sha256=p0Pp1slGJGwQE_XyEguoPT0LGjWmmZyod_HtJopxbj0,2144
+qai_hub_models/models/midas/export.py,sha256=xy5PlUy7hfO-db4QFs3AxM8UEcvG2FXRnF5fjBp0I7c,8474
+qai_hub_models/models/midas/info.yaml,sha256=l6mp3avgpaYDvJeEnbIbuVgfHbwFF8HRs7KGdq10I40,1100
+qai_hub_models/models/midas/model.py,sha256=If_ebqLf3TDnhFwxRV34x1e1ZF-JgN0K9I2y4yQPC8Y,1811
+qai_hub_models/models/midas/test.py,sha256=NxH8_nIoBNiaPQlwMmjE9BnllfisbmY45pv8nryp6As,1919
 qai_hub_models/models/mnasnet05/__init__.py,sha256=d3xeB69OYbrgNbOBMVMVsEqwClH2MMhlJqhfSUIx8UY,472
 qai_hub_models/models/mnasnet05/conftest.py,sha256=NXm_I3IX1pUwEMMPte8jmUXWFNkhtetc0uaAfzwfxMA,1314
 qai_hub_models/models/mnasnet05/demo.py,sha256=Fgz0-kQofu_pf78mBfAA0kcenEwRIYtoLuaofrbcfYA,533
-qai_hub_models/models/mnasnet05/export.py,sha256=SldbNVN6E5WZjjwM7CEjIS2t6fQvdy71YwpXJWFgwfw,8160
+qai_hub_models/models/mnasnet05/export.py,sha256=S2CnlkhBEk_gTo0FlVN0rpkQL9mjEBVPooBrKGsvXvc,8498
 qai_hub_models/models/mnasnet05/info.yaml,sha256=A7bKsnRyqJJ6Jnx8SMQmn6EBaOMvHwXW08xOCLdT60o,1333
 qai_hub_models/models/mnasnet05/model.py,sha256=kkgjfQ0BbYuE_ls2Yh1zHIqncG678OyyvGTJGJemIjo,699
-qai_hub_models/models/mnasnet05/perf.yaml,sha256=FWn6SwNKfAYmibBks0Le1giaWV1CgZdIhAwr6dQLIL0,4528
+qai_hub_models/models/mnasnet05/perf.yaml,sha256=UUmxaidAqympZIxSWIJSyVG8wEQN2y5fc_0Noy0VCQA,6026
 qai_hub_models/models/mnasnet05/test.py,sha256=Vcc7zhvyUHQXDj3mcFZetvZmV0y36askMPKqPZRJ1i8,882
 qai_hub_models/models/mobilenet_v2/__init__.py,sha256=EF3fi_jeNIAmKhFrVJdKaZjazdFlsYXrW1TZT3fDaxY,474
 qai_hub_models/models/mobilenet_v2/conftest.py,sha256=r_wUwYWkmuceVH50ftHmgbdfXepuJRneZfQ96zEpjgI,1411
 qai_hub_models/models/mobilenet_v2/demo.py,sha256=GR2uHGNaKtqt-bIO7rjjgR2V0fdziHAQWwDZs7FvZV4,540
-qai_hub_models/models/mobilenet_v2/export.py,sha256=vnkg1FacVBIg4GEdKLf1OPS0ZvyiM4FhjYiudjXRPnE,8172
+qai_hub_models/models/mobilenet_v2/export.py,sha256=9b3mlVnOvgXCPJ2hBvNPog4GwF2HFEKroTGEn4SDp9k,8510
 qai_hub_models/models/mobilenet_v2/info.yaml,sha256=irPP5qDlcSbPbjkqJSO4k_iA159GEVB1u4pIruYAlyc,1380
 qai_hub_models/models/mobilenet_v2/model.py,sha256=Tn-8Pf3YJEf0dXEOgoO2XYWquN8AtKSCYOtE1XJaHNk,2457
-qai_hub_models/models/mobilenet_v2/perf.yaml,sha256=MddUazC5xUNu3Sjo0gsbBE6A46EisCjrZ8Af2_gy6kY,4553
+qai_hub_models/models/mobilenet_v2/perf.yaml,sha256=1pyYmPCkPiKZXkKLVWKBzHpf3nKRjhZ7EICLQTRlAks,6029
 qai_hub_models/models/mobilenet_v2/test.py,sha256=Kjj6s-5jkKoqRaV7GkE9zf7js34k37uBSluNMFM3V-4,1091
 qai_hub_models/models/mobilenet_v2_quantized/__init__.py,sha256=MQOqYDYL-abmQsTEsRl7dBpFWu0Vd2mAW64gj02npN4,485
 qai_hub_models/models/mobilenet_v2_quantized/conftest.py,sha256=QjSfo5VicitRAzkuz14oKseEztLYNb6rPN4bFaxG75M,1421
 qai_hub_models/models/mobilenet_v2_quantized/demo.py,sha256=oxbLdz1mYSbr0GA4W6nRtFvnAp9Muma8F3eTP2VEKNo,585
-qai_hub_models/models/mobilenet_v2_quantized/export.py,sha256=HVIE43btnCjjEhiTFDI3vpg713c69a0xAVvsdztg-Fc,8636
+qai_hub_models/models/mobilenet_v2_quantized/export.py,sha256=Hf7D48j-XgIxHrC-AUvbVc8RUlvoqqcdAF6ewGOFlHQ,8974
 qai_hub_models/models/mobilenet_v2_quantized/info.yaml,sha256=B8RS3tsewoOtaOVFYWCp6aVqKJ2VIiCK3GJHfz8ghPo,1362
 qai_hub_models/models/mobilenet_v2_quantized/model.py,sha256=gye1E7azV4sqwZTh4113gmdytvv-SHf1sPOgSmFny88,3429
-qai_hub_models/models/mobilenet_v2_quantized/perf.yaml,sha256=bHG3p_hnr273plVtUWv6aAVfxQ0vJHvx_4G883euPdE,6626
+qai_hub_models/models/mobilenet_v2_quantized/perf.yaml,sha256=qWYiar_c0U7vhcNRYcbZjVtkYhDpmn6RMEwlNB2ry5g,7719
 qai_hub_models/models/mobilenet_v2_quantized/test.py,sha256=h54ZCxWmJA87qmOXeFpYZl8_3wLUQgDAJgowNlk_QhM,1002
 qai_hub_models/models/mobilenet_v3_large/__init__.py,sha256=7a-2Ng3rVspm46Uf4wyI8WfezV5IVaKm-oCfdkM0JVQ,479
 qai_hub_models/models/mobilenet_v3_large/conftest.py,sha256=rk5NFEOGN4kuCK49JsWa8iDOysqt1fVh2s9gQE0kXYc,1323
 qai_hub_models/models/mobilenet_v3_large/demo.py,sha256=4KxbUpanGuUgvVcRk-YTTf90uQlEsMaBhbpjmXYEPUI,556
-qai_hub_models/models/mobilenet_v3_large/export.py,sha256=4HzSxL1HlZ7xz9ovkU_Ncv1ZSkYeeh_R1wx6FD0s9ZY,8216
+qai_hub_models/models/mobilenet_v3_large/export.py,sha256=Co2eHYBOLzCUx8mCaXp5eW3xJfej6nqkqFMu40Y_wMI,8534
 qai_hub_models/models/mobilenet_v3_large/info.yaml,sha256=CbC2G_Zw1YshI-danUA20dWHRyzP2ZFsqHnVZU1nBUc,1340
 qai_hub_models/models/mobilenet_v3_large/model.py,sha256=AjaEjXhIddKyIg6FSzzGloeZibD96zC7HORYX7MZOqw,721
-qai_hub_models/models/mobilenet_v3_large/perf.yaml,sha256=V3tX_Km-DZZnq3EuFk-IhS1dd3IS03ZVUbfAogIzlqo,3392
+qai_hub_models/models/mobilenet_v3_large/perf.yaml,sha256=O9ub6AA2435ft4945trJxT5kWuH9hg6p-N4UPoUoq5g,6038
 qai_hub_models/models/mobilenet_v3_large/test.py,sha256=BKu3TBqXd6EelITtwPlPq58e20I530caSxC4vahg_MA,879
 qai_hub_models/models/mobilenet_v3_large_quantized/__init__.py,sha256=HWjXMytQDRVZh4KUJTPN8z_oCl6vbY5dtI1lC6C79ZI,607
 qai_hub_models/models/mobilenet_v3_large_quantized/conftest.py,sha256=V-2BAa7ZQPyxO1Nmgch5prnoJPmMlKeNeeVGPAD9OXY,1333
 qai_hub_models/models/mobilenet_v3_large_quantized/demo.py,sha256=XKC0CItIVpPeV60QD8VR7zAKBasKdjjcLZGuutvTm3s,748
-qai_hub_models/models/mobilenet_v3_large_quantized/export.py,sha256=f-X87IauD1Roih9AG7UIatzAXhoDvlGLyEZp99bEpf4,8368
+qai_hub_models/models/mobilenet_v3_large_quantized/export.py,sha256=Vh1eQIyu53ZIkxNbxu_oBzqOp91RMdF7ck8YQ8wSypg,8950
 qai_hub_models/models/mobilenet_v3_large_quantized/info.yaml,sha256=w72_OYX5316wOqxjPIrMG1CD4q9mqAsRbE1tnsS7IOM,1374
 qai_hub_models/models/mobilenet_v3_large_quantized/model.py,sha256=4H8KV9f1ju-Wv77NqjmXTJaI2FXffV6sYLMDDgxi7rw,3075
-qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml,sha256=O3MlwhBUrVeNwYNW1qyFyiM_lfUhCfm4Ns3VDNEp61k,5123
+qai_hub_models/models/mobilenet_v3_large_quantized/perf.yaml,sha256=G57p7Gxa482EPfyBYnSy64B--e3sjXqK_254VzEGGdk,7744
 qai_hub_models/models/mobilenet_v3_large_quantized/test.py,sha256=ruAffWPJDAifH_miRT6hNoOnaNY7tJpSXZ42U8zM9Yc,917
 qai_hub_models/models/mobilenet_v3_small/__init__.py,sha256=RhAKXSevAm64F3DtEwC1whT3RiawVPUuZRpHxupCwvY,479
 qai_hub_models/models/mobilenet_v3_small/conftest.py,sha256=-bA5nmhYo-6fvaMmH5_LgV-SXzc1sYWrg2zwyWuQEro,1323
 qai_hub_models/models/mobilenet_v3_small/demo.py,sha256=f2RqGEK_t9UJ2MNkIYTPnAkeNhOk5ORtBgORks8pQzE,556
-qai_hub_models/models/mobilenet_v3_small/export.py,sha256=2O1IaPXT4r6O3FBtrID9KVcYFrqKQR1R7A-frc0pV9Y,8216
+qai_hub_models/models/mobilenet_v3_small/export.py,sha256=jyesQJOqYuFUT8dKRnSgbIFae-h8KZ4nR_YmOONkwGc,8534
 qai_hub_models/models/mobilenet_v3_small/info.yaml,sha256=1UbgDdCaGry4M8mc17lsEQHN0c7dzg1OYujzQncLDGw,1338
 qai_hub_models/models/mobilenet_v3_small/model.py,sha256=AMSPekj2w7Odv-HX_YUPs5AIFiDMDmbaq-77eRM2MtU,721
-qai_hub_models/models/mobilenet_v3_small/perf.yaml,sha256=14u3wQu306ykCqpCTMnIghT9t0TPb7uRSElNIcPUyQc,3396
+qai_hub_models/models/mobilenet_v3_small/perf.yaml,sha256=_7F7SD5pItC0Zbf7fXBzPTOTBuW4uusfRPZYn9E9tcw,6042
 qai_hub_models/models/mobilenet_v3_small/test.py,sha256=PkDBlLPWfV8ktLcict2uFWM7pn2nl8B_NtKHKIp9njM,879
 qai_hub_models/models/openai_clip/__init__.py,sha256=zmmQCL2hQl0hA_RIPa0pQrKUYOXbMBQWmaYYlSy-XrU,394
 qai_hub_models/models/openai_clip/app.py,sha256=hgOce5Y7aS2Y_V-EltMyp6CkLRXQDL0PTqelz-pBf9Y,3958
 qai_hub_models/models/openai_clip/conftest.py,sha256=rJ4rxdfv_hfnNfwrOCcUuxDfRdSzAkckAFSEH0f_tQU,1410
 qai_hub_models/models/openai_clip/demo.py,sha256=3zSDWd1n8MHfreShD06TLyTTivGTmydTLeiA1UUJy9Y,3404
-qai_hub_models/models/openai_clip/export.py,sha256=elz1iWJqlsmLQgoIWdgKhH75eHXdmmkySXk0TLpSBdg,10000
+qai_hub_models/models/openai_clip/export.py,sha256=2nh7wrs2oP5U2KkMTeoT7F6YnxjpvZms8iNNTHf-Iy4,9933
 qai_hub_models/models/openai_clip/info.yaml,sha256=tZQO7SbgP6M2JFAeoGOzl6WnuLhG_lyfWbiCc9WtpMM,1494
 qai_hub_models/models/openai_clip/model.py,sha256=KSRCG8Z9_KE_PyGUoTpzMSH41s0xvXEEnbGrg3p0hLk,5374
-qai_hub_models/models/openai_clip/perf.yaml,sha256=gHAIb_FzMc4vpys4IsNmtCe13wihaSl0kJhoswKVk8w,6070
+qai_hub_models/models/openai_clip/perf.yaml,sha256=PUwgLFFiMRqMFeZg8iqNaLR5vDfcUZGBomw6Zq_143s,11341
 qai_hub_models/models/openai_clip/requirements.txt,sha256=qCIDOVe-LijMtnGXDgJtHPUP8Yig6otJASVYYaxE4JQ,29
 qai_hub_models/models/openai_clip/test.py,sha256=PV-_Wn-pUdg4M0DbbemnPREGDskjQnv8yqm19L9M2L4,2118
 qai_hub_models/models/openpose/__init__.py,sha256=DJnYvOA5-007mXB0GUl1TnKyBSNqFsQYlpGCpixb9Jk,402
 qai_hub_models/models/openpose/app.py,sha256=MrS_HKYzz8-ylXJH9_x2jEJg1H0ZZDBaYaWDOmme-Uc,12008
 qai_hub_models/models/openpose/conftest.py,sha256=EHKFXp3E3-vyR4Og9fU3JhPrjOpnLCse4_zaK8iJZTQ,1407
 qai_hub_models/models/openpose/demo.py,sha256=rPuIuHf7S_ncKOIFWoit8rR1jFG4uvHrAZ_NviMthuI,2053
-qai_hub_models/models/openpose/export.py,sha256=iMH_Z3RM9t0Q8WPrU9yCP3xT9qqhiZWP122qv9e7Kco,8452
-qai_hub_models/models/openpose/info.yaml,sha256=zZAZK46yA0iB2H7zccZHf0cxuQjdMdO3osgi1K-hRZo,1246
+qai_hub_models/models/openpose/export.py,sha256=9msgX-vCtvBiCaZwyx2_bEcZnOR31dd7N7coMN73CIQ,8860
+qai_hub_models/models/openpose/info.yaml,sha256=CkiRkTaBKL5wnVXPdO_fI8y-scfKsX_KdtwRnNhSz8w,1247
 qai_hub_models/models/openpose/model.py,sha256=iWpglU2znltOYKlQ9w-1xBt9yrrySJW5YhNBAH6lMkw,5084
-qai_hub_models/models/openpose/perf.yaml,sha256=mHhw8IhKgQAk81GfaVDKpWPWARDrPG8k4dqQqFBpzW8,4572
+qai_hub_models/models/openpose/perf.yaml,sha256=sGeEcb4WHrlPI64YT1SmudKbvpaLNVGSu-k_Buc-eys,6061
 qai_hub_models/models/openpose/requirements.txt,sha256=0xg0sn4HQZDSpN03SNwngv61Ry0AXQZvwPHrXtSuR00,31
 qai_hub_models/models/openpose/test.py,sha256=JhSAWSbHck2xqktdTa_2RAaBdoJOL86g6jLigVNHrgU,1321
+qai_hub_models/models/posenet_mobilenet/__init__.py,sha256=Zwg0hNRlea_9eo2yMzN6CSyC80JZDyTCrMDAz7hUzbk,402
+qai_hub_models/models/posenet_mobilenet/app.py,sha256=NOD7EYAc0VNpT_AFbLcWCiT15N1AjFR1X0_VsCw24Ac,20176
+qai_hub_models/models/posenet_mobilenet/conftest.py,sha256=t8Q9cfkUjsn4dyO1hYo7ZmHrLmE3Dt5IDJyzKC_x51I,1416
+qai_hub_models/models/posenet_mobilenet/demo.py,sha256=qzZDHrPMu-kZhbeZuOC9CgQfSaa9srUasUrrPWCGhYM,1998
+qai_hub_models/models/posenet_mobilenet/export.py,sha256=-TAzavvMTXVG98B3TNOtyymxIIsyLwkx6HHNnw5fMPc,8486
+qai_hub_models/models/posenet_mobilenet/info.yaml,sha256=uGDUVpns1toVdJRsjwk3MdRnPV1FvWd5WDk7iUPQHG8,1221
+qai_hub_models/models/posenet_mobilenet/model.py,sha256=LfUaLXzIo_siV3eXdAQfgexsBkcWIIZPjSDoAIh6QIg,2661
+qai_hub_models/models/posenet_mobilenet/test.py,sha256=PaWOM_GFPX2snoJ2LbyB5Qs7Uh8NtYKGCAW4DuHlaU8,1757
 qai_hub_models/models/quicksrnetlarge/__init__.py,sha256=n0s0M56rmewoor90_VOznUSWRXF8hBUbBkwMNjKg1xQ,472
 qai_hub_models/models/quicksrnetlarge/conftest.py,sha256=cgAWpB2Y43uSdUEMn91Tbx0WVdMcJkC6sr-LDbhaJx8,1414
 qai_hub_models/models/quicksrnetlarge/demo.py,sha256=IgRtHWqQVLGsuNXJTb9kwaWkiR9xuz36Qnhc9Z34jKA,972
-qai_hub_models/models/quicksrnetlarge/export.py,sha256=9gKWn-8mXf7ySkKe9ZjeS8GdguiqMyYImtB4fpCetzc,8462
+qai_hub_models/models/quicksrnetlarge/export.py,sha256=Mnp5cAosavhsJUZPqzkB1GFmpzrTf0gir9JymUdgeZ4,8862
 qai_hub_models/models/quicksrnetlarge/info.yaml,sha256=jwBhAffyc2KTYL7kvsETHA6a9gvrc0XEZ2vEOgaRqdU,1248
-qai_hub_models/models/quicksrnetlarge/model.py,sha256=DZpev_PEQicNemo1YZgQ1PXNZG0QgRuvQ53H2DNdHWc,3170
-qai_hub_models/models/quicksrnetlarge/perf.yaml,sha256=W_RXXBBTB3aSqyvq15h5wnryYB9qRmHGtor4a8Dw1Y8,4555
+qai_hub_models/models/quicksrnetlarge/model.py,sha256=v-X_WDeLwzYkvfULaESwYjofk6b2-ti4_Z9AUR8Avak,3140
+qai_hub_models/models/quicksrnetlarge/perf.yaml,sha256=ieAo0fjhHYSu8bbSio9WcBZW5i7mDmtJmby0A1-z-5w,6029
 qai_hub_models/models/quicksrnetlarge/test.py,sha256=HICEbmnkI-D2g1MYjiUGkzMcl1Sg9NjC9JS7vttnveU,1422
 qai_hub_models/models/quicksrnetlarge_quantized/__init__.py,sha256=kpW-qloH72jOP0aPE4r_0ZBcV8MackQ1Ro4yiunY9sM,483
 qai_hub_models/models/quicksrnetlarge_quantized/conftest.py,sha256=gBUr78hNbQe_2YCqX0AMIpjSTka3cYhWFL4pO0MLFqU,1424
 qai_hub_models/models/quicksrnetlarge_quantized/demo.py,sha256=gScOAfsMlVs9d-GBMW8zHfPeP7CJiFawQldlU1CVuvQ,891
-qai_hub_models/models/quicksrnetlarge_quantized/export.py,sha256=xw3tIDNA4aU2-G9TdTD6Ip1RtuM2eEjZHGj8-u5CcoY,8898
+qai_hub_models/models/quicksrnetlarge_quantized/export.py,sha256=B8DPP6c6Zx_Vo9kqqk6TZqvEM65wL69dtScK-xqJPd0,9278
 qai_hub_models/models/quicksrnetlarge_quantized/info.yaml,sha256=qcahBBa33N55My5C2mINys5t3l41qQOFE3h-nLFR-ec,1274
-qai_hub_models/models/quicksrnetlarge_quantized/model.py,sha256=XeQCDOktDx1m2s56AlOEGwcsfUCkvI6i_MNU5VWPErI,4587
-qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml,sha256=Tcn_-tngDBRtdecap65fjdmDO3olrkmjBvFiLXQlot4,3932
-qai_hub_models/models/quicksrnetlarge_quantized/test.py,sha256=C7ytsb9iOxrsxGz__BL5XrGU2xBqYUwM9A1XOa6--Zc,2921
+qai_hub_models/models/quicksrnetlarge_quantized/model.py,sha256=Utrvwm_JW9b_FwbEtqMk1BBqOYhTXArHdLhLSA-LKmE,2980
+qai_hub_models/models/quicksrnetlarge_quantized/perf.yaml,sha256=_Y0P_EUj-tZxBWLCRIYzkx0KlROjMOIfKArlXX1xyRI,7710
+qai_hub_models/models/quicksrnetlarge_quantized/test.py,sha256=aVouW6V55RkbpSJ4_5JYWzR6P9yDvAb8bgWE1spKxbY,2925
 qai_hub_models/models/quicksrnetmedium/__init__.py,sha256=GTyPcoDHE8Hz702v_Bk__uhwi2N0E2KqyXxPQ5wpY54,473
 qai_hub_models/models/quicksrnetmedium/conftest.py,sha256=DB4QeL-aok28nxmwvITQPtS7TpJrYinykdGhQTR7XHs,1415
 qai_hub_models/models/quicksrnetmedium/demo.py,sha256=EPLslU6kBps_nPCOUFcfVYWE66rGNIRGx9zC8gAoOes,976
-qai_hub_models/models/quicksrnetmedium/export.py,sha256=UqUMI5pyU3rndWHruZ8y601RY2FGnmTALBFBv6xcW2E,8466
+qai_hub_models/models/quicksrnetmedium/export.py,sha256=ap6oTLdfRUdB8X-G863z6hxfXILjLvMLgPeJ7H_ZJ_E,8866
 qai_hub_models/models/quicksrnetmedium/info.yaml,sha256=qF6K6GLHopGKnuT0H3OBAwFMssi5KAt-4LW2ijIHWd8,1242
-qai_hub_models/models/quicksrnetmedium/model.py,sha256=ChFo9Fr8BHeFB7b_l6FfiWwE85fFZMMlmh_AxTxoCE8,3177
-qai_hub_models/models/quicksrnetmedium/perf.yaml,sha256=6UxYjjldX9TaPmSy2LhowneUmlZqbfnhgLy5lHU11Ws,4554
+qai_hub_models/models/quicksrnetmedium/model.py,sha256=pbtCYOXq7pkTXGDZBju7qbPWW1ZovC7TFxZfHa0R3g4,3217
+qai_hub_models/models/quicksrnetmedium/perf.yaml,sha256=kvb3M0rc1J-y9_ps8k5PnBCUbLQIYlVTPaD5pZDFB-0,6013
 qai_hub_models/models/quicksrnetmedium/test.py,sha256=7MmkvCPJul33giNuZK8iAyRN11ci7Zw4bOq8LcjW824,1428
 qai_hub_models/models/quicksrnetmedium_quantized/__init__.py,sha256=1S_5eD4OACNMsJuLjPa1A7qoFokADw1fpuU3MtBkx6M,484
 qai_hub_models/models/quicksrnetmedium_quantized/conftest.py,sha256=k5oeKWvIP5oH9yEMhmhVmTJxhZ6KgCfvY8jueF5UWYE,1425
 qai_hub_models/models/quicksrnetmedium_quantized/demo.py,sha256=6AGfl5ee02A7ZnM08cIJDOG14YEpQ1gWhJvPqBNVHEE,900
-qai_hub_models/models/quicksrnetmedium_quantized/export.py,sha256=zF_-spk5ATOfz_5QeXzNiDv3Uxk-98z7REjhJIc_fEQ,8902
+qai_hub_models/models/quicksrnetmedium_quantized/export.py,sha256=eA01GytINvFquxAPRsHqByK7gIKQ9eR_JhWF4dmhvio,9282
 qai_hub_models/models/quicksrnetmedium_quantized/info.yaml,sha256=oas-ZZ1q7bu5_jE1tiAPIAvsbbCRii6V-9piQUVh1Tw,1282
-qai_hub_models/models/quicksrnetmedium_quantized/model.py,sha256=QtXYgZiBkUH4YXV1w-1jFmvDVYFLafAIlnxmcHYI19M,4597
-qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml,sha256=Wzi1GCUXD1CG18gjdehngpm1T1MVcq3f_Cw6wbrShqg,3935
-qai_hub_models/models/quicksrnetmedium_quantized/test.py,sha256=g10Mox6qmH2HVihYkCXfOeBEo6CVGDA5CEXsuMIG1Es,2912
+qai_hub_models/models/quicksrnetmedium_quantized/model.py,sha256=rpSUHuIhrY-7ByBUqHcAFMfuaCWA_cNUFVFSoStX_G8,2988
+qai_hub_models/models/quicksrnetmedium_quantized/perf.yaml,sha256=zE6Sv4THEmLqvoqSh5v5S0bx5g9KVqLX-KxSBSDRyao,7724
+qai_hub_models/models/quicksrnetmedium_quantized/test.py,sha256=ItMdEAdVFKkyZRSGPBIIPTWcIdlEZumjj6UpBMmstUk,2916
 qai_hub_models/models/quicksrnetsmall/__init__.py,sha256=x3REqFLXfs8YWq2ld1olPTKLwdQ5aRavepT-WIj9f2Y,472
 qai_hub_models/models/quicksrnetsmall/conftest.py,sha256=Bzd6X75IGiPiue1s_4U6xM1w0HKvXh3aNDw6GVoxtB8,1414
 qai_hub_models/models/quicksrnetsmall/demo.py,sha256=y1Z_GTvbWhOaJ7iyVMqTYaUxxePBhUI0J0bguRaEUB4,972
-qai_hub_models/models/quicksrnetsmall/export.py,sha256=-FcI0gtqUe0YnKaZuXQ9_jDlvk95dH9CKuu6f1shphk,8462
+qai_hub_models/models/quicksrnetsmall/export.py,sha256=Dl3zK8mD1Z9PgTBI3P6ipvmuLW6avmUD_5PQuze2bI4,8862
 qai_hub_models/models/quicksrnetsmall/info.yaml,sha256=cPEJEcn1_ctwMF7nHQ9-AeUsPaq9mhjhfb4Gm77Vgp8,1238
-qai_hub_models/models/quicksrnetsmall/model.py,sha256=6cufAzO8C3wItx4iQpkXrexkV5W9nUsi8z9OUcIhogA,3170
-qai_hub_models/models/quicksrnetsmall/perf.yaml,sha256=CDccV19aNgO_3cubSoakWcw7pZKKGp5bd0_OuNW_X0A,4546
+qai_hub_models/models/quicksrnetsmall/model.py,sha256=zzoA3Ejs0RXgahmKxr_XTDY5JudoY7NHqHL1vaslRm4,3140
+qai_hub_models/models/quicksrnetsmall/perf.yaml,sha256=GSG5lCzmVrdfWYIygRrCuXfpySBqHBRG79S5rkCSKFI,6010
 qai_hub_models/models/quicksrnetsmall/test.py,sha256=d3OvtLPqk_3uzP2cSQMa6G4MJu9kx27aXOLBaeCzdPc,1422
 qai_hub_models/models/quicksrnetsmall_quantized/__init__.py,sha256=DYwTcgaBjIFARfMJ64zY-VPAT1DWz0kdcMunIa0uNuM,483
 qai_hub_models/models/quicksrnetsmall_quantized/conftest.py,sha256=k3SFCtby_o6U06FOTiUoXQJLdFffSkkwN7dG9eVZHKA,1424
 qai_hub_models/models/quicksrnetsmall_quantized/demo.py,sha256=mYIQyNQniXfQEVkmli0ft40hNwiXkeXkjUJWhT08YvE,891
-qai_hub_models/models/quicksrnetsmall_quantized/export.py,sha256=gmy33X5dO7mNfr9QzcVby-C3ldgJV5pgVAWigaa_ETU,8898
+qai_hub_models/models/quicksrnetsmall_quantized/export.py,sha256=6jpTeDs1EYGr4gJwlFMCWZdVqOtKwhkOtxLGGr4ckCM,9278
 qai_hub_models/models/quicksrnetsmall_quantized/info.yaml,sha256=OiqbT9kVtT8AxmvJ0QTU3i-kZwOaoDj2bkc9F7iS4mk,1278
-qai_hub_models/models/quicksrnetsmall_quantized/model.py,sha256=GfLJVnZxxDgWFcGmhHJbEdX28SYgW_QoqlBCdbBZroc,4577
-qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml,sha256=CnIxFzRzXG71MU-nQlopoGiiRRJAn_9BA1TpBlJM3Sc,3932
-qai_hub_models/models/quicksrnetsmall_quantized/test.py,sha256=bQdBBfrTuY5mN24JOeq8QYVz95uoQkw3NxFG9cUuVn4,2859
+qai_hub_models/models/quicksrnetsmall_quantized/model.py,sha256=pcfMdGkvAW6lmK2mnU6eLsn_JhSM0EvqZURsWtL7OIQ,2975
+qai_hub_models/models/quicksrnetsmall_quantized/perf.yaml,sha256=q4Uky6Kzzk2Lue--MaEGNmtZcl32FEz7RHL5n6eHJRc,7711
+qai_hub_models/models/quicksrnetsmall_quantized/test.py,sha256=1EbCksWcV2i1EqeSonml36DiGLXdQP_eLJaKmw_hNWc,2863
 qai_hub_models/models/real_esrgan_general_x4v3/__init__.py,sha256=-eD72P0MMcOI5S_6ZbVDAtQ5MAsZFiAZ2-Rs5wkfGCc,481
 qai_hub_models/models/real_esrgan_general_x4v3/conftest.py,sha256=hJY_booqt2Gd904osWkT08IKF_vSn7Q1bo8EtZGR8eI,1423
 qai_hub_models/models/real_esrgan_general_x4v3/demo.py,sha256=mdJUw7jmSDTPXDVyGHAOMgI-RUQdcjhvdLqju0Zeskg,1280
-qai_hub_models/models/real_esrgan_general_x4v3/export.py,sha256=denqqxbWKnPw2LKWU92CsuG7VRKbgn03wTTCEHInuOM,8498
+qai_hub_models/models/real_esrgan_general_x4v3/export.py,sha256=khAKCxJjcHan4nNgbbavkMuozzQZ7pGBw1GIJzk4Q5g,8898
 qai_hub_models/models/real_esrgan_general_x4v3/info.yaml,sha256=pKlB9DD4Qw5WDJgKW9xuxvccY-48a3vXY14Cp5M7RQ4,1206
 qai_hub_models/models/real_esrgan_general_x4v3/model.py,sha256=hBBSs9w-CVg2ueIi_BxNKgaqCrI5YRf3ABM0HJK2gfw,5435
-qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml,sha256=S0EOLjf4SX4qFsLvC5YG0ZBO0yGp8VL8BKCveATmERA,4580
+qai_hub_models/models/real_esrgan_general_x4v3/perf.yaml,sha256=bvV7KJqBMKNmH_-CmEbVLTSCAAJE-BIkoemPd3ju1o4,6051
 qai_hub_models/models/real_esrgan_general_x4v3/requirements.txt,sha256=eivyaYj7iqy9Z98WwPoKDVihhM_HDbjOBu49PUryfJI,44
 qai_hub_models/models/real_esrgan_general_x4v3/test.py,sha256=CfOp0wKNA7sXZihxigo9inRQH-AQLkdzeg_o5KMKZIg,1480
 qai_hub_models/models/real_esrgan_x4plus/__init__.py,sha256=9EdQhD9AYsj6uAL3JlTGCNhMZc-2MiyfpZDJEmjspMQ,475
 qai_hub_models/models/real_esrgan_x4plus/conftest.py,sha256=hIJbBMJWuYVBnLbn6XUc64sHl34twzivNigkOQj2WvA,1417
 qai_hub_models/models/real_esrgan_x4plus/demo.py,sha256=_X0Jc79lhoHqB01NfCidG-TFBMaDtez-4Ipc5DCCL8Y,1256
-qai_hub_models/models/real_esrgan_x4plus/export.py,sha256=XNyE1x5ss124oVcDa379Ep06kwWvfB2QjJuFJ1qs9Wc,7935
+qai_hub_models/models/real_esrgan_x4plus/export.py,sha256=9If4RI5diFdWWWTW8yUNrrrl9YSasVjEASfgZZqBAYA,7927
 qai_hub_models/models/real_esrgan_x4plus/info.yaml,sha256=xP8_0t2kPCJmXXY0M1oTjK5mPYGNaWMpgmRZ9rXNfUk,1330
 qai_hub_models/models/real_esrgan_x4plus/model.py,sha256=aH0QcIXpz336Nw7ca7BUPzMtuVFRvaFNOlun9pFVhQc,4443
-qai_hub_models/models/real_esrgan_x4plus/perf.yaml,sha256=FtcUnJVTFvIQYcqXd7FpeJ99oT9vL4m3FCfM53GyYYQ,3412
+qai_hub_models/models/real_esrgan_x4plus/perf.yaml,sha256=K03PggGZxFt40Kjnz8g8seZLGQPydc7DXFOxji2Rr4w,6100
 qai_hub_models/models/real_esrgan_x4plus/requirements.txt,sha256=eivyaYj7iqy9Z98WwPoKDVihhM_HDbjOBu49PUryfJI,44
 qai_hub_models/models/real_esrgan_x4plus/test.py,sha256=amJIYWs1sw7ndZgviSvm9X4uZSUH8TAz9Sa_9lyCTUA,1440
 qai_hub_models/models/regnet/__init__.py,sha256=eVvJ3D2GfGdcBriF_ihwwMVGDFu5OtZSuPz61h-uKsM,469
 qai_hub_models/models/regnet/conftest.py,sha256=Cv_og-XOwnHDzfkA7dhoVsxEcrknTDFBtWNduCktnPE,1311
 qai_hub_models/models/regnet/demo.py,sha256=QTBBLpnIfWQLVnJOzzj1-pW5QWbpl68HJQ55_rtMAZs,524
-qai_hub_models/models/regnet/export.py,sha256=Vw0mzJDRryIyZ-R2MI2ef9uxjTcEJM0hgpmThZ1ejhA,8148
+qai_hub_models/models/regnet/export.py,sha256=5PKkrtYMRoJN70SGUvXWC-vuZw3vA8dgpMdBRwDIC0E,8486
 qai_hub_models/models/regnet/info.yaml,sha256=hdUbHswd8D4ya7xX4_YPl4ZOT1ItebCTvId0VEh-mBg,1291
 qai_hub_models/models/regnet/model.py,sha256=ZAd-Ymzr5ak1Fe3oMS5a2DFvwnOWEKYJZaEP2tu-fQo,635
-qai_hub_models/models/regnet/perf.yaml,sha256=_DtbeTSVawkgu0VMbR-qA8cJX8vKDok5TK9kQvF3xt0,4558
+qai_hub_models/models/regnet/perf.yaml,sha256=Eb7erIgPce9eQ0rK92EuV9pN1-LINPiNo05B3mfL_Pg,6033
 qai_hub_models/models/regnet/test.py,sha256=zZ4cDdGDWjkZ15L8zTOn770-LYfeTyRPaR-J2OA-RnI,984
 qai_hub_models/models/resnet101/__init__.py,sha256=giKR_QG6BGLXFiSVr_sRAJ-h2b2Ea6L8rhZgxEoJ7jY,472
 qai_hub_models/models/resnet101/conftest.py,sha256=an87gaJVW9hYWAb78cfkimhVyJkmWtLtBZcoTtM3838,1314
 qai_hub_models/models/resnet101/demo.py,sha256=RYdwNDEnxQpmUsUPP4qvYiGYJar-1WfLBbO5cVMx6N8,533
-qai_hub_models/models/resnet101/export.py,sha256=CPHqkl4h37aNobFM2oK3NrvYpM-Xf0FVSHRb6zJ9Wzw,8160
+qai_hub_models/models/resnet101/export.py,sha256=e3AIiGqJlByELVdPeMjgI4yIIhhmxW6FBoZStrYT3KI,8498
 qai_hub_models/models/resnet101/info.yaml,sha256=fbGPKTdV8UBcjMHR0aVK6amiizctrlbiQFaDSLrotcw,1312
 qai_hub_models/models/resnet101/model.py,sha256=Pjnm6vSdazT6uhrfn0Hx7hLU-U2rF2KiTMa9jWlmm7o,609
-qai_hub_models/models/resnet101/perf.yaml,sha256=-01MYMB5_KCkmihHLhqNy-UZTK7dV3DaRUV-VLnfNcs,4570
+qai_hub_models/models/resnet101/perf.yaml,sha256=Ob50TxhjMsYahN-4VJVJRjMtzatbf-1AmdojQejty3w,6052
 qai_hub_models/models/resnet101/test.py,sha256=Gr4R3pO8Be43sRBm3KEc5NaJ3fHofnMMWfKTJ3K9lW0,961
 qai_hub_models/models/resnet101_quantized/__init__.py,sha256=aJsoYEIOw9cKKPYWV-wtGWvN45BTKvqZVoN31Eag4TA,483
 qai_hub_models/models/resnet101_quantized/conftest.py,sha256=snEUk5hF5Digs9PrP8fE4lL-I-mttGaoOCMmFwAHHms,1324
 qai_hub_models/models/resnet101_quantized/demo.py,sha256=to8ShYfeBVmzd0OkyV4SxeflUXeedhqmrc6dwbnanfQ,578
-qai_hub_models/models/resnet101_quantized/export.py,sha256=FyABn-zxrzf4eaOqXpI_k2HFD9_8M5fLeCbfREoMk-c,8623
+qai_hub_models/models/resnet101_quantized/export.py,sha256=mXUMYWYvW4R3qs_ecala8ygUDyQTzDcCV5K6kr_1eOY,8961
 qai_hub_models/models/resnet101_quantized/info.yaml,sha256=28u2ptmsdw9O6ZAljymiAaTOZxV6DDD-_eXsXYZx3X8,1346
 qai_hub_models/models/resnet101_quantized/model.py,sha256=7QEdqZJ4vJ1w51aJK4O11WvuJZuXcyBkFXpx5Blmi_4,3210
-qai_hub_models/models/resnet101_quantized/perf.yaml,sha256=_HEIS4QwfrG4kUhcNCSlqhnR4ucs_liNHRu45g1rEB8,6649
+qai_hub_models/models/resnet101_quantized/perf.yaml,sha256=ZpBerc0CVULN7fc5bzaPW1GENk0YsFc29rdnxDoipX8,7756
 qai_hub_models/models/resnet101_quantized/test.py,sha256=C-8zqBiwN-nCgJb1I9RYfMykLPSKJIWW_t_Pbh97mJM,921
 qai_hub_models/models/resnet18/__init__.py,sha256=YXCoY4ADkV9rxnJQmx6th3bEwYaxE4UF6ECJMkuN4gY,471
 qai_hub_models/models/resnet18/conftest.py,sha256=it7mAYPB7-SDG_KypQElINnLd14JXA1S_bWW80XYuto,1313
 qai_hub_models/models/resnet18/demo.py,sha256=e_s-tuUKno5C8PmEh8AISsqywzoNtYUQVFJjyaJRLYU,530
-qai_hub_models/models/resnet18/export.py,sha256=OPAmZyZJH19KE3waNduxC7y2jrJRxn_k575r7kRWNIM,8156
+qai_hub_models/models/resnet18/export.py,sha256=fM_cg3WwexBHggu0iZB9-5oDKn73BXwnnxdJ5cCacCo,8494
 qai_hub_models/models/resnet18/info.yaml,sha256=EApwQW1yExiQzFT8qWYraKeeULj1aF-_j15imZQshuU,1310
 qai_hub_models/models/resnet18/model.py,sha256=gl5xAhxkR3kMkbQ5DQXSZ42eGFE0uLN2gNmrianZWEY,607
-qai_hub_models/models/resnet18/perf.yaml,sha256=lf6DXi_bZmuGSLPWphOXqJkT5QaVeaL-HNc0gUo_Egw,4539
+qai_hub_models/models/resnet18/perf.yaml,sha256=8D6sVrEdTzwBEBeFc1gNt23cV6Lm2l3gPySwXlTtpvA,6014
 qai_hub_models/models/resnet18/test.py,sha256=raI736nKtyy7_kNAmWEci7wQYPqPQ_ajP0h7MTfpdW4,955
 qai_hub_models/models/resnet18_quantized/__init__.py,sha256=6MfeTP9-civpAklCTt8I8kwEj4qfK2bpGpBFzi7Gp9Q,482
 qai_hub_models/models/resnet18_quantized/conftest.py,sha256=K1SyZCqYud_LeDwOtoo0V07J4hJAUehFN9WspAhNOQo,1323
 qai_hub_models/models/resnet18_quantized/demo.py,sha256=J0yoGrm2LP5iUCCMXqc-paqC0VJZPP_Zi2heqCd_FGk,562
-qai_hub_models/models/resnet18_quantized/export.py,sha256=M_bPCh1zlyHw1Ceebl5m54PnXhmY9SVsGo3Cl4pGydE,8619
+qai_hub_models/models/resnet18_quantized/export.py,sha256=Be-OOyOMYyyL8sV3vssXkKIIwyLbX72v3M-t9W8eHtQ,8957
 qai_hub_models/models/resnet18_quantized/info.yaml,sha256=EF_rGgvR-mbYSxraghrHtbP_TYxrI6cLx4xG2PuRPmE,1343
 qai_hub_models/models/resnet18_quantized/model.py,sha256=ayCXNSVAyPBqDnxYyE-UNG77dsNGhXuS6nBH-yNP7uo,3012
-qai_hub_models/models/resnet18_quantized/perf.yaml,sha256=PArdCZ3z3i5Cjrxe_NfAGyxLDqvR0c7BOD9zYPrYe4M,6614
+qai_hub_models/models/resnet18_quantized/perf.yaml,sha256=iq722hxdWmImBi7cKa6OwMe4I0LV8AyZ4l_5g0GyrpA,7715
 qai_hub_models/models/resnet18_quantized/test.py,sha256=gB-2uNRrC-phYHX6dwCn6cVSAxpVq41FFX_UwPwb5qk,917
 qai_hub_models/models/resnet50/__init__.py,sha256=i6A3by1VSJeHInRvYv3jO-x3QRBzywCjnlKuG6ALGA8,471
 qai_hub_models/models/resnet50/conftest.py,sha256=0FMjxPcc8P38c9qy2GZFP6OtPr5cpGtvCnnA19DusgU,1313
 qai_hub_models/models/resnet50/demo.py,sha256=Ad2nWpI-kmP5M1sGhFAqSZFZ1Mz0Lov4q0ilsJIsdBg,530
-qai_hub_models/models/resnet50/export.py,sha256=PHrEv8xlw4QdxE-vHyK1WswIpf8icAfGmqm0DMkY2tM,8156
+qai_hub_models/models/resnet50/export.py,sha256=KZqU0Zfocr6R5xbtSiBPfP60sf7kLalL9oFcgyamsCo,8494
 qai_hub_models/models/resnet50/info.yaml,sha256=mF1FdZhk7GG8vBDs0sW6l0VWW9jBxyZAkyS6SdaAF7M,1303
 qai_hub_models/models/resnet50/model.py,sha256=3Uz5cop4u1xHyZuR9ss0Lg6cSxWyV_xuJcgFENbiHlY,607
-qai_hub_models/models/resnet50/perf.yaml,sha256=FZmELMtiKEx4G08Xkdw98VWOh2Y_n0c1sN-_a7AUx64,4553
+qai_hub_models/models/resnet50/perf.yaml,sha256=tMJvYoquh3pvsfJfFmbSL7dP7UMZEQmar_DABJkRfGc,6045
 qai_hub_models/models/resnet50/test.py,sha256=7iQrq9aU53jUwGBRxj6dRXH9SwvFqTwegbMMxTmO5Fc,955
 qai_hub_models/models/resnext101/__init__.py,sha256=KsXu0bXmQcD_vUB9tOkoYL8cPsCuexuQfSA_ZoSEtxc,473
 qai_hub_models/models/resnext101/conftest.py,sha256=QBDbnLIcZmM38CpOpAycixR5lvhAJ4cmg0PpQiofl-8,1315
 qai_hub_models/models/resnext101/demo.py,sha256=W_MBGy--3Gyo6IkASUus4IeC2OJo6w7_vh8AN--pHj4,536
-qai_hub_models/models/resnext101/export.py,sha256=VTd20lCOqzNc-xm048r25mjGQKHEdSJhxuBf0thb20w,8164
+qai_hub_models/models/resnext101/export.py,sha256=x4hr_jqjHG3pM2WZO2dVgz0btalxTLWMwEAaQNxjN6I,8502
 qai_hub_models/models/resnext101/info.yaml,sha256=QQQ7IKSLieegTlThhnySi_BdYTnXcrBzSt-FNmAfspo,1324
 qai_hub_models/models/resnext101/model.py,sha256=dD8WyCTD397PbDRLg_EoNX-hcQekN2P7S8OrzHRz7SU,617
-qai_hub_models/models/resnext101/perf.yaml,sha256=YDFYgDtCC-LrIuKYQOFNJ4TqQJ7uP9ntfhWrCEu5-Fw,4568
+qai_hub_models/models/resnext101/perf.yaml,sha256=kjBas4hUIMFXI6lxmc7THfz5e1bNU_4BXXWwzExAbbs,6050
 qai_hub_models/models/resnext101/test.py,sha256=Gf470ZYn46oHSNisYxPiShq74rXykhqvriYbnNRnwoY,897
 qai_hub_models/models/resnext101_quantized/__init__.py,sha256=Sr23eMhxryEjt6hRjftWbdgS9YcDk1b4OshOi7irTY8,484
 qai_hub_models/models/resnext101_quantized/conftest.py,sha256=Zj1RXi90rgF6NokRUCy5f6ific7tmppJ-L-INSWepoc,1325
 qai_hub_models/models/resnext101_quantized/demo.py,sha256=Po6sLvHLkvqNm9tZ-vAeM-A-t_FLKaE18gTnJh6oSuo,581
-qai_hub_models/models/resnext101_quantized/export.py,sha256=LOIaRxka7taUtpxiTdniot7o2-mA502aD9uk8IPVE1o,8647
+qai_hub_models/models/resnext101_quantized/export.py,sha256=zjDEd_v6Kod4Zz6fZTNdkRH2F_UoTRVm-G6__q3p0aU,8965
 qai_hub_models/models/resnext101_quantized/info.yaml,sha256=Zlrngu5Y1UGGPhy_4bgpaE86oIzeB5NIeiAslZh8cto,1365
 qai_hub_models/models/resnext101_quantized/model.py,sha256=BQAwwRR0Z2ix9UGwm2DtMZYUohwaSXrQyZ8Da298LYg,3017
-qai_hub_models/models/resnext101_quantized/perf.yaml,sha256=HLTteNi_dXEF1x_QSINZa12b6rD80HnGjgsgLBAKeUs,5115
+qai_hub_models/models/resnext101_quantized/perf.yaml,sha256=lKMH-06ZrWHF-01jdUXmg2FlXjYwLE1_oQZGr76uN6Y,7769
 qai_hub_models/models/resnext101_quantized/test.py,sha256=jw2RahxL2zJq95ittlG6mBH1HKfCe-5MLpuL6f_9NOw,925
 qai_hub_models/models/resnext50/__init__.py,sha256=mvGLhLT1_Hj0eQ5sERqw2qBaaHTc8q9nGaoIMc_zVwE,472
 qai_hub_models/models/resnext50/conftest.py,sha256=xg4KNyUABmbqbR54NpX-c22OYxK-J4KSahzIw4zqmyY,1314
 qai_hub_models/models/resnext50/demo.py,sha256=layKRO8OVlf7zDRarcUKuBwziX6mh0TahnxW_B4hOR8,533
-qai_hub_models/models/resnext50/export.py,sha256=tjwa-TPeonJXejIx_x3otlbotm9ZL6eQgw-y7LyiC58,8160
+qai_hub_models/models/resnext50/export.py,sha256=MQnkoNr-spe4a3PkcNJ1XS2mDsm3e7eoQfIU2lVJgk8,8498
 qai_hub_models/models/resnext50/info.yaml,sha256=x36YHWLVNgYYWVx4Y9GrJYKmwbgCmIXcUrVuX5Mv_XI,1322
 qai_hub_models/models/resnext50/model.py,sha256=g_2NntA_nWKxlgNZJiLaSpk-VcUO4S4EBtK1fLqvFH0,704
-qai_hub_models/models/resnext50/perf.yaml,sha256=f1xiIcuMRavdZrfMLLJ-YKKpLwY08wcUwHNQzMS2-xo,4554
+qai_hub_models/models/resnext50/perf.yaml,sha256=eg_oEq5kik2ggrHh-LXV4w707hligAAar0cBrdfF-vY,6041
 qai_hub_models/models/resnext50/test.py,sha256=mO0iDd4c_7F_QjcOypdCsexc47QbdJpKFKFU5Mbwipk,840
 qai_hub_models/models/resnext50_quantized/__init__.py,sha256=4Oqop6yfnVXt2Zh9RoVJLqXYnXkriZIMSa90EcFh08Q,483
 qai_hub_models/models/resnext50_quantized/conftest.py,sha256=cKaMIuZ_Lxy91femyBDaT2Vnx9Y229XaDJSsohDgl_E,1324
 qai_hub_models/models/resnext50_quantized/demo.py,sha256=sl3AOEAMwvqU2v3_7mpy3yUD-8j1JpcL_WCTpkel_8w,578
-qai_hub_models/models/resnext50_quantized/export.py,sha256=Qc3JwIJ4tlJXf5oAN9QFnHVK9ZRg4rRaQLkKCGkJMQ4,8643
+qai_hub_models/models/resnext50_quantized/export.py,sha256=TQQ3o3t7ixxVgZgWINBz9u_88stjaVMm4e7KZx5WuHs,8961
 qai_hub_models/models/resnext50_quantized/info.yaml,sha256=cC9rkqVl4QXSu_EjJRQY97AAqh8FsBuez-Rf3XSWae0,1362
 qai_hub_models/models/resnext50_quantized/model.py,sha256=0mq9UHGSzZfPTnAkiEZmPVs1HCZizJ7PPiIoLiN9jN0,3008
-qai_hub_models/models/resnext50_quantized/perf.yaml,sha256=dvQcvRhmF_9jtoOT_QamNLyEST4-dK7e29IGoc7YhYM,5084
+qai_hub_models/models/resnext50_quantized/perf.yaml,sha256=zuCVDiGWwfUjxmJdAJFSVcaPiqDVPHgy4ChJ8t9HL58,7731
 qai_hub_models/models/resnext50_quantized/test.py,sha256=zmx2xRiJL2_B6zZwUmhMWzSm7cmcbCGyay7dB9O1XKY,921
+qai_hub_models/models/riffusion_quantized/__init__.py,sha256=26tWdw5CVbQ3llULCZBwTZm3tFn1qSnJhH8uY3W9qpU,453
+qai_hub_models/models/riffusion_quantized/demo.py,sha256=CvrCcMZytuZPmPaBiQ-5C7fvSgS1jZqHSa0tq5FNYOE,1714
+qai_hub_models/models/riffusion_quantized/export.py,sha256=nqIqdtxG7jXYnuR2ImlBeAY2yUMqQQglrgzbsCrsqwg,7676
+qai_hub_models/models/riffusion_quantized/info.yaml,sha256=dIKYuamr0GJAfjg9nH1uKIeYoQhyxnbseZiRMs2FDs0,1422
+qai_hub_models/models/riffusion_quantized/model.py,sha256=C-KOK_OHsWqCiMMI3e9YvNf9b1TmBfCkGQxaC5YLPT0,3452
+qai_hub_models/models/riffusion_quantized/requirements.txt,sha256=HWz6kAx8f-AYWf5IWvv-q1IOznXS94gXTQrtKGY4L70,46
+qai_hub_models/models/riffusion_quantized/test.py,sha256=s7wRkgXMv_nerMenUqxDhK193vZV1y8KrS8C-FkDTsY,995
 qai_hub_models/models/sam/__init__.py,sha256=z2l9N1qWNXdFDly6YvRC6MBOTANKL28klDYmF2qF7Qs,404
 qai_hub_models/models/sam/app.py,sha256=jCDEbspglkuXi5SS86A96Ek0SaqO-xhHWwHPh_3N8LY,5101
 qai_hub_models/models/sam/conftest.py,sha256=LTnCPPC4ahdYfXzeghcDWpzA5Dr0IgkPvv6UPrjGaPE,1402
 qai_hub_models/models/sam/demo.py,sha256=G-Fv5x7usY6c1YeSdRzKBpw4KhC-9akcxINsspJBiHg,3088
-qai_hub_models/models/sam/export.py,sha256=6yNX5zuoAHMBrMgEnwIuvl1jI1Wub8adKgPqUfQyTfY,10468
+qai_hub_models/models/sam/export.py,sha256=zRU2jbaKmR5le9DTL_ugrCnk2iVep0TedMyYoKu3vzw,10440
 qai_hub_models/models/sam/info.yaml,sha256=b0Dez8dcQ4IAQs0xAHWsYkAvolTKi_hbFSRJvr19f4s,1391
-qai_hub_models/models/sam/model.py,sha256=qaW2mQiobMnmQ-SDme_JYPdbzUKHMqjeu4B7CJTEkx4,12000
-qai_hub_models/models/sam/perf.yaml,sha256=6hgw3pxu-Su8e5iJKjqtMAyWaRQTTbR1A_UMGJUWdVA,3411
+qai_hub_models/models/sam/model.py,sha256=ZBTA4o-p5xrCccU8Hb66svejIJMIYvSZEPvAS_ddjtU,12018
+qai_hub_models/models/sam/perf.yaml,sha256=ejN_tlLXtqmdMhiySuxhnr1X9iXB39AnowBvEPmRBy0,8143
 qai_hub_models/models/sam/requirements.txt,sha256=vtppKtEacR8giw86K6Z90D_Xko9hityPiHQx7PxkAgo,37
 qai_hub_models/models/sam/test.py,sha256=cmynvGV9g_6A0etO-ORNxj16gyCF97MVYxZQK0CNzIM,3062
 qai_hub_models/models/sam/utils.py,sha256=5Zc1YnUm0HXsJ_T-R_Ge0CjMARxWxHA-ITG424uOCic,826
 qai_hub_models/models/sesr_m5/__init__.py,sha256=kS2GuYY43QHK5jRUcssK2u92IM8LMLy3SIbV5YCEsio,464
 qai_hub_models/models/sesr_m5/conftest.py,sha256=flksqnAEfg8jkxYZ1tam9lAn80Cjis6id5VbWIwiEdU,1406
 qai_hub_models/models/sesr_m5/demo.py,sha256=uQqrkMLYcKnXc6_uDW8Niuj1YQBiMya3sFZwMGVXlOk,923
-qai_hub_models/models/sesr_m5/export.py,sha256=hAv-JZzAo-UrVrHq93WqeQZLK0OHcFyibJP29cbtvSA,8287
+qai_hub_models/models/sesr_m5/export.py,sha256=S4T2JF1JlS7WxZOQ4IBzF56I_ydYHa6wxzatcl4SKCQ,8687
 qai_hub_models/models/sesr_m5/info.yaml,sha256=xDzw26L21cJQ14jXZFrODO_klhrPTV5L5ALknfT4MnU,1104
 qai_hub_models/models/sesr_m5/model.py,sha256=c9-Q3psWvkFyFKgaOfWN5rhJpsvMZOKIUvapZhv6JFs,2984
-qai_hub_models/models/sesr_m5/perf.yaml,sha256=FMyaSEBP7eMuC87kDtYuI5A2UX5dZUO0F_hflAqczwY,4547
+qai_hub_models/models/sesr_m5/perf.yaml,sha256=_-6rR0F-In01mdxx5Nl7vktxGXc1g_EVnCrCA-Y_lWI,6005
 qai_hub_models/models/sesr_m5/test.py,sha256=hH27IYEOa4fPbfgHeQYuiUhHdPvMHX50DdW-LY7btng,1471
 qai_hub_models/models/sesr_m5_quantized/__init__.py,sha256=PZUoiiSQjJOpVhj2TNp_ixSeoy6RwK34nb45232QDsM,475
 qai_hub_models/models/sesr_m5_quantized/conftest.py,sha256=hY20QoeSDSpnl1h-O3nQGc7MDj0rKk4XtPPEHPiS2Xk,1416
 qai_hub_models/models/sesr_m5_quantized/demo.py,sha256=OD2D8ExW3h3SYF2_KX_7Qv_GQ8ylmjui2tmpWsqyAqg,990
-qai_hub_models/models/sesr_m5_quantized/export.py,sha256=fvERC-GQAWIOoqHawayYgrE40loc3BuHci9z03gjfsY,8431
+qai_hub_models/models/sesr_m5_quantized/export.py,sha256=6xACIE0mhx1Cn21YPprU6-F5Yy1aiFbLxg500DJVOpE,8719
 qai_hub_models/models/sesr_m5_quantized/info.yaml,sha256=J1XebgMlJ73n3is32JI0bduz8pFhlImgxaxr6khZfvU,1151
-qai_hub_models/models/sesr_m5_quantized/model.py,sha256=6O-GFAI1vPgyqLIRuMt1MWPQ8xAqfGvTBk4ohFy3w0Q,4269
-qai_hub_models/models/sesr_m5_quantized/perf.yaml,sha256=4FoWtASZBBY5XkHhJ8DroR6ki5PchJDxUya3J4pnYVM,3925
-qai_hub_models/models/sesr_m5_quantized/test.py,sha256=3EXTzeXWm0qVHMZmGJeiy7OPzAafb2b-ORgUMtQ5KiI,2927
+qai_hub_models/models/sesr_m5_quantized/model.py,sha256=7gJBv88D6AJqFEWeLIBEOTF1gKXn1Wkr5KaXk-jeSg4,3956
+qai_hub_models/models/sesr_m5_quantized/perf.yaml,sha256=aOf6BBxOcvoFy4bFng3F7EepQzE90gQlOGw2i4UQrg8,7726
+qai_hub_models/models/sesr_m5_quantized/test.py,sha256=Qao7RpHNifOinpmqbOPG7O920qPT0bSbngnFedEIwjE,2931
 qai_hub_models/models/shufflenet_v2/__init__.py,sha256=V__9vnDnnEeNjHp4_cliHUq8Ea7djP5ppdXt1VH3MI4,475
 qai_hub_models/models/shufflenet_v2/conftest.py,sha256=_HeqWMK0_TRVqWHvS7pDERferegiiH1N-YxpfbFzj0c,1318
 qai_hub_models/models/shufflenet_v2/demo.py,sha256=-yorlVXF_fAdjOp_xfBQeZ63w2hU952DdIcWeLJDMJU,543
-qai_hub_models/models/shufflenet_v2/export.py,sha256=TysuC1uW0qOWpxhJbYftSyUf5xiVA2NZig4-WWBgcII,8176
+qai_hub_models/models/shufflenet_v2/export.py,sha256=q2UxMkDBkRyICodLxL6wD7ioFma-RY3b7wiQAN8IAx8,8514
 qai_hub_models/models/shufflenet_v2/info.yaml,sha256=5myQBF6qAlWS_h8a9emJbmux2ZpQIgsf3o7ZERL-nVE,1353
 qai_hub_models/models/shufflenet_v2/model.py,sha256=TLi9PPf3FIDi5l-pgCkmJGRA8TduOSRpsB3CbNhtVX4,713
-qai_hub_models/models/shufflenet_v2/perf.yaml,sha256=cqlZ3rvK-NnFSyxvgJmRiuEyboA3NjnZzB_LwN5-tto,4561
+qai_hub_models/models/shufflenet_v2/perf.yaml,sha256=_bIWput4gQbPz-sWlBe6tXdqoweqHCFfMeyFYPdFShs,6037
 qai_hub_models/models/shufflenet_v2/test.py,sha256=11Yio-WBjhooUiLFMX7XC70gjCjq-MeiJr8zM854Rhw,857
 qai_hub_models/models/shufflenet_v2_quantized/__init__.py,sha256=CjZcBsRqc8WZXtl4c5QvzSJtfCx0TgArcKOymMp9yw8,584
 qai_hub_models/models/shufflenet_v2_quantized/conftest.py,sha256=0RqB7hStDPCSLtKJh8GX33EfKeVUJJsNZ7pLMfPew1k,1328
 qai_hub_models/models/shufflenet_v2_quantized/demo.py,sha256=DFJLJ83mzCD7bDyJ9KVFnO8c9rHSYygU6BlzoSEE-OA,588
-qai_hub_models/models/shufflenet_v2_quantized/export.py,sha256=_HbdfqhIGzonvIaNItGe4vrtSwHokcN5XTnitaWbWvY,8639
+qai_hub_models/models/shufflenet_v2_quantized/export.py,sha256=AqYiCB7kqjTiwxkozOoA5jBAgkaitepN4eeDnykgJ_c,8997
 qai_hub_models/models/shufflenet_v2_quantized/info.yaml,sha256=fFtlMqu2yN4_tu5ZZtayMfrNCYxxHuDjXQvOy9k6tw4,1383
 qai_hub_models/models/shufflenet_v2_quantized/model.py,sha256=q8sHReFVrVZwHRn_iD8fZjNYt5jb4AehzQpcoeLEo2Y,5989
-qai_hub_models/models/shufflenet_v2_quantized/perf.yaml,sha256=94hF7BlzH7Nl53cYx21nPOnFxPVxSk6wFaBkOJsoVtY,5481
+qai_hub_models/models/shufflenet_v2_quantized/perf.yaml,sha256=z9Eq0WJbp_tfcym93czynzJ8npLny6jEQlZ0wyBE_C4,7689
 qai_hub_models/models/shufflenet_v2_quantized/test.py,sha256=_U1q8NxyW4nqeb-jfVwqNcISHQpMO9mvVSK7QQK5Hro,899
 qai_hub_models/models/sinet/__init__.py,sha256=WIFJwnJg_47VGh96lUb-Ned-MCqHd9KL_OHzc2GU8Qc,396
 qai_hub_models/models/sinet/app.py,sha256=1w0xj_VwsDGJWRitmBq-iBWjDNI2dB-MOKHqCe7_pr8,3793
 qai_hub_models/models/sinet/conftest.py,sha256=hl1kupzGIfejOOwWhudHdebFh9fARIpjvXtOAGTtQrY,1404
 qai_hub_models/models/sinet/demo.py,sha256=1H922wHmgorXtZKVNVWn30PHA9Xz7NPPkM5wR77vVkA,1657
-qai_hub_models/models/sinet/export.py,sha256=q3BJPljYVnEM0AbRNEiOnflZ6qjjrHIFNuL0rXKCkOo,8422
+qai_hub_models/models/sinet/export.py,sha256=pNlBpAZHdqK1E20UKsoiManTbEGeTa05UgfICWCcQ10,8822
 qai_hub_models/models/sinet/info.yaml,sha256=L-fe7Oh7vvQkdYXQ_xaMDXBqtusxxXtBjD2or5fa_8w,1260
 qai_hub_models/models/sinet/model.py,sha256=kTdRqRgfCXsIUcxUwSsIoqsePRM0o2UNKRajxDIK9Og,4770
-qai_hub_models/models/sinet/perf.yaml,sha256=UxYqzdyQSfIpKFz2o5-M7QOGGVRPua1EksLHAtBQ_vk,4518
+qai_hub_models/models/sinet/perf.yaml,sha256=b3TkBtR0qiXMonLjq35dIDxLtt076zmdmgTCL41kbzQ,6033
 qai_hub_models/models/sinet/test.py,sha256=3WEZ8v6IIXSA2f_wfWWIeIJaeFCc1h_layHLP2LEf9E,1355
 qai_hub_models/models/squeezenet1_1/__init__.py,sha256=wWegyK2XfBcw4YceoPl-TZKn1uoRNNveDHXHOzA_tGQ,473
 qai_hub_models/models/squeezenet1_1/conftest.py,sha256=1_BoD4N0lI_Fj9Xt2wQ1SqHdp-L_GVDYc8JFT4PjaFY,1318
 qai_hub_models/models/squeezenet1_1/demo.py,sha256=v2sws22uWCnGNOPIUPT_NolPTvTDLH-phqz8IYG6bVg,539
-qai_hub_models/models/squeezenet1_1/export.py,sha256=cjiINdF6EctJADM9JI3RM-tewwH-6ud2dJDYb61zbgI,8177
+qai_hub_models/models/squeezenet1_1/export.py,sha256=xa5LW89gi3T1getHk8aEP3EXBa03ZDdrhPdlNJcdsfQ,8515
 qai_hub_models/models/squeezenet1_1/info.yaml,sha256=fhuQnYnNgu3GELnXi1xO655VmJ4EgaHkWn013HHiD1c,1325
 qai_hub_models/models/squeezenet1_1/model.py,sha256=mfmXr4T7EmhLa_86TXHOwv3XvlZMEsYAuiQmn68mt4c,696
-qai_hub_models/models/squeezenet1_1/perf.yaml,sha256=YD5LRgpgd7H1SXNnZl5DFHyUnnSKubpNhqeh9aeIpCU,4551
+qai_hub_models/models/squeezenet1_1/perf.yaml,sha256=rPCgWikwwkRbBuZq0y_rQK5ptnIdKoFv0RCF9sqhqe0,6015
 qai_hub_models/models/squeezenet1_1/test.py,sha256=ytrCi-ZAAYWJ2rTV-6k_yZhYZkfrQRWD9FFoo8vUHEM,851
 qai_hub_models/models/squeezenet1_1_quantized/__init__.py,sha256=lQk7lSxtBwX8Y2q5w0xHIfz9bhvub3pkezT88Tore-U,582
 qai_hub_models/models/squeezenet1_1_quantized/conftest.py,sha256=-XWcvLJnZN6qOqPjP04YkDW8-5wVmUHD151lLtRU7Zc,1328
 qai_hub_models/models/squeezenet1_1_quantized/demo.py,sha256=khZa64iQFJ9MIwUA5pjiAiq8bOQwXiFxAzNsVLObTgw,584
-qai_hub_models/models/squeezenet1_1_quantized/export.py,sha256=brrHFKlW16krmEwhEe5mgrlfJ69n-wZcZjncgC4XrUM,8592
+qai_hub_models/models/squeezenet1_1_quantized/export.py,sha256=XpL1sWB11t75TGo16qqbrc6jvqSJkuIXXROiBuoV32I,8930
 qai_hub_models/models/squeezenet1_1_quantized/info.yaml,sha256=ORH-gyADqkVAWBPF08xspyFovFxhdm6LHlMmRztfK5o,1358
 qai_hub_models/models/squeezenet1_1_quantized/model.py,sha256=FXk13i2nFxCAy5H3NDXn0MCvJ4Tr2UJ7Rstqvbyh3a8,3023
-qai_hub_models/models/squeezenet1_1_quantized/perf.yaml,sha256=9nA5iKB1U2ACGoatk0hyoXLglfjoklIhE48VgFVsj3o,6622
+qai_hub_models/models/squeezenet1_1_quantized/perf.yaml,sha256=MFq_0yjH5tuRNn0cg1RPwSBUyiVSFS5piuHCfmDxsG0,7723
 qai_hub_models/models/squeezenet1_1_quantized/test.py,sha256=5e9_c2yGKzkl1OaYkwWXBijYxQijS-bKt7Q10Oo7P3M,895
-qai_hub_models/models/stable_diffusion_quantized/__init__.py,sha256=vtVBYmQud6hMNb2GOWcqEiUr6ue5QU-Jftnss61bUoc,540
-qai_hub_models/models/stable_diffusion_quantized/app.py,sha256=o9Ao1qdv1trphz35GuPUjCi3leh29KoyyYGd3A7LdA8,7966
-qai_hub_models/models/stable_diffusion_quantized/demo.py,sha256=barv_jVC8nGiXsdL6SS8oDbMRSk5Bwm1gfw20St2d_Y,5765
-qai_hub_models/models/stable_diffusion_quantized/export.py,sha256=zC4U4Lp8yiIKuiJaOzRCARY8gz_-Er8xQA3l0UbrQFU,7704
-qai_hub_models/models/stable_diffusion_quantized/info.yaml,sha256=bnzWKCd80EDo1AOmlfnTDXrAAl0VXbO8CI0VjmdATyg,1355
-qai_hub_models/models/stable_diffusion_quantized/model.py,sha256=Wx2-g4gqcUMQ3hjM9Ah2WZiOuQ85qkVpNqd2P1Mj_aE,3604
-qai_hub_models/models/stable_diffusion_quantized/perf.yaml,sha256=EJT82CEPbK653yyloO5L_exjE2HXoIdU0utrOZ0yHNw,6384
-qai_hub_models/models/stable_diffusion_quantized/requirements.txt,sha256=HWz6kAx8f-AYWf5IWvv-q1IOznXS94gXTQrtKGY4L70,46
-qai_hub_models/models/stable_diffusion_quantized/test.py,sha256=IIEtaueWT-pakv6P0_Uy10jPdKE0suxjwUHJPpij2VA,1599
+qai_hub_models/models/stable_diffusion_v1_5_quantized/__init__.py,sha256=gYXFXZjz3dEWTGPUMTJJtqYNUVx_1_HWMzDIsFQsNP8,492
+qai_hub_models/models/stable_diffusion_v1_5_quantized/demo.py,sha256=ph6iU9PV03xh_oGniJhh6ezqrn0hCOLfPLd-tjaI3RQ,1735
+qai_hub_models/models/stable_diffusion_v1_5_quantized/export.py,sha256=q4VK4zzWHXjuPrUX2AkdDVqA82vOk5_y398aoSfIYHY,7724
+qai_hub_models/models/stable_diffusion_v1_5_quantized/info.yaml,sha256=AjRtVd-SdFgU6RnvE49MwLbYDRfLPfhKe88y0ohb3xo,1401
+qai_hub_models/models/stable_diffusion_v1_5_quantized/model.py,sha256=A2l9skv9THTUZMZIe-TonoWRiPMWsxvy0nvI4Ps9Il8,3604
+qai_hub_models/models/stable_diffusion_v1_5_quantized/perf.yaml,sha256=EJT82CEPbK653yyloO5L_exjE2HXoIdU0utrOZ0yHNw,6384
+qai_hub_models/models/stable_diffusion_v1_5_quantized/requirements.txt,sha256=HWz6kAx8f-AYWf5IWvv-q1IOznXS94gXTQrtKGY4L70,46
+qai_hub_models/models/stable_diffusion_v1_5_quantized/test.py,sha256=Y-ruQ72OokkCEHqulKBRoE9M7QpkQyCnoSbmSxwpn3g,1052
+qai_hub_models/models/stable_diffusion_v2_1_quantized/__init__.py,sha256=UGHL3gWa0WHzeM-K36K5SQks11-EbTvLi6IGA9oNcR4,492
+qai_hub_models/models/stable_diffusion_v2_1_quantized/demo.py,sha256=rEwioVaOGHc5Xn8u0_sEeNHEB0ENwBv10rS6w1JQGfo,1811
+qai_hub_models/models/stable_diffusion_v2_1_quantized/export.py,sha256=RADErtvi4NypsJkdUQxDCarf3wBb9DpApQjx2cA8EKY,7724
+qai_hub_models/models/stable_diffusion_v2_1_quantized/info.yaml,sha256=QjqVGjq8y_sywpK5lXBVgbgARkg3gF2w75KggXvuAq8,1401
+qai_hub_models/models/stable_diffusion_v2_1_quantized/model.py,sha256=jbNVSkfDypdlt4lPnfZKBylyU9HBOX8UOCY8UzM7leI,3469
+qai_hub_models/models/stable_diffusion_v2_1_quantized/requirements.txt,sha256=HWz6kAx8f-AYWf5IWvv-q1IOznXS94gXTQrtKGY4L70,46
+qai_hub_models/models/stable_diffusion_v2_1_quantized/test.py,sha256=PROiQGf6_dOO-Xj0ShW59-4FVVY_0NFYLqgM6mm8dvE,1056
 qai_hub_models/models/stylegan2/__init__.py,sha256=DEYHc9DKA6dqX9iajm9yRNNFLLVZzwzyo4jepbBghDg,404
 qai_hub_models/models/stylegan2/app.py,sha256=lif1hpxwzpEWKHJUWJn_b8U_UdNQuqOecVqAZxqHGDI,4155
 qai_hub_models/models/stylegan2/conftest.py,sha256=iynzCBgS9hzFA4G9nr3Eu6x0jdC46CXjkb4XlO8XMlY,1408
-qai_hub_models/models/stylegan2/demo.py,sha256=FzMK1wvc9Qsf46XK2HqFdzCIel_q5zjOGCoBjlA2BuE,2847
-qai_hub_models/models/stylegan2/export.py,sha256=gq-J8BClePpemvI_um2LFItsmmv-gkbSEAN3sf15SAo,8043
+qai_hub_models/models/stylegan2/demo.py,sha256=_f8uUbIyh6w7BwY60vi4UO8p9HZTlWC0tk0eTJ1wfc8,2758
+qai_hub_models/models/stylegan2/export.py,sha256=PLJpD1ZoZnq2Hzn0pyfjH4_W0CSpvkDET3bs7zEk110,8404
 qai_hub_models/models/stylegan2/info.yaml,sha256=V9PJJsIuSz0pAWl_YfsVUO9-INABRs-aFTpxfEK3jGk,1084
-qai_hub_models/models/stylegan2/model.py,sha256=GxCrDsAd8ovfqMNsYKuTz8VBOqbv8WlUYdXM_0xk_VY,8406
-qai_hub_models/models/stylegan2/perf.yaml,sha256=-OYHtOuxDSl45xZLNJSn9jeNlbTzxEiTWhoP_HzePRs,3385
-qai_hub_models/models/stylegan2/requirements.txt,sha256=qOlq51yRk-GCNSKWjcoQno6BFnDOutA_gOCsMRiEm50,11
+qai_hub_models/models/stylegan2/model.py,sha256=GhVmUDfaYx7Ua4i8i75nydyT3C6PVLe-dDafsIJZ5aA,8870
+qai_hub_models/models/stylegan2/perf.yaml,sha256=X4rfrCIC3NHtEHkDr_fDLQ-fkw_bCwEsW9eaYlvZZUs,5595
+qai_hub_models/models/stylegan2/requirements.txt,sha256=HYeGyK7KbJ1n7XdbUGhxeQSdcqP2uCbGxN8baWFyr7o,13
 qai_hub_models/models/stylegan2/test.py,sha256=yv5SE96wBsLm-6YNITvLtP0MQUMEUce-66yyvAbtMfY,2497
 qai_hub_models/models/swin_base/__init__.py,sha256=p2IuNVS1xnxD94TMxel73mdH8VFqkLGalELLTDN2rQk,471
 qai_hub_models/models/swin_base/conftest.py,sha256=g07LL7AVF9Y0ug2jkLKRRyvgc7fJoWvAHlj7lVhsdV8,1314
 qai_hub_models/models/swin_base/demo.py,sha256=YsZ3rMrIIZI14QSkHgvlkzk4H4Wb9eVxnR7Ne7ohA5M,531
-qai_hub_models/models/swin_base/export.py,sha256=q7IGC3PW0q5AH0Hpq3k3OhkjdqFxpB5RkxQ8LWMFIlo,8180
+qai_hub_models/models/swin_base/export.py,sha256=-NDFk1yO9ytkFM_HHB0_YY5oB2AYPxhk3J4GJ_5YElI,8498
 qai_hub_models/models/swin_base/info.yaml,sha256=3LfGk69zgLbjR5j35drDhHRZ8F0ZkA6wuZlAB_85JEE,1383
 qai_hub_models/models/swin_base/model.py,sha256=l_TKswq9uU6F5XSqWxgffQrdGD7yPuqvoAWFl1hBM-g,1241
-qai_hub_models/models/swin_base/perf.yaml,sha256=jwuSKz3_Qo0ZyZVGwE_lRmMxGm9cVFqeN5SWGAp59bU,3405
+qai_hub_models/models/swin_base/perf.yaml,sha256=SsPB3WZRH9L7mXkEYbqUPLVvmFkm_IyuIUIpWv1NgvQ,6053
 qai_hub_models/models/swin_base/test.py,sha256=u4-I37mNGQRA9fPOUkcP6RsIWC82mEDlaHO-gBy_45k,1358
 qai_hub_models/models/swin_small/__init__.py,sha256=HOXEswRvtQUl3Mx-uVvmr9RMaEZ5pYSkMG-TiNK-EZU,472
 qai_hub_models/models/swin_small/conftest.py,sha256=cGLnQTo-GX4paLhN3y83S2qAqulC4svgSpQJs3KOZOQ,1315
 qai_hub_models/models/swin_small/demo.py,sha256=QpJfG2oCPFgZVNW2OZxOSmQCFBXmsCbQSTrAi-xipp0,534
-qai_hub_models/models/swin_small/export.py,sha256=1fdpZrIMiwrQt8ws6ErU6FgHUP5ka2EMIjxf-0D4o24,8184
+qai_hub_models/models/swin_small/export.py,sha256=a-N9WRLcy3KF_n0iwwcwBFdfi7lyOBUjyY7_tGI2CMk,8502
 qai_hub_models/models/swin_small/info.yaml,sha256=F5iz-3X5s-d6JW-HeCHW4AkSRPcd6QSUTS0Rt_E9ObA,1378
 qai_hub_models/models/swin_small/model.py,sha256=1G-ENU7Vf5cbpwH_cqzHLG7qhTgkDPp85ZVcxydBglg,1242
-qai_hub_models/models/swin_small/perf.yaml,sha256=2eQoBGfEq69y4vNwWQG_4_gTcm2LF6xnw76rPT0wvQM,3404
+qai_hub_models/models/swin_small/perf.yaml,sha256=z80prjniDfCS0bnEjIN91eE1bB-OMwVfiYTLpXpccIY,6080
 qai_hub_models/models/swin_small/test.py,sha256=6AzCPufcvpD7II5eHrzckEA-T_jaKHSNVWpQmgBPjAA,1364
 qai_hub_models/models/swin_tiny/__init__.py,sha256=KMabHS9sJvTh7SR1iY71UsqXV8UXvWCCYc8rL-Ifbi0,471
 qai_hub_models/models/swin_tiny/conftest.py,sha256=-7XOvcvqTNJBC1lF325iC7vhykLVdXuOV9DiGA59mV0,1314
 qai_hub_models/models/swin_tiny/demo.py,sha256=sgvM7gVMjsiMPGwZRGg0MBn7MUdD6PoNj-WD1ejr70M,531
-qai_hub_models/models/swin_tiny/export.py,sha256=_gLaa_v79vzyyyMW3Wh0aIx22j4Zd5T_NAIXm3nWE_c,8180
+qai_hub_models/models/swin_tiny/export.py,sha256=zrs6glVej4DT0HYUwIHEvvt247munc9kNZ6_79uSNqU,8498
 qai_hub_models/models/swin_tiny/info.yaml,sha256=xFXYmV7ol4weKYVxrdW2RleeH603mzkJSV1fYQ9XPlM,1376
 qai_hub_models/models/swin_tiny/model.py,sha256=_dgBGuAk4ZWHYPKWLqeHyU605TkcgWOgfQQlDJyBw1c,1241
-qai_hub_models/models/swin_tiny/perf.yaml,sha256=seU2s-neq4KxWmxkHFOaOWgx1rFE7Zaom5AhdMTOqTs,3398
+qai_hub_models/models/swin_tiny/perf.yaml,sha256=yjb_qhxyZoWa1jFqc6ajlO6r-GE021Jz0Ja44P2zAMs,6025
 qai_hub_models/models/swin_tiny/test.py,sha256=pyRcuX_EtrOTbrjGtVXD-vqiW2s_SAFiJSN8SSCqrAI,1476
 qai_hub_models/models/trocr/__init__.py,sha256=wXm8GYsyaT3M3HQlZczeJy3t5Qifdcarms23LJcFT4E,396
 qai_hub_models/models/trocr/app.py,sha256=kiE57iafy8ZZ7_uxo8SR2xs9-LFzaBiNEAjaVzrXbHg,10207
 qai_hub_models/models/trocr/conftest.py,sha256=UMnuW9OGM1Zf3xgCWXrLxkWbudICtfJkg_n9780jCZg,1310
 qai_hub_models/models/trocr/demo.py,sha256=aFQgalfyjuUsZvP5VT7TshR70MCW6UUGfQd8A3nJVag,1779
-qai_hub_models/models/trocr/export.py,sha256=c3xMa8M0gwXcoe_gDraO8FkZnV-GEacEBKayVSY4Hus,9944
+qai_hub_models/models/trocr/export.py,sha256=pya0kv73YM-JGHBG7oTOXs8lHwXqn7lgqD9IUWjp8SE,9911
 qai_hub_models/models/trocr/info.yaml,sha256=XYdHrSlr6IJkxEIvt9trKhBdhW-HSvPA_CFjW-viEcs,1370
 qai_hub_models/models/trocr/model.py,sha256=VF0yXAP60MolsUgkBz1nV-D0X4to_qrNLApcx-TYLnM,10482
-qai_hub_models/models/trocr/perf.yaml,sha256=szBEQsEWW84Z3eQCc4hJZj7158ypy2JjC0GZmPF4uug,6080
+qai_hub_models/models/trocr/perf.yaml,sha256=1vxPzPcezT6I41wHt-ZlUNdlrm7yRre_Zd6do1AcBmE,10503
 qai_hub_models/models/trocr/requirements.txt,sha256=cnbvoRqx9v7r3NmR_f70Cgvd_PGeI7GWEBGEY_vOTyw,42
 qai_hub_models/models/trocr/test.py,sha256=OPMndfm1qj4PKF6YmEyHkbjKi_hpXh6zx6D57XDwNvM,2357
 qai_hub_models/models/unet_segmentation/__init__.py,sha256=eCv56Tgc7rcutJMfHTjnd3W1aUffmcuJtgjSURzbOGc,348
 qai_hub_models/models/unet_segmentation/app.py,sha256=DdrUoO1W-Jk99IuBc-JNHgGNlBXQj-p1f0h_xyxiQ1w,1305
 qai_hub_models/models/unet_segmentation/conftest.py,sha256=o9t9jgaZIDwQ5GOGEJzu7IxvJBUcG8QvUrZkGJtBgdQ,1322
 qai_hub_models/models/unet_segmentation/demo.py,sha256=6WhmNetb5POKcNluaRClyIsyPWHDGRSn1-p6v82yQUE,2509
-qai_hub_models/models/unet_segmentation/export.py,sha256=n7wUCeNcoi6kGUhcOpsCmVRIXnCznEYMdxvj3ExxRQQ,8470
+qai_hub_models/models/unet_segmentation/export.py,sha256=YXe1cX33HDwZ7YlDmSFisvWO7Cx6zHA0T-io4VUxsrg,8870
 qai_hub_models/models/unet_segmentation/info.yaml,sha256=XeH1Ge0eqcX-iNM7iZOczv3o_TYjmdXqYlD2qzzt690,1310
 qai_hub_models/models/unet_segmentation/model.py,sha256=RpV7TfLdmaQNQuyX0FNy7KvJOlBXNyZMuQN70LLaR-g,2666
-qai_hub_models/models/unet_segmentation/perf.yaml,sha256=ywrvCp5xOuI2lRC7m-cnSiqwYWHuZD5WGtsy8TQMSBM,4595
+qai_hub_models/models/unet_segmentation/perf.yaml,sha256=RXt_ghCsspWVMZKgjkvsHupzc4SdKtvIr9FcdI7CwV0,6075
 qai_hub_models/models/unet_segmentation/test.py,sha256=tMInbwc85HhxL0n8tKutOUcc2xXe0GlcEYhGhkynjPg,1215
 qai_hub_models/models/vit/__init__.py,sha256=sb8lq6b6jX0ld11rHa7wllm3un53paM2DsdWQkwsfCQ,466
 qai_hub_models/models/vit/conftest.py,sha256=Uoqu964BWITybAfHp4NzwAr--MuncmyWLd07BxRjMa0,1308
 qai_hub_models/models/vit/demo.py,sha256=WJKe7oH0jOK6__GnIWESX-a2v7LhjX0C6A6vUZvrmZM,515
-qai_hub_models/models/vit/export.py,sha256=grf_TbilWo1LPOref6BVjrKToJ9eJDADMktoRJ_QjZA,8189
+qai_hub_models/models/vit/export.py,sha256=-AVZ0sNs1-Fd7JyzqgRPKN2GsDWOkUROgTrXRqi5ioM,8527
 qai_hub_models/models/vit/info.yaml,sha256=FDb8b6QwZlnXV4D53BxZecQ_AUunJMqgfOMRi-ArmHE,1342
 qai_hub_models/models/vit/model.py,sha256=kIDZHjG_XZs2afBUm5MbB-tv1H3q9Mc7hLzwJ43ITgY,685
-qai_hub_models/models/vit/perf.yaml,sha256=bDq72pXxRjnJnMUAiULm7okuFJW-6LCJvuna9zSi9wU,3394
+qai_hub_models/models/vit/perf.yaml,sha256=9Y7PVin8NJVa6SGua-MNT4-3ysRZvXZYLib3pVXawDg,6035
 qai_hub_models/models/vit/test.py,sha256=2rVAdhoCo3hpKuMinwO_pU_N27Sj7KD_iG2kWeOFpcU,807
 qai_hub_models/models/whisper_base_en/__init__.py,sha256=gPx7T0jgsv-7IFKaRf-Mm-D99qIGGDdvEZngoTC9hL8,444
 qai_hub_models/models/whisper_base_en/conftest.py,sha256=AIIagCL25iXni4px4vYYX2pjPpvrofzcSz6187mHui8,1320
 qai_hub_models/models/whisper_base_en/demo.py,sha256=2Ud55e8xLWbFjwnZBHAdV2xWmRCXUnFl9CylXMLADWE,483
-qai_hub_models/models/whisper_base_en/export.py,sha256=gzYGNeyRdXUUtwrlM5fPGaYo5GDm3Zy62niBZb5igLA,9996
+qai_hub_models/models/whisper_base_en/export.py,sha256=BGohXm1pod6aZpmY296wq2FNLUtQHCs7Fa61VoAO1uo,9929
 qai_hub_models/models/whisper_base_en/info.yaml,sha256=h0b_rJzb3Me1kvHEPgRFCvxVHjdaWNUpd_t9be6KpdY,1849
 qai_hub_models/models/whisper_base_en/model.py,sha256=HPsCIIHuJvVb62t0wYBofNUr6ehqIXrdMYSSrKH65YA,558
-qai_hub_models/models/whisper_base_en/perf.yaml,sha256=jRuFAQS78mKmLd0HiduuAdttru6e7uBgY0ZAIXA0Zok,6064
+qai_hub_models/models/whisper_base_en/perf.yaml,sha256=iY80qCXbaGUpj4OA2NtWgbjklHF9TijZNUigGKRBxjE,11352
 qai_hub_models/models/whisper_base_en/requirements.txt,sha256=6mPzVdJaLJE0PzWpF06bHbvLYjjD6uqQgJK9df11eQk,31
 qai_hub_models/models/whisper_base_en/test.py,sha256=YbVbfTk_wimEZ6dNVV5vPC1u6BpWlbCBzZlaSVnlDpM,696
 qai_hub_models/models/whisper_small_en/__init__.py,sha256=AdhsuSgpnju4sajPAn1AD0Ej3eQ67mW802ehQeWftu0,445
 qai_hub_models/models/whisper_small_en/conftest.py,sha256=iRKz8OphA0wtmFOs_uqmHYCPthiY7WGgj5wV13dVHak,1321
 qai_hub_models/models/whisper_small_en/demo.py,sha256=Qcz9_UlcZMjkRkob6yEHpH-vreKiEPf2Mz0-IGEsGNA,486
-qai_hub_models/models/whisper_small_en/export.py,sha256=5NhaQckv8xC-FcxoZsMWg1sKgNnWYT_gNtLjvIoogcI,10000
+qai_hub_models/models/whisper_small_en/export.py,sha256=T6xF5bNGyL6lQD3-8Bz9oWgM5xzvpIz1FNyPhRpbKLY,9933
 qai_hub_models/models/whisper_small_en/info.yaml,sha256=d2gUKgMzU0fHZ8v1Uo0PVchcMFny9O9XV2B7_gS3p0c,1848
 qai_hub_models/models/whisper_small_en/model.py,sha256=pzVmdcKIbKzQDdJfmSpSqrr2nqUXmEctuVNYaxIpB_Q,560
-qai_hub_models/models/whisper_small_en/perf.yaml,sha256=WeXhaEPc59KV3-kikoYznx3EymfWSpYfNxuzk-LJgOE,6078
+qai_hub_models/models/whisper_small_en/perf.yaml,sha256=JbAYFQVnWfBpTGyNmYm54xC6-eR1N9XQkAkNKMkbVj4,11327
 qai_hub_models/models/whisper_small_en/requirements.txt,sha256=5VmKKgsGZpToTOQ9qGw40spPnoGgu1RiCV1LlwT8PZE,38
 qai_hub_models/models/whisper_small_en/test.py,sha256=YbVbfTk_wimEZ6dNVV5vPC1u6BpWlbCBzZlaSVnlDpM,696
 qai_hub_models/models/whisper_tiny_en/__init__.py,sha256=TmMgJVcrcymvkH9ZV4B1C4ap5GOxKq77dR7BWJsQtd4,444
 qai_hub_models/models/whisper_tiny_en/conftest.py,sha256=MeXHufPXmi_-5qysf0l1Q0lY_l39hJ9cAdsvE_9bnGc,1320
 qai_hub_models/models/whisper_tiny_en/demo.py,sha256=gvvmZMuOmNLc3eybBOyMcghS4tmMTXec9Q7C0f2Idv0,483
-qai_hub_models/models/whisper_tiny_en/export.py,sha256=1qhqnpJ6kYSpErIZwk9Dsbs9yVQtg0njlDky04whlKU,9996
+qai_hub_models/models/whisper_tiny_en/export.py,sha256=v64sHA_N-rtcSZ1ZtZKZ8uIh0p85Xs_02Nz5DeV1ZJk,9929
 qai_hub_models/models/whisper_tiny_en/info.yaml,sha256=r2AoylFZ7T77Mgi8-YSzF8zhHQFGI72-g127nS1Lw8w,1849
 qai_hub_models/models/whisper_tiny_en/model.py,sha256=osptTKHSqPTiEdh4aypQ2WNjEaHnv6yNFY5JXgGjnqg,558
-qai_hub_models/models/whisper_tiny_en/perf.yaml,sha256=K-fOSHskOx1Rpkv2tAMllWtbNPbdisuPX_9Rkf8bva0,6057
+qai_hub_models/models/whisper_tiny_en/perf.yaml,sha256=cvh922EgLOYcNm1nORxf1bHkZ0buRvbglYr65V4uGI0,11252
 qai_hub_models/models/whisper_tiny_en/requirements.txt,sha256=6mPzVdJaLJE0PzWpF06bHbvLYjjD6uqQgJK9df11eQk,31
 qai_hub_models/models/whisper_tiny_en/test.py,sha256=YbVbfTk_wimEZ6dNVV5vPC1u6BpWlbCBzZlaSVnlDpM,696
 qai_hub_models/models/wideresnet50/__init__.py,sha256=2gofgGLnNTfjSpI_9iWz-W1gmfqP32BJ8e7hR2OmyhE,475
 qai_hub_models/models/wideresnet50/conftest.py,sha256=rkl85btEPOePl1IWtCr-ASgm1n2ceLg-uQ8vk13zJXI,1317
 qai_hub_models/models/wideresnet50/demo.py,sha256=kPn94bMa4KF6Q4-yzgeDIr8UfaxAsSGTblk3xG4m9ns,542
-qai_hub_models/models/wideresnet50/export.py,sha256=75ouW_cjJXWTZQK1IhX6pL5eE1wx81UzGRZJBHk7Cao,8172
+qai_hub_models/models/wideresnet50/export.py,sha256=fUJNRSjId0l2ToOn0h2NNE_6KMJTjabhqKSCno5MV7s,8510
 qai_hub_models/models/wideresnet50/info.yaml,sha256=MAq6BAeuSZXhglFGyKF_GWmzrYFqLbZFBOxpLaQ1w3A,1298
 qai_hub_models/models/wideresnet50/model.py,sha256=JkEEOEgGikkD_gPlOZDhI9bnZU-pPfxFj8UifbjP4KQ,710
-qai_hub_models/models/wideresnet50/perf.yaml,sha256=RLTuihLss5P5L-JYr11pKW3_blyGSFGKDa0FrLcCy1o,4567
+qai_hub_models/models/wideresnet50/perf.yaml,sha256=JFFkxnFFJsbhleWSMqpcKP5JqQ6Y1q_q_x_1dNisNWQ,6048
 qai_hub_models/models/wideresnet50/test.py,sha256=4pY8YCYKR4_rURwM3byuRUanThobgwlhlVMG44bnwHA,855
 qai_hub_models/models/wideresnet50_quantized/__init__.py,sha256=OVOoOn80LrgiO61p2JZTOPk4wd6MBIpV78Qoa5ei6Jw,582
 qai_hub_models/models/wideresnet50_quantized/conftest.py,sha256=Zx0Dx2rMLGHf77PHhmcaGXO8n-mTNtXasa1_jrIC9nc,1327
 qai_hub_models/models/wideresnet50_quantized/demo.py,sha256=WTRZHUaAnWHPKjZvyvDF8XZU1dGrqfjoyjxYSGhLaos,587
-qai_hub_models/models/wideresnet50_quantized/export.py,sha256=MZfVj1Qz1vP2_FX2OEWr5NlaQnXyYQomYdamTJZzMWU,8588
+qai_hub_models/models/wideresnet50_quantized/export.py,sha256=1HM79mFi2MT6--rsQ87dyPq7_HmAv7zc0_Z-pGSwIpA,8926
 qai_hub_models/models/wideresnet50_quantized/info.yaml,sha256=gP6lIjMqJDuVtTo8IqaoPNc2upCrOyBKfXBucpG-Mw0,1333
 qai_hub_models/models/wideresnet50_quantized/model.py,sha256=8_KCwK6ydEW0fpAvBGfZlyqygntXmPP-5TavgJIE7WQ,3214
-qai_hub_models/models/wideresnet50_quantized/perf.yaml,sha256=EQAnd0mkztow2NsF6M2jphkvCYFVW5dBzjXQeHlqJUc,6629
+qai_hub_models/models/wideresnet50_quantized/perf.yaml,sha256=gJF2xmQmmf6IWRm54KIdA3qv6IHUMwZ6xX9VYT_jSS4,7729
 qai_hub_models/models/wideresnet50_quantized/test.py,sha256=YJwhgaYpuN_9QXUb-o-vzZJnzKXR_x2iT0Xnqg82-xk,932
 qai_hub_models/models/xlsr/__init__.py,sha256=Xj4k054juNe3bPUFmKqSI-I5zJ7qJLcNhSoUEW8Qwgs,461
 qai_hub_models/models/xlsr/conftest.py,sha256=EQcp6Xa_gttR4T6pr2HxqvkSxg11k2BHCVr6SUejr4g,1403
 qai_hub_models/models/xlsr/demo.py,sha256=xBY0HiM5h0F6F5LSCMbRG6hiKsINFANd_ifc4NIrFyQ,742
-qai_hub_models/models/xlsr/export.py,sha256=uGlZwbt0BBel1wsMLwRUrG4djuyzNqzfi8zYiPZQVRg,8275
+qai_hub_models/models/xlsr/export.py,sha256=bchvoSnUFNHMx8KhNDvazG4GrVcIyu0sK5itPjg4-YY,8675
 qai_hub_models/models/xlsr/info.yaml,sha256=2rij6izH-44dhEtjANktxfoMRctZpcEyvq2583qHVV0,1156
-qai_hub_models/models/xlsr/model.py,sha256=OaTY4JT5HHVVLXJaePfqgtC604FhAZzddfQTMx82y1w,3403
-qai_hub_models/models/xlsr/perf.yaml,sha256=pkUsHPitSIvfTHsEiy_-Mz90FTgPM30U_kR6ssfzgts,4545
+qai_hub_models/models/xlsr/model.py,sha256=Gk2H50gy3p3e5fuVO9qe6sRKVkyg_b52H5ue9qBKMlI,3373
+qai_hub_models/models/xlsr/perf.yaml,sha256=mJEnWt7zNumm970qJdafm4sRn0QlRP_ugTn9KgwRMTI,6009
 qai_hub_models/models/xlsr/test.py,sha256=Qyu0UUHhC2zr6cN-3u44Ag66nyD7Dod5o17Le3XRnfI,1402
 qai_hub_models/models/xlsr_quantized/__init__.py,sha256=T-8JP8m6jhMk9yhGMGibcnc_uDL-LMR_IHSc5zCqD_E,472
 qai_hub_models/models/xlsr_quantized/conftest.py,sha256=lOttq-aUmsLNE6-sp_15A--e2a99Kpc2K_3UYftuoHk,1413
 qai_hub_models/models/xlsr_quantized/demo.py,sha256=mWpJz3Ignt_53oi5Gn757OxdgTQoVWuz1KXESXaOXyk,956
-qai_hub_models/models/xlsr_quantized/export.py,sha256=Jju7SKqyVrqejFMU_j6jvRCXB4mq1DjdxanQWtEpRQs,8711
+qai_hub_models/models/xlsr_quantized/export.py,sha256=2ZuD5zSRjMBsE8kQNS9d9JXTdhDpga_kR-5YnXS6wlw,9091
 qai_hub_models/models/xlsr_quantized/info.yaml,sha256=6qK2zWzJLt9NrUj4EIX0gGSc_RfbExbFDVbmmDoB-Ts,1191
-qai_hub_models/models/xlsr_quantized/model.py,sha256=1gGN57aBaLu0sxnE5GFDYZ2TsvBIzydv1mEhgVB-zmo,3915
-qai_hub_models/models/xlsr_quantized/perf.yaml,sha256=bmOIaQbM_zqxgr2GubAeQkg4jJsIVW4C-eXT-Cqc1PU,3919
+qai_hub_models/models/xlsr_quantized/model.py,sha256=1rDpq5QEMf-oSHDdXbBO6g8czAFIiQ27d0VLKMGoKW4,2816
+qai_hub_models/models/xlsr_quantized/perf.yaml,sha256=QzkgRV7BJUbUSY5fQ1CzgNjXMzJzskyXP52VIgjmn18,7716
 qai_hub_models/models/xlsr_quantized/test.py,sha256=b5hK1dbE8zYxn0fePctYKEbEQfIJBTETUjBZblBG5TY,1607
+qai_hub_models/models/yolonas/__init__.py,sha256=z_SMm0g3YPxUvEoBuL01VMlBdd9BQ8CdFCYq4dNxTNM,439
+qai_hub_models/models/yolonas/app.py,sha256=eiDhfibghnbRZtGhB0h5Iy2JmxCiq5H-zx69xjKfcHA,2185
+qai_hub_models/models/yolonas/conftest.py,sha256=x91WUL3ICvl6E8F0VLN-1jbrIdSRzwg7qjLVDkgq3DE,1406
+qai_hub_models/models/yolonas/demo.py,sha256=Unipvgfk8yBRROeJEAYYbQO0bpozuFn9pjYnKIc1zaY,811
+qai_hub_models/models/yolonas/export.py,sha256=T7ogrNfED8vJsdrr24qYPNlEpwPg9Az6ryKTUKNuCvg,8490
+qai_hub_models/models/yolonas/info.yaml,sha256=fA0TjTP9KO1PcBHiaEpikNadBkZ9QlNnM4WunBa-Vmo,1270
+qai_hub_models/models/yolonas/model.py,sha256=ZwxzUsk1GL5_JUY_KBhZIRCnfJ1TcWL9b-r4b5uUkWo,6291
+qai_hub_models/models/yolonas/perf.yaml,sha256=2UK6xwlayXUHoxHW-_VKcyJU-ikjd5M1tko6igg-7GQ,6062
+qai_hub_models/models/yolonas/requirements.txt,sha256=BWGE2irEgh5wGwjU2JJDtsdV28RxEIG-csodp2YwjQQ,173
+qai_hub_models/models/yolonas/test.py,sha256=0y9teeAlf6L-_IYuFwH0Q-KQAWS3VkLS01fde_lq0n8,1516
+qai_hub_models/models/yolonas_quantized/__init__.py,sha256=wqkEusrFiIgKkTTNi9q4KYCenOg_URO3DHYZCteqIR4,450
+qai_hub_models/models/yolonas_quantized/conftest.py,sha256=bQLOfeuMHVJSky6jHztBarE2pIfjX52yRGkgz2zr8j8,1416
+qai_hub_models/models/yolonas_quantized/demo.py,sha256=62z-RaIhyzlywCe0jVeIEO-igbfDw2j1_9KMi_zgg0c,854
+qai_hub_models/models/yolonas_quantized/export.py,sha256=yyoG90yU-Z-xefNqmkBBjci3nMRtEMN_T4VWchkw9UI,8946
+qai_hub_models/models/yolonas_quantized/info.yaml,sha256=-Gmk1hLz9uvzUacy9EbeDLihrBTXf75E-QwaRxJIkfM,1399
+qai_hub_models/models/yolonas_quantized/model.py,sha256=_vjrdve7DJt73UDkwRFq8f205D8xsWFiL4_5ykTrmYQ,3232
+qai_hub_models/models/yolonas_quantized/perf.yaml,sha256=9qTxZmvlnYmJ9FlA698iVaUWje0FCjMllN1lXCcLGig,7205
+qai_hub_models/models/yolonas_quantized/requirements.txt,sha256=BWGE2irEgh5wGwjU2JJDtsdV28RxEIG-csodp2YwjQQ,173
+qai_hub_models/models/yolonas_quantized/test.py,sha256=-TBVYgDzepAs0O4MBdrhsQ6O632YB9ddZZ8yO00Ddf4,1583
 qai_hub_models/models/yolov6/__init__.py,sha256=Nj-zEnhLQO70kQCq81cFp9dgULeIydtL993JsUebRVs,436
 qai_hub_models/models/yolov6/app.py,sha256=vZ9FeEPzCDxyetqe9PRRung35W7VlB2agaRnXPJ3a1I,1071
 qai_hub_models/models/yolov6/conftest.py,sha256=IEuRYdXu-l03Qj2P856JzCzdqwkGglnJ3WUm2R15JAA,1405
 qai_hub_models/models/yolov6/demo.py,sha256=53T1nvu3uhTeAjz1OQC8UuOgdHqSYW0lLigjTGj6UD0,1027
-qai_hub_models/models/yolov6/export.py,sha256=QdSC04hrbeM7g_m1Wnr25CDW_qJDNHW1vDYrfGu870A,8178
+qai_hub_models/models/yolov6/export.py,sha256=BdKXbZGoBKLIXr2T98TNCWUivnbwgBu1M3G-CX9HsKw,8486
 qai_hub_models/models/yolov6/info.yaml,sha256=lhJNwEPUVxBxRgiHqrAz4lV4Sr48haa7dASp65oXZYE,1168
-qai_hub_models/models/yolov6/model.py,sha256=M5LrnuwB1SE4sOQ-mXTeEtnEBbKCcyCNqiazGavHS5I,4686
-qai_hub_models/models/yolov6/perf.yaml,sha256=ZdjTaEIbTQwnLhDqU_9z9Yd4MIVGcTAcs_a1ONpzpRw,4574
+qai_hub_models/models/yolov6/model.py,sha256=RF-n1zAHmcT4IiSdffvfhODRzx1M0wyxz7SHUITHZtI,4677
+qai_hub_models/models/yolov6/perf.yaml,sha256=1CwhfZ1PbPirkXrmdvHBjLxrBNx4ovQ95woOKWudLiw,6058
 qai_hub_models/models/yolov6/test.py,sha256=LaGoMWkMYM-w4WgX8gomuU_3nWa4f-H8mDxNmjk22A0,1845
 qai_hub_models/models/yolov7/__init__.py,sha256=Eou1LnQavGFVBbp1fNjErCYGt5uD82Jy-ID8GRjZb50,436
-qai_hub_models/models/yolov7/app.py,sha256=C_u1KX19yUvyWPCijwIawFhWPWWwIDY84PoIofAYQT4,2033
+qai_hub_models/models/yolov7/app.py,sha256=pE3YmY1O069p5-RRM4zoQ6xbT13Hf0FCEeOzV13wo-8,2188
 qai_hub_models/models/yolov7/conftest.py,sha256=7v6cG4X5mCFo7jU6pL3uhGQncRwaUWV5r1F0QowDvDU,1405
 qai_hub_models/models/yolov7/demo.py,sha256=d9tyOftqvwQdh_xP8Bo4xDAeoWx4mynA41caE18xKLY,909
-qai_hub_models/models/yolov7/export.py,sha256=QJjnAbrpU2FxuWLOcnJfWIxdS2N5DbhlfkBxbBQLims,8198
+qai_hub_models/models/yolov7/export.py,sha256=M4tH_NXpUNEJ9XjB6Zgv44Gna1zhhJFSye8X1q1cUZA,8506
 qai_hub_models/models/yolov7/info.yaml,sha256=IafIJ3NrcVkjlgn9mn1OT44HhnS-Cle3Tq_8vAjX-fA,1131
-qai_hub_models/models/yolov7/model.py,sha256=WReHjQa9--_co8yv9TLQrZALcS3XqY6oWj_Xc5jNrw0,11972
-qai_hub_models/models/yolov7/perf.yaml,sha256=895i61OrTu24omgWHQqov3OhiYVoxFk_xs9SpuP6Jl4,3408
-qai_hub_models/models/yolov7/requirements.txt,sha256=PW4Lx0JxDAgGAISG4XX8_qTk1AAh9xaF5VUU9on5LG4,83
+qai_hub_models/models/yolov7/model.py,sha256=D-LyIykzV15jp1lypZjfsqpQSV7m2-Rnkoy2yEed4dM,12010
+qai_hub_models/models/yolov7/perf.yaml,sha256=V2O-Sc6idiaySYkpDYsrolcWPC6Xp8BEgMfPEg8i75U,5604
+qai_hub_models/models/yolov7/requirements.txt,sha256=mSlIXNgathZIRgd4fQBICrBbeKbD2PCzsSkW7lb0zxs,98
 qai_hub_models/models/yolov7/test.py,sha256=AtgMa81Mg20TrZaKK1aVSkZuj7jf0sjMXZ-kda5sfE4,2322
 qai_hub_models/models/yolov7_quantized/__init__.py,sha256=NceTvKTE95x6nsrxuUev_YhyWyBdFWfF_BfGT3H9ajU,447
 qai_hub_models/models/yolov7_quantized/conftest.py,sha256=JEUUBieJJGJZMFLu_QIQiCOV7L244KyNZlcl6sxiIEE,1415
 qai_hub_models/models/yolov7_quantized/demo.py,sha256=ksBNS182rXuOGjOKclcsnKfN8wqmL9P1IWHO7YCXHHg,853
-qai_hub_models/models/yolov7_quantized/export.py,sha256=XRr6tlXRGjRcGmEKLQqMCLeX8DVW3mGvAjwvDT56IHM,8614
-qai_hub_models/models/yolov7_quantized/info.yaml,sha256=DxF2q2V6Hy3wV1IUvmUhowiciPxKfeEyqypOCoBqIIk,1318
+qai_hub_models/models/yolov7_quantized/export.py,sha256=U2qlnXU6t0OQXe8UHLPpWzRLfHjChbPvyURho8iRw2c,8942
+qai_hub_models/models/yolov7_quantized/info.yaml,sha256=f6KsXs4YUtTKkbCMsMLOIy0JqDwPnt6WtoPuHfZekns,1285
 qai_hub_models/models/yolov7_quantized/model.py,sha256=Uai9cHpJ1qzIm3_CGa5fVpjanCXOBV2wLS769NbNLT8,3251
-qai_hub_models/models/yolov7_quantized/perf.yaml,sha256=EzBPG6MD-V5Y4QhBkTZsfHaPN85599QMshm-i3mVWqs,4045
-qai_hub_models/models/yolov7_quantized/requirements.txt,sha256=PW4Lx0JxDAgGAISG4XX8_qTk1AAh9xaF5VUU9on5LG4,83
+qai_hub_models/models/yolov7_quantized/perf.yaml,sha256=uXJlUYTIB8U9MUYmo1HAG4hDjsvtorrDKOnuRpj94Ag,7224
+qai_hub_models/models/yolov7_quantized/requirements.txt,sha256=mSlIXNgathZIRgd4fQBICrBbeKbD2PCzsSkW7lb0zxs,98
 qai_hub_models/models/yolov7_quantized/test.py,sha256=YHD_l2dPCxYrfVwHdGAzatCBKpLmizd3PMILop4PnRI,1558
 qai_hub_models/models/yolov8_det/__init__.py,sha256=Nb2Zbj0lG9sZBbpuQNXewrVWXEWCWVeP4lceLzYEhnw,415
 qai_hub_models/models/yolov8_det/app.py,sha256=av7rEkUBYQeL6bO24j_8C_RjLGzR-iO9QG4a_tQ89yo,892
 qai_hub_models/models/yolov8_det/conftest.py,sha256=xwU-0TlimV953NUiCY6kybxhYJDidMtsT5U9RU0QESI,1409
 qai_hub_models/models/yolov8_det/demo.py,sha256=wQWSWP6jzBUI9jUo5yA8P2Of-JE9IXI1DguJ2mg8du0,926
-qai_hub_models/models/yolov8_det/export.py,sha256=Dz5c2iKzU1OcybgD4-eOyCPnXYa12Ul2VPsOnMK8g-A,8252
+qai_hub_models/models/yolov8_det/export.py,sha256=Hue6cLgcG51viR1Jc5Pmuiw-cZSNtbQjle_J54lpR5A,8540
 qai_hub_models/models/yolov8_det/info.yaml,sha256=nkmJtXLXqc8u7zPVbu0F0ztsfGtrQ7dPtVPqJaGWkO8,1171
-qai_hub_models/models/yolov8_det/model.py,sha256=SFBzfbrmsk9o9QOp_X8QCiz-QQobW8H3ei6m2riAI6g,8031
-qai_hub_models/models/yolov8_det/perf.yaml,sha256=OFGlCVDTzvn_qreH2Uhdwz3v4DxLM_5tmwfOVBUHpeI,2768
-qai_hub_models/models/yolov8_det/requirements.txt,sha256=KeSSTmOmqqXkmr85OvV2DHCUUVVwe3wB7bLEQFKdwfg,100
+qai_hub_models/models/yolov8_det/model.py,sha256=eV9F9jV1-kwTzBq9HOM3_EwhhSHfZw2f_Xt8iQ84e3o,8061
+qai_hub_models/models/yolov8_det/perf.yaml,sha256=H6iFXX2R9PbQ4D1nfSE9h6lIbnQ894SLlwumMbRJihQ,6067
+qai_hub_models/models/yolov8_det/requirements.txt,sha256=0A7AM_DrVXsUcpZcCvsB6jRloGAHalo7n7qdgCYGZ18,115
 qai_hub_models/models/yolov8_det/test.py,sha256=3FiaunOU6sy0X_MKDxlV2-sOGaJ0qZUV26WCDZmcQrE,2270
 qai_hub_models/models/yolov8_det_quantized/__init__.py,sha256=bQ-K9zPqN3IIaEPpmzX0zB0JGpk1ggt_C2IpOa4sQlw,459
 qai_hub_models/models/yolov8_det_quantized/conftest.py,sha256=Mh8heAplzMXMsYTug5yM6pftIHMoEDYwtKa_LFBDjLo,1419
 qai_hub_models/models/yolov8_det_quantized/demo.py,sha256=ekFtmhgdxaaaqJSNC11Rz-6HgIO9yvVlPfdjQZnBuMs,804
-qai_hub_models/models/yolov8_det_quantized/export.py,sha256=e9dzpurKzg__uaIDIuog4Mkdg0HXIeD6wzl8-QEhR18,8635
-qai_hub_models/models/yolov8_det_quantized/info.yaml,sha256=9b68CjV_PSrw7K5Z65IXvvPkyH3FhERnkblNJEU-Tm8,1354
+qai_hub_models/models/yolov8_det_quantized/export.py,sha256=GW7hyVF6tKzYlxnCkQt2m58m3BH4W4X5H1cI4N-Ipmo,8963
+qai_hub_models/models/yolov8_det_quantized/info.yaml,sha256=AFTz7ZgxNjLiJQrPH2zJMj87sVItOcDi9WBRIAyIEOs,1321
 qai_hub_models/models/yolov8_det_quantized/model.py,sha256=BeEaUPdbgZEZjWlQrBnxtKzUKXatByqjDFV-_RmhVvs,3540
-qai_hub_models/models/yolov8_det_quantized/perf.yaml,sha256=DDIFMDp0hoasej6Gt35PeW_ylIP0suVKeKdtAc9NaZ8,2777
-qai_hub_models/models/yolov8_det_quantized/requirements.txt,sha256=KeSSTmOmqqXkmr85OvV2DHCUUVVwe3wB7bLEQFKdwfg,100
+qai_hub_models/models/yolov8_det_quantized/perf.yaml,sha256=J7Lp4rV9a8eHxzijyheXJ8BpJnO3FWzq79roMZ14Fcw,7225
+qai_hub_models/models/yolov8_det_quantized/requirements.txt,sha256=0A7AM_DrVXsUcpZcCvsB6jRloGAHalo7n7qdgCYGZ18,115
 qai_hub_models/models/yolov8_det_quantized/test.py,sha256=IFXvXCBWHsuleQUM3C8LaeTtpTg3mq7aTd6AD0quNCk,1542
 qai_hub_models/models/yolov8_seg/__init__.py,sha256=lX4Am1C4ATTR-7lkE-t0OObLD1MzUuPH8EgaNNZRKcY,419
 qai_hub_models/models/yolov8_seg/app.py,sha256=J0ULAGgwVPwk3qA_YbJ8QI9HSaK2z4ziPFifs4SigZ0,7698
 qai_hub_models/models/yolov8_seg/conftest.py,sha256=xL3qFsShwnb9F_FNBleCnPHKweYOl6azYL_fyhaFcGA,1315
 qai_hub_models/models/yolov8_seg/demo.py,sha256=vSnKZpPUHS1JoFvo7BHU3Ob3LRUyu4AUVimU5JeikjU,3155
-qai_hub_models/models/yolov8_seg/export.py,sha256=qJgfgCq91z4vo_MGJ6NylprBIAo0BuIn2Z6BG2F4EEI,8255
-qai_hub_models/models/yolov8_seg/info.yaml,sha256=pNHALG4CJQq4sYjwljtyRttWOqx8bUWBYkSrV3R335Y,1287
+qai_hub_models/models/yolov8_seg/export.py,sha256=p7jxDFjPKlkRDCUkDch5fbBDDvMdc3baBdvV1dCnhHk,8563
+qai_hub_models/models/yolov8_seg/info.yaml,sha256=4jbfMiHj5iT6eRfW5nckw2YE2jU2W3hz4hxoBBCXmZE,1288
 qai_hub_models/models/yolov8_seg/model.py,sha256=Ge6Jt8U-Ibc55bgJqWi5B15h6VNf3fP6jLOrZBeDx-4,4663
-qai_hub_models/models/yolov8_seg/perf.yaml,sha256=IC5-KuQKt2RaqK3leQgODc_WZPhu-tVD_5f43aXghnk,3409
+qai_hub_models/models/yolov8_seg/perf.yaml,sha256=qsCIfboOTme-SiI7_PAdPdLHKVraPBUDzCyrVgGHZiQ,6073
 qai_hub_models/models/yolov8_seg/requirements.txt,sha256=bzLR7n9PMXzABwKJJUEb6gOX23t_wZuYJkbyRPJjjQI,64
 qai_hub_models/models/yolov8_seg/test.py,sha256=pagCgV1nOZpFXyx8Z34mi6b4okh7hMIPxbGzhNzT6iY,2536
 qai_hub_models/test/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/test/test_async_compile_jobs.py,sha256=YpUEjSSeDKtLUYeglO0OxIH1ph5z88Lmz1W0VIAl9xw,1043
 qai_hub_models/test/e2e/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/test/e2e/test_aimet_compile.py,sha256=q4dXkf3-pvNK_mR5OmgQv0ZzAFPfPRyOB1rC9ZXqTT4,1661
 qai_hub_models/test/test_utils/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/test/test_utils/perf.yaml,sha256=XjLXpqrOwleDdnxWXkgyZZtkRHTq6UoLkaKIAVdTl-U,1493
 qai_hub_models/test/test_utils/test_info_specs.py,sha256=zSIdFoI8SOrcy8MmwC2j7vMAzZFZCBJ-rWvXpMZteLE,3229
 qai_hub_models/test/test_utils/test_perf_summary.py,sha256=BRlkxFyjaq5wqTA14rHE5xbbsDwt6qJ2yp1s82f5bek,6525
 qai_hub_models/test/test_utils/test_qai_hub_helpers.py,sha256=0pwOk90MDeUJjubQaMDC8NSTtcX_PABa6mdLZJdpGWU,3295
 qai_hub_models/utils/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/utils/args.py,sha256=3Y36PdzNBMo4V1rPTysk7ShOtB50tuObn8aGuOHINlo,17286
-qai_hub_models/utils/asset_loaders.py,sha256=ZFkgBC9vCBumh4t8-RGaayQW9MFODdDibaCjHJM641Y,34094
-qai_hub_models/utils/base_model.py,sha256=u7JEUyXKcay0qSVKclIdDqj79K46t0ustgtl_Yialac,5946
+qai_hub_models/utils/args.py,sha256=HTw62syqizx8CQLPRGlAPk_jeNQa3XovqKB6J2OTmTI,17968
+qai_hub_models/utils/asset_loaders.py,sha256=ya1PPkC2-XozKDfjvDlF_1gBXRELBE7bOhIEZruYR4E,35317
+qai_hub_models/utils/base_model.py,sha256=GtI8e9RHqzRsV9mjf5u7vrZX3uSV06OBK6CnO82An7E,7243
 qai_hub_models/utils/bounding_box_processing.py,sha256=Y7-situFnXBfbr2_hbKX1VTyr7u9mZDfuymEF0Ki82k,9261
 qai_hub_models/utils/camera_capture.py,sha256=9W3v2XIEUB2lRuO6DYtLLZs-aaRMwh29Vu7eeQN3M_Q,1771
-qai_hub_models/utils/compare.py,sha256=3Da26Q7DYYI6AGA1C9boVS-rnrwxOLuD-lMGOB54UAY,5276
-qai_hub_models/utils/config_loaders.py,sha256=vTnqqrNtu1kn1zASMErFHD60hrtFDwFe4nhy6gNbXKE,32743
+qai_hub_models/utils/compare.py,sha256=bBdIBGzO3WUux2M8LiuFjSdYXBkyiJOEXUzu8BmuQhU,5596
+qai_hub_models/utils/config_loaders.py,sha256=MKAnI4Rw4eSBQxe0HU41p06cob03zznnZdn2rzqAwy4,33382
 qai_hub_models/utils/display.py,sha256=YlvgoKyKVckfSJfbIbWC110LyQ2ziqK1ZKZUXeggU7c,3066
-qai_hub_models/utils/draw.py,sha256=o5N3H1-QKXKA7gIKaQ7tclI2gjRPiPyvIo-CJqbL7Cs,6403
-qai_hub_models/utils/huggingface.py,sha256=OSLKW14892oyrAYn3ovL1d-TUVHDja-BVylnlnDQnSI,1549
-qai_hub_models/utils/image_processing.py,sha256=hYp8LJ0xoCRAzVs0QXafG7fr3aWmPEaBHTDLzJ8wc7w,13246
-qai_hub_models/utils/inference.py,sha256=hRR0jwhIoam_ydKri15bkbkyE2QwfT-adWSIucKTTjQ,12482
+qai_hub_models/utils/draw.py,sha256=CrGshKGpIMonlujy6SDV96u4IAC9RqURiUaRP-l6-K4,6359
+qai_hub_models/utils/huggingface.py,sha256=tXaKda1hegM62LHUC0Cspkv_0lo1REvSUare-Gbk74g,3259
+qai_hub_models/utils/image_processing.py,sha256=IbL12KWajXjyOi_KZt0-DVObGcJQb_uUyDCE3PW7emo,13609
+qai_hub_models/utils/inference.py,sha256=DioRyCZMMhZDjLvAI0XRQsR17-tq6-A5a8SnE6X_VDM,13114
 qai_hub_models/utils/input_spec.py,sha256=3PW9fB0UufkPWiSoH_QPzzFix0Bv_SOYp75IOlAeGRc,1308
-qai_hub_models/utils/measurement.py,sha256=DUsed_0I6RcwD789evI0iCVHZma9z3GK0LF-BMfH2EA,4559
-qai_hub_models/utils/model_adapters.py,sha256=nJ47EPkXSdp8DwRbplCvekFo8sVgiWIgNfmE-l_WLsw,1577
+qai_hub_models/utils/measurement.py,sha256=mPEa1MC-ynzFBeqD2z5qN4d2LBXnyAFZyj96lnj8nwk,4593
+qai_hub_models/utils/model_adapters.py,sha256=tdTRVGvVRnmx5RDGJS-A1cY83flShGNfAFjd3X5rfOw,1567
 qai_hub_models/utils/path_helpers.py,sha256=WM38eDNpJ3nkI7VyQDknnaEJ7p9BW3kjzjXFuhdHgK0,1406
-qai_hub_models/utils/printing.py,sha256=3Gz9w9sqjPaQO0MMBQ2V5SMVSZ3w11lw3VEoIjD3pbY,5007
-qai_hub_models/utils/qai_hub_helpers.py,sha256=8v5uwdgSDi6jbXrhbmP_xXWdjC4lE7rFjdgxv2wgqbA,5365
+qai_hub_models/utils/printing.py,sha256=Z9_IsxwSu9UqRX9hnEfF2YYsL4k6J5f4CLFv4ISO21U,5953
+qai_hub_models/utils/qai_hub_helpers.py,sha256=B8IubkRLCCWmOAm6MYK1N8osKDnFyo0Sc7GmoA2ODrQ,5378
 qai_hub_models/utils/qnn_helpers.py,sha256=TegEPjCtmcRtX2jj3JQlOpuuNHyW1x_X3kaDE2OpqsM,1463
 qai_hub_models/utils/quantization.py,sha256=ah5-teAxv075l2U2THUWUXUy4XF9zSfFE9gg2Uu0OKE,2170
-qai_hub_models/utils/quantization_aimet.py,sha256=SNcwH-Pim52QqsdO8hUllVYmoF5_D-Xerbaz_WJInSU,17774
+qai_hub_models/utils/quantization_aimet.py,sha256=8dO4Q77pUBwgqGJ2zQGhFxTPDsbIuoy4VQY9Unia6uI,19925
+qai_hub_models/utils/system_info.py,sha256=7kPt7Xw-6WzeRBS9sgHRo9beF-bpXqxFNQf-eGFYDLs,2278
 qai_hub_models/utils/test_compare.py,sha256=m9jsmf6l30obMQAnlk88refyfP9HU3W_qcV1-tZq_7o,754
 qai_hub_models/utils/testing.py,sha256=sa2y5NXWyEVih5_0dL8qfTReXUt0pFgP7hpjlViuECM,3173
 qai_hub_models/utils/aimet/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
 qai_hub_models/utils/aimet/config_loader.py,sha256=KxZzYmw4550LnQnlJQclgulV2MHNJhxKkHYQDXu3bKU,876
 qai_hub_models/utils/aimet/default_config.json,sha256=uq3OIQ03ON9IHyRbNvLybGMPA4mpCXsE8yE_n57PqkI,1233
 qai_hub_models/utils/aimet/default_config_legacy_v1.json,sha256=2YxxGBslxx_u-bG8n9UGFzc12_13zS6DEQ3cJD5KxH8,946
 qai_hub_models/utils/aimet/default_config_legacy_v2.json,sha256=Beb1iFG4Uu_iCF5aoWNEZ_E4-rqfGbD_qtE5NEDnOKc,955
+qai_hub_models/utils/aimet/default_config_llama.json,sha256=qCVDiYz13y9ESTO4vS705wQNTMW7sNSCafseCUbwfAM,2725
 qai_hub_models/utils/aimet/default_config_per_channel_qnn.json,sha256=G08RroTHx9H9VrlAcCya4gJ2B9tl1c23NN_SH8N8_Yk,919
 qai_hub_models/utils/aimet/repo.py,sha256=d33BJ9T8pUXA_lOE9uwJrdUn-tH_a2wetk82CewnsAI,1187
 qai_hub_models/utils/scorecard/__init__.py,sha256=M8gSdRkFNJhhoTjURjFFwAos_vFZBfrK9VNXYB7yEzo,259
-qai_hub_models/utils/scorecard/common.py,sha256=aCh3eEgl6ArFWNtd3f-d7eGqBt3cxOsC3J-_uDZTkM0,1289
-qai_hub_models/utils/scorecard/job_summary.py,sha256=evdsH80qe_c0tZKdAa-ZPMgo9Okx2LzbX314pBXeWjI,12491
-qai_hub_models/utils/scorecard/model_card.py,sha256=Src_R8XdlkoQDcXKlpX7LLIMy3sWpTiwgzcX5EWNyUI,13385
+qai_hub_models/utils/scorecard/common.py,sha256=BDdCBx2wa30wdk8rDFr3QCttJ6kjJ5N4nLSz6lVb8EA,9556
+qai_hub_models/utils/scorecard/job_summary.py,sha256=6mKPg5L9lI79lP3xvPfTMofVVXUrlmWTQ7hWk1xRgxM,12040
+qai_hub_models/utils/scorecard/model_card.py,sha256=DduTaBq-aVb5Pv05qbMA6r0Nw2xJhXAR7hiyENBmJAs,14429
 qai_hub_models/utils/scorecard/perf_summary.py,sha256=nrAKooK5FxonON_qbe45nDw933r0s0rFTHdvNmccxGA,11415
-qai_hub_models-0.5.1.dist-info/LICENSE,sha256=i2rmENXGu1jwHqNMu7arhPkIcgMnWTcOyMyXktqe5PA,1481
-qai_hub_models-0.5.1.dist-info/METADATA,sha256=IKtHVehOHzum57df78rKjL0DNsi84jHza2G_o0OBxw4,43084
-qai_hub_models-0.5.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-qai_hub_models-0.5.1.dist-info/top_level.txt,sha256=p1WCkillFWC1qnvse7gwhc-dqH0dNRTpd4Xe-wqn4IY,15
-qai_hub_models-0.5.1.dist-info/RECORD,,
+qai_hub_models-0.6.0.dist-info/LICENSE,sha256=i2rmENXGu1jwHqNMu7arhPkIcgMnWTcOyMyXktqe5PA,1481
+qai_hub_models-0.6.0.dist-info/METADATA,sha256=od8c5Zf1fedavM1SSgfbSAR9WvUjnDC7di7hYwQQi84,48427
+qai_hub_models-0.6.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+qai_hub_models-0.6.0.dist-info/top_level.txt,sha256=p1WCkillFWC1qnvse7gwhc-dqH0dNRTpd4Xe-wqn4IY,15
+qai_hub_models-0.6.0.dist-info/RECORD,,
```

